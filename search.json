[{"path":"index.html","id":"译者序","chapter":"译者序","heading":"译者序","text":"本书为 Generalized Linear Mixed Models Modern Concepts, Methods Applications, 2nd Edition 的翻译。可点击带 \\(\\blacktriangleright\\) 的标题以展开图表，例如\n数学公式有两种字体，可在任意公式处右键 → “Math Settings” → “Math Renderer” → “HTML-CSS/Common HTML” 中进行选择，默认为后者。若公式位置错乱可尝试刷新页面。在原书出版商网站 的 “Support Material” 栏中包含了原书所有示例的 SAS 代码、所有练习（必要时进行了勘误）及其 SAS 文件。翻译本书的初衷有两点。其一，中文统计教科书/专著屈指可数。当然这要排除入门级统计教材，因为这已经泛滥成灾了。当统计学专业的学生接受了前两年的本科教学，试图寻找中高级统计学亚学科的教材/专著时——在 2024 年，统计学在中国独立于数学成为单独一级学科的 13 年1后——会看到这样的景象：英文专著浩如烟海，中文专著屈指可数。我就想，既然有能力的人不愿意写，没能力的人写不出来，不如由我来翻译吧。为什么是我？不为什么，而要反过来问，为什么不是我？其二，应用统计界对于方法的滥用误用到了令人发指的程度。这不是统计人的错，而是各行各业研究者对于统计方法的误解、对于统计建模的无知所导致的。这其实很矛盾，因为如果统计人能写出优秀的中文专著，而不是在浩如烟海的英文专著里披沙炼金，会在很大程度上减少如此事件的发生。统计语言本身就已经高深莫测了，再加上自然语言对统计学科学习的阻碍，这是火上浇油的事情。事实上这两者是“相辅相成”的。大量的中高级统计学方法是通过“计算机语言”来学习的，这类文本是最多的。写这类文本的作者会语重心长的教你怎么“拟合模型”，这行命令代表了模型的哪个公式，输出的结果应该怎么解释？。但我们最好在“几行命令就生成大量统计输出”的浮躁世界中停下来想一想，在“统计语言”、“学科语言”中间插入一个“计算机语言”，这对于学习或者做研究有什么影响？从根本上来说，统计语言是计算机语言的爸爸（当然统计模型的发展离不开算法的进步，现在在很大程度上是相辅相成了），通过计算机语言来学习统计语言就像是“先有儿子后有爸爸”，甚至是直接利用儿子生出孙子（以学科语言表达的研究结论），而不关心爸爸，这令人百思不得其解。当我们用统计软件包做统计分析时，我们需要明确想做什么、正在做什么，以及想做的是否等于正在做的。事实上只有科学地进行了如此的认知和比较的过程，我们才可能得到科学的分析。再加上作者在第 3 章开头说的，我们还需要在学科语言与统计语言之间的灵活转译。三门语言都十分重要，最不重要的应该是用的最多的计算机语言。为什么？首先，统计学的生命力在于应用，统计学是服务于其他学科的工具学科，从这一点上讲，学科语言是最重要的——身为统计人的我对于统计学没有主要话语权也表示遗憾，但我认为事实就是如此；其次，计算机语言依据统计语言来编写，只有搞懂了统计语言才有可能搞懂计算机语言——否则在统计软件包之间进行切换，会对结果感到一头雾水——若反过来则是本末倒置的笑话。这本质上是一个“质疑xx、理解xx、成为xx、超越xx”的过程。会有人说，时间精力有限，三门语言我不可能同时掌握。或者，会有人试图在有限的时间精力中掌握三门语言，然而这是危险的（如上所述）。这就是统计人的用武之地了：统计学能独立于数学成为单独的一级学科，统计人自身的价值和重要性不言而喻。当统计世界似乎对 R 等开源软件敞开怀抱时，为什么原书要用 SAS. 我十分认同作者给出的理由，还可在作者所言的基础上加一条。SAS 的全称是 Statistical Analysis System，这可能是当今浮躁世界少有的名副其实了，因为它确实称得上是统计分析系统——统计分析、并且非常系统。我不敢保证读者会喜欢本书。首先，读者可能会认为原书作者喜欢说教，而不是一上来就写出模型、抛出示例并甩出代码。但至于读者认不认同“忠言逆耳利于行”，这是读者自己的事情；其次，贯穿全书的软件是闭源收费的 SAS，这与许多应用研究者的想法背道而驰。但作者的意图很明显，我们需要慢下来仔细想想统计方法之原理，统计建模之思想究竟是什么。希望你喜欢，并且希望你在线性模型的世界里玩的开心。有表达不清、错别字、遗漏等任何问题，请联系 Email: 1210683652@qq.com\n王震\n\n广州红花岗畔\n\n2024.05\n","code":""},{"path":"扉页.html","id":"扉页","chapter":"扉页","heading":"扉页","text":"\n广义线性混合模型：现代概念、方法和应用（第二版）介绍了使用广义线性混合模型 (GLMM) 作为总体概念框架的线性建模的更新介绍。对于刚开始接触统计建模的学生来说，这本书有助于他们看到大局——线性建模的广泛理解及其与统计设计和数理统计学的紧密联系。对于在统计实践中经验丰富但刚开始接触 GLMMs 的读者来说，本书提供了对 GLMM 方法及其基础理论的全面介绍。与专注于经典线性模型、广义线性模型或混合模型的教科书不同，本书将上述所有内容都作为统一的 GLMM 家族的成员进行介绍。除了基本的理论和方法外，本书还通过丰富的使用 SAS® 软件的实例来展示 GLMM 的实践。第二版根据 GLMM 从业者所面对的最佳实践和建模选择方面的经验教训进行了更新。新版还增加了两章，重点介绍 GLMMs 的贝叶斯方法。主要特点：大多数统计建模书籍涵盖经典线性模型或高级的广义和混合模型；这本书涵盖 GLMM 家族的所有成员——经典模型和高级模型。结合实践经验与持续研究中的收获，提供了关于最佳实践的最新示例。阐明了统计设计与建模之间的联系：提供了将研究设计转化为适当模型的指导原则，并深入说明了如何实施这些指导原则；使用 GLMM 方法来改进规划和设计。讨论了边际模型和条件模型之间的区别，它们旨在解决的推断空间的不同之处，并讨论了每种模型适用的场合。除了基于似然的频率派的估计和推断之外，还简要介绍了 GLMM 的贝叶斯方法。Walt W. Stroup 是统计学荣休教授。他在内布拉斯加大学统计系任教超过 40 年，专门研究统计建模和统计设计。他是美国统计协会的会员，内布拉斯加大学杰出教学和创新课程奖的获得者，同时也是三本关于混合模型及其扩展的书籍的作者或合著者。Marina Ptukhina (Pa---nuh) 博士是惠特曼学院统计学副教授。她对研究的统计建模、设计和分析及其应用充满兴趣。她的研究领域包括统计学在经济学、生物统计学和统计教育中的应用。Ptukhina 在内布拉斯加大学林肯分校获得了统计学博士学位，在德克萨斯理工大学获得了数学理学硕士学位，并在美国国立技术大学“哈尔科夫理工学院”获得了管理学专家学位。Julie Garai 博士是 Loop 公司的数据科学家。她在内布拉斯加大学林肯分校获得了统计学博士学位，并在多恩学院获得了数学和西班牙语的学士学位。Garai 博士积极与统计学家、心理学家、生态学家、森林科学家、软件工程师以及学术界和工业界的商业领袖合作。在业余时间，她喜欢与她的狗悠闲地散步，和孩子们一起参加舞会、吹长号。","code":""},{"path":"目录.html","id":"目录","chapter":"目录","heading":"目录","text":"前言\n第一版前言\n第二版前言\n第一版前言第二版前言第一篇：基本背景建模基础\n1.1 什么是模型\n1.1.1 两处理均值模型\n1.1.2 线性回归模型\n1.1.3 最终评注\n\n1.2 模型的替代形式\n1.2.1 两种线性预测器形式：单元格均值和效应\n1.2.2 两种模型形式：模型方程和概率分布\n1.2.3 说明模型方程形式缺点的转折\n1.2.4 非高斯数据建模的替代方法——初始概念\n\n1.3 模型效应的类型\n1.3.1 分类与直接效应\n1.3.2 固定与随机效应\n\n1.4 以矩阵形式编写模型\n1.4.1 仅固定效应模型\n1.4.2 混合模型：具有固定效应和随机效应的模型\n\n1.5 小结：模型完整陈述的必要元素\n练习\n1.1 什么是模型\n1.1.1 两处理均值模型\n1.1.2 线性回归模型\n1.1.3 最终评注\n1.1.1 两处理均值模型1.1.2 线性回归模型1.1.3 最终评注1.2 模型的替代形式\n1.2.1 两种线性预测器形式：单元格均值和效应\n1.2.2 两种模型形式：模型方程和概率分布\n1.2.3 说明模型方程形式缺点的转折\n1.2.4 非高斯数据建模的替代方法——初始概念\n1.2.1 两种线性预测器形式：单元格均值和效应1.2.2 两种模型形式：模型方程和概率分布1.2.3 说明模型方程形式缺点的转折1.2.4 非高斯数据建模的替代方法——初始概念1.3 模型效应的类型\n1.3.1 分类与直接效应\n1.3.2 固定与随机效应\n1.3.1 分类与直接效应1.3.2 固定与随机效应1.4 以矩阵形式编写模型\n1.4.1 仅固定效应模型\n1.4.2 混合模型：具有固定效应和随机效应的模型\n1.4.1 仅固定效应模型1.4.2 混合模型：具有固定效应和随机效应的模型1.5 小结：模型完整陈述的必要元素练习设计要务搭建舞台第二篇：估计和推断理论GLMM 之前的估计和推断基础知识GLMM 估计推断（一）推断（二）第三篇：应用处理和解释变量结构多水平模型最佳线性无偏预测计数率和比例零膨胀和栅栏模型多项数据事件时间数据平滑样条曲线和加性模型相关数据（一）：重复测量相关数据（二）：空间变异性GLMM的贝叶斯实现五个贝叶斯 GLMM 示例精度、功效、样本量和计划","code":""},{"path":"secpre.html","id":"secpre","chapter":"前言","heading":"前言","text":"","code":""},{"path":"secpre.html","id":"secpre1","chapter":"前言","heading":"第一版前言","text":"曾几何时，“线性模型”的意思是 \\(\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\)。事实上当 \\(\\mathbf{e}\\) 假定为具有高斯分布时，\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 通常称为“一般”线性模型 (“General” linear model).曾几何时，已不复存在。按照当代标准，\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 只不过是一个特例。称其为“一般”似乎有些奇怪。这必然具有误导性。“线性模型”现在意味着：一个线性预测器 (linear predictor) \\(\\mathbf{\\symbf\\eta=X\\symbf \\beta+Zb}\\)，其中 \\(\\mathbf b\\) 是随机向量以 \\(\\mathbf b\\) 为条件的观测的分布（或至少是准似然）不必是高斯的一个连接函数 (link function) \\(\\mathbf{\\symbf\\eta}=g\\left(\\mathbf{\\symbf\\mu}\\mid\\mathbf{b}\\right)\\) 其中 \\({\\symbf\\mu}\\mid\\mathbf{b}\\) 是给定 \\(\\mathbf b\\) 的观测的条件期望。我们将线性模型拟合到连接函数，而不是直接拟合到数据除了高斯数据，残差项 \\(\\mathbf{e}\\) 没有任何作用。事实上，对于许多分布，它没有有效的定义换言之，截至 2010 年，“线性模型”是指“广义线性混合模型” (generalized linear mixed model, GLMM)，所有其他线性模型——线性混合模型（高斯数据的混合模型）、广义线性模型（线性预测器中不含 \\(\\mathbf{Zb}\\) 的用于非高斯数据的线性模型）以及线性模型（原先称为“一般”线性模型——线性预测器中不含 \\(\\mathbf{Zb}\\) 且仅针对高斯数据的线性模型）——都被视为 GLMM 的特殊情况。在过去的二十多年里，GLMM 课程，或线性混合模型或广义线性模型课程，已作为线性模型的第二门课程来教授，而第一门课程几乎完全针对“一般”线性模型。我在这个行业工作的时间越长，我就越坚信我们需要重新审视这种方法。我相信，我们正在做的是在刚开始学习统计学的研究生中牢牢地嵌入“\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 思维定式”，这恰恰是对当代线性模型错误的理解，并且对于许多学生而言，这似乎会干扰他们重新学习处理随机效应和非高斯数据的线性模型的能力，尽管这些模型仍适用于高斯固定效应模型。我有一位同事的座右铭是“永远不要故意教你以后必须不教的东西。” GLMM 课程的很大一部分内容都是在纠正学生学习 “\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 思维定式” 时积累的不良思维习惯。本教科书的前提是我们可以而且应当做得更好。因此，我尝试将本书写为线性模型的入门教材，而非用于线性模型第二门课程或 GLMM 专题课程的 GLMM 教材——尽管它也可用于这两类课程。我认为，初识线性模型的学生首先需要了解的是整体情况。我们试图实现什么目标？它如何与统计学的核心课程相契合？统计建模者需要考虑的主要问题有哪些？这些问题，连同随机模型效应和非高斯数据（这些也是本书的重要部分，而非留待日后讨论），应该从第一天起就主导我们的讨论。为什么这很重要？请考虑以下示例。想象一项研究，旨在比较两种治疗方案——方案“0”（“对照”方案）和方案“1”（“试验”方案）。研究兴趣在于有利结果的发生率。研究人员采用配对比较设计，其中每个配对中每种治疗方案都观察了 100 名个体，且每次观察都是一个二元响应——即给定个体的结果要么是“有利”的，要么是“不利”的。假设观察结果相互独立，那么每种配对-治疗方案组合的有利结果数量将服从二项分布，其中 \\(N=100\\)，有利结果的概率用 \\(\\pi_{ij}\\) 表示，这里的 \\(\\) 表示治疗方案，\\(j\\) 表示配对。该数据将在本前言的末尾提供。任何统计分析的目的都是估计 \\(\\pi_{ij}\\)，并利用这些估计来比较治疗方案，这是一项涉及到统计建模的工作。一种方法是采用正态近似。当 \\(N=100\\) 时，这似乎是合理的。基础统计学课程通常会用逐渐增大的 \\(N\\) 值来演示二项分布，以此验证中心极限定理。对二项分布进行重复抽样得到模拟数据的直方图，在 \\(N=100\\) 且 \\(\\pi\\) 在 0.1 到 0.9 之间时，几乎无法与正态概率密度函数区分开来。即使 \\(\\pi\\) 等于 0.05 或 0.95，在这些模拟中，正态近似在视觉上也是令人信服的。采用正态近似，得到的线性模型可描述为：响应变量：令 \\(p_{ij}=y_{ij}/{100}\\) 表示第 \\(\\) 个治疗和第 \\(j\\) 对的样本比例，其中 \\(y_{ij}\\) 表示 \\(100\\) 个观测结果中的有利结果数。模型为 \\(p_{ij}=\\mu+\\tau_{}+\\rho_{j}+e_{ij}\\)，其中 \\(\\mu\\) 表示总体均值或截距，\\(\\tau_i\\) 表示第 \\(\\) 个治疗效应，\\(\\rho_j\\) 表示第 \\(j\\) 对的效应，\\(e_{ij}\\) 表示残差并假定为 \\(\\text{..d } N(0,\\sigma^2)\\)。我们知道，该模型得出的结果与考虑配对和治疗效应的方差分析相同，而方差分析的结果又与基础统计学课程中教授的配对 \\(t\\) 检验相同。在此，样本比例的治疗均值，对于治疗“0”为 0.738，治疗“1”为 0.895，标准误为 0.062. 对于等治疗比例的检验，\\(p\\) 值为 0.1132——按照大多数标准来看，这不足以得出治疗对有利结果的预期发生率具有统计显著性的结论。上述分析有一个明显的问题：尽管采用正态近似，但响应变量仍然是二项分布，这意味着方差取决于 \\(\\pi\\)。假定 \\(\\pi\\) 因治疗方案（或配对）而异，那么等方差假设自然就不成立。“传统”线性模型的“补救”要求采用方差稳定的转换方法。对于二项分布的响应变量，标准的转换方法是反正弦平方根变换。使用该方法，我们定义一个转换变量，\\(p_{ij}^*=\\sin^{-1}\\left(\\sqrt{p_{ij}}\\right)\\)，并为其拟合上面相同的模型，即 \\(p_{ij}^*=\\mu+\\tau_i+\\rho_j+e_{ij}\\)，此时对于等治疗比例的检验，\\(p\\) 值为 0.0605. 然后对每种治疗的平均 \\(p_{ij}^*\\) 进行逆变换，对于治疗“0”得到 0.760±0.067，治疗“1”为 0.916±0.040.此时对于这个例子，我们已经达到了“一般”线性模型的极限。在 20 世纪 70 年代，这就是统计学艺术的现状。三十多年后，两个相辅相成的发展极大地改变了这一领域：线性混合模型和广义线性模型理论及方法的发展起初是平行的，后来则越来越交织在一起，同时计算机能力也在快速且持续地提升。因此，到了 2010 年，在这个例子中，广义线性混合模型会是什么样的？它会得出什么结果？所有这些发展有多重要？传统线性模型方程形式为“观测值 = (解释变量之和) + 残差以及残差的概率假设”，广义线性混合模型与之不同，其涉及三个部分：观测值的分布：此处 \\(y_{ij}\\mid\\rho_j{\\sim}\\text{Binomial}\\left(100,\\pi_{ij}\\right)\\)线性预测器：这里 \\(\\eta_{ij}=\\eta+\\tau_i+\\rho_j\\) 其中 \\(\\eta\\) 表示截距，其他项定义如前。请注意，没有残差项——这不是疏忽，而是就是如此。更多内容从第 1 章开始。如果这些配对代表了更大的总体（通常是这样），那么模型必须包括它的假定分布。这里，假定配对效应 \\(\\rho_j\\) 是 \\(\\text{..d } N(0,\\sigma^2_P)\\)连接函数：对于非正态数据，分布的对数似然的典型参数 (canonical parameter) 通常比均值本身更适合拟合线性模型。对于二项分布，典型参数为 \\(\\log\\!\\left(\\frac{\\pi}{1-\\pi}\\right)\\)，因此连接函数为 \\(\\eta_{ij}=\\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right)\\)。当你估计线性模型的参数时，你就估计了连接函数——对于本例为对数几率 (log-odds). 因此，\\(\\pi_{ij}\\) 的估计为 \\(\\hat{\\pi}_{ij}=1\\Big/\\left(1+e^{-\\hat{\\eta}_{ij}}\\right)\\)对于本例，广义线性混合模型在检验两种治疗有利结果的等发生率时得到了 0.0355 的 \\(p\\) 值。有利结果发生率对于治疗“0”和治疗“1”分别为 0.781±0.072 和 0.928±0.030.现在我们可以更具体地表述“为什么要用 GLMMs”的问题。本例中的三项分析对每种治疗有利结果的发生率得到了不同的估计。其中，只有 GLMM 得出的估计是对真实二项概率的估计。其他的则不然——尽管人们通常认为它们是。理解原因需要理解 GLMM 理论。GLMMs 有一个“你跑得了但躲不了”的方面。使用 ANOVA 或“一般”线性模型方法并不能免除 GLMM 相关问题——它只会让你忘记它们的存在。这三种分析还得出不同的 \\(p\\) 值。事实上，所有三种方法都大致同等地控制了 类错误。然而，GLMMs 有更大的能力将模型效应识别出统计显著性。本例中 \\(p\\) 值之间的差异说明了线性模型理论的预测。这是统计专业人员培训中必不可少的核心理解。“为什么要用 GLMMs？”。因为它明确地估计你认为它要估计的东西，而不是其他东西。如果存在非零的治疗效应，GLMM 至少具有与传统“一般”线性模型一样大的功效来识别它——当数据不呈正态分布时功效更强。本书的目的是帮助你理解原因。下表是我对 Locke 和 Devoe 在 2007 年 USCOTS 会议上所做报告的进一步阐述，提供了额外视角。当我们将线性模型描述为“响应变量 = (解释变量列表) + 随机项”时，我们需要仔细思考我们实际上在说什么。我们知道，在当代统计实践中，响应变量有多种形式。上表提供了各种响应类型的概念，但并非详尽无遗。我们还知道，模型的右侧，即“解释变量+随机项”，涉及的也不仅仅是处理+误差。“一般”线性模型实际上将注意力局限于该矩阵的两个单元格上，并没有暗示当响应和模型位于该矩阵的其他 182 个单元格时，尝试使用这种线性建模的有限观点所引发的问题（例如二项配对比较的示例）。","code":""},{"path":"secpre.html","id":"本书的编排","chapter":"前言","heading":"本书的编排","text":"我把这本教科书分为三个部分——在我的线性模型课程中，我称之为“篇”。第一篇“大局问题”。第一章涉及“什么是模型，模型应该做什么，模型的基本元素是什么？”最后介绍了如何用矩阵语言建立模型。第二章基于我多年来的观察，即建模困难在很大程度上源于设计问题。即使是样本调查和回顾性研究也涉及设计概念，尤其是当目标是评估预测变量对响应的效应时。如果不了解设计原则和建模之间的相互作用，就无法进行良好的建模。第 2 章重点介绍有助于澄清这种相互作用的技术。第 2 章介绍了两种根据数据集结构构建模型的技术。一种是我称为“实验单元是什么”的方法——它借鉴了Milliken Johnson’s Analysis Messy Data, 2nd ed. (2008) 的思想。Dallas Johnson 在 Conference Applied Statistics Agriculture 的休息时间主持了一个名为 “Stump Statistical Consultant” 的游戏——“实验单元是什么”模板是对他和 George Milliken 的方法的松散改编。另一种方法我称为“Fisher 会怎么做？”，这受到 Terence Speed 发表在 2010 年 5 月 IMS Bulletin 上的题为 “ANOVA Thing” 的专栏的启发。Speed 描述了方差分析和统计建模的不安共存，以及 Fisher 对早期统计建模工作的非常消极的接受（用 Speed 的话来说是“愤怒”和“困惑”）。Speed 建议阅读 Fisher 在 Yates (1935) 发表于 RSS 的讨论论文 “Complex Experiments.” 之后的评论。阅读 Fisher 的评论后，我恍然大悟：原来如果正确理解了 ANOVA 的思维过程，它就可以成为构建适当 GLMM 的有效工具。另一方面，对 ANOVA 的错误理解——将其视为与普通最小二乘法和二次型相关的算术练习——是“\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 思维定式”阻碍理解当代建模的绝佳例子。我不知道 Fisher 如果真的运用现代理论和计算技术会做什么，但很容易相信，他之所以对建模感到苦恼，是因为建模最终走向了“一般”线性模型，而 Fisher 的直觉告诉他，这种方法的限制性太强了。第 3 章是后来加入的，但可能是书中最重要的章节。它涵盖了在“一般”线性模型中根本不会出现的推断问题。前两个问题——广义线性模型的数据尺度与模型尺度以及混合模型的广义与狭义推断——已经在其他文献中讨论过，尽管这些问题仍然没有得到广泛的理解。第三个问题——条件推断和边际推断——是统计学界刚刚开始认识到的问题。让我说得更强烈一些。真正可用的 GLMM 软件才出现几年——例如，SAS®PROC GLIMMIX 直到 2005 年才出现——统计学界才刚刚开始理解条件-边际问题的影响。这是本前言中二项示例的根源。我们越深入探索这个问题，就越有说服力地证明，当我们在培训中向统计专业人员介绍线性模型时，应该首先介绍 GLMM 的概念。第二篇这些是涉及估计方程和推断的章节。第 4 章为广义线性模型和线性混合模型开发估计方程，然后将它们“结合”为广义线性混合模型。总体方法是基于似然的。会实时引入所需的矩阵运算，例如矩阵的广义逆、矩阵导数。第 5 章开发了推断工具——标准误、检验统计量、自由度近似、偏差校正（如 Kenward–Roger 调整）、三明治估计等。与第 4 章一样，所需的分布理论以及 GLMMs 中出现的二次型，是实时引入的。随着这些章节的深入展开，我们会发现传统线性模型初级课程中的内容，即“一般”线性模型，很大程度上是由更为全面且包容广泛的线性模型理论的特殊情况构成的。我的经验表明，这部分内容在第一篇之后讲解更加合适。过去，当在课程初期就介绍估计方程时，学生们常常会说：“这些内容虽好，但我完全不明白我们为什么要这么做。相比于上学期我们学过的‘一般’线性模型课程内容，这些估计方程太复杂了。”而调整后的教学顺序得到了更好的反馈和效果。第三篇这些章节重点介绍 GLMMs 的常见应用。这包括高斯和非高斯数据的多因素裂区、聚类和多水平模型、重复测量和空间数据的相关误差模型、计数数据和过度离散、二分类数据出现的特殊问题、多项分类数据的模型、以及 GLMM 的其他应用。本书的这一部分必然是一项概述。关于 GLMM 的特定方面——如生存分析、分类数据、空间统计学等——有专门课程讲授。本书无意替代或与任何这些主题的深入教科书竞争。它旨在作为一本入门书，为新手提供足够的 GLMMs 知识，以便他们能够处理常见的 GLMMs，并在需要时学习或自学更专业的主题。本篇中的一个重要章节涉及功效和样本量。关于实验设计的传统观念在很大程度上植根于高斯数据的传统。用于功效和样本量计算的商业软件基本没有受到 GLMM 概念的影响。因此，基于传统设计观念或商业功效和样本量软件做出的功效、样本量和设计决策可能是不恰当的——有时甚至会造成灾难性的后果——尤其是在计划的研究涉及非高斯响应变量和随机模型效应时。GLMM 理论在设计规划和功效及样本量计算中的应用并不难——本章将展示如何做到这一点。","code":""},{"path":"secpre.html","id":"本书使用的符号","chapter":"前言","heading":"本书使用的符号","text":"撰写一本关于 GLMMs 的教科书，就像在用符号串起来的钢丝上行走。一方面，从“一般”线性模型文献中继承的惯例或多或少都是标准的。对于任何从事统计模型工作的人来说，它们都是熟悉的。另一方面，一些继承的惯例并不适用于 GLMMs，或者更糟糕，它们往往会强化“\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 思维定式”——这种思维定式会阻碍学习和真正理解 GLMMs.我觉得有必要在这里提及四个有关符号的决定，以提醒读者并解释为什么经过几年的尝试和错误，我在本书中使用它们。我遵循 Searle 的惯例，在可能的情况下，用希腊字母表示固定模型效应，用拉丁字母表示随机模型效应。例如，表达式 \\(\\eta+\\alpha_i+b_j+\\left(ab\\right)_{ij}\\) 意味着因子 是固定效应（\\(\\alpha_i\\)），因子 B 是随机效应（\\(b_j\\)），因此 ×B 交互作用 \\(ab_{ij}\\) 也是随机的。此外，我更喜欢对构成交互作用的因子使用的相同符号（在需要时从希腊字母切换到拉丁字母，如从 \\(\\alpha\\) 切换为 \\(\\)），而不是引入新符号。我认为这样更便于读者理解。粗体字符若为小写则为向量，若为大写则为矩阵。例如，\\(y\\) 是一个标量随机变量；\\(\\mathbf y\\) 是一个随机向量；\\(X\\) 是一个标量；\\(\\mathbf X\\) 是一个矩阵；\\(\\beta\\) 是一个固定效应参数；\\(\\symbf\\beta\\) 是一个参数向量。线性模型教材几乎无一例外地将方差分析类型的模型方程表示为（例如） \\(\\mu+\\alpha_i+b_j+\\left(ab\\right)_{ij}\\)，其中 \\(\\mu\\) 表示截距，或者如果对其他参数施加适当的约束，则表示总体均值。对于非高斯数据的广义模型，这会令人困惑。例如，对于二项数据，对于第 \\(ij\\) 个 ×B 的组合的典型连接为 \\(\\eta_{ij}=\\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right)\\)。连接函数的标准表示法是 \\(\\eta_{ij}\\)。线性预测器（GLMM 中模型方程的类似物）为 \\(\\eta_{ij}=\\text{intercept}+\\alpha_i+b_j+\\left(ab\\right)_{ij}\\)。多年来，无论分布和连接函数如何，我都使用 \\(\\mu\\) 表示截距。但是听到有人说“\\(\\eta_{ij}\\) 是 \\(\\operatorname{logit}\\) 连接……其中 \\(\\mu\\) 是总体均值”后，我最终还是绝望了。连接尺度上的截距不是总体均值，但很明显该符号与信息相悖。当我开始使用 \\(\\eta\\) 来表示截距时，如 \\(\\eta_{ij}=\\eta+\\alpha_i+b_j+(ab)_{ij}\\)，学生们反馈说它远没有那么令人困惑。该符号贯穿本书。在整本书中，我使用通用符号 \\(\\mathbf{\\symbf\\eta=X\\symbf\\beta+Zb}\\) 来表示 GLMM 线性预测器的矩阵表示。一些教材使用 \\(\\symbf\\gamma\\) 作为随机效应向量，但这违反了希腊语/拉丁语规则。一些教材使用 \\(\\mathbf u\\) 作为随机效应向量，但在介绍 GLMM 的边际分布时变得尴尬。此外，学生们告诉我，当我在黑板上写字时，我的手写字母 \\(\\symbf\\mu\\) 和 \\(\\mathbf u\\) 很难区分，但我的 \\(\\symbf\\beta\\) 和 \\(\\mathbf b\\) 容易区分。Harville 使用 \\(\\symbf\\beta\\) 和 \\(\\mathbf b\\) 并且效果似乎很好。","code":""},{"path":"secpre.html","id":"为什么用-glimmix","chapter":"前言","heading":"为什么用 GLIMMIX？","text":"有人问我，当统计世界似乎正朝着 R 的方向发展时，为什么要将本书中的例子锚定在 SAS®PROC GLIMMIX 中。原因有三点。第一。这本教科书的一部分基础源于 Oliver Schabenberger 与我在 2008 年 Joint Statistical Meetings 上共同讲授的两日短期课程。Oliver 是 GLIMMIX 程序的开发者，多年来，他与我一直在思考 GLMM 课程应该是什么样的。嵌入在 GLIMMIX 中的思维过程反映了 Oliver 和我对 GLMMs 的看法。其次，我已经是 SAS 30 多年的老用户了，并从 2001 年到 2010 年担任系主任。当我同意写这本教科书时，我对 R 的回应是：“在任何给定的时间段内，我只能应对有限的学习曲线——如果你们希望尽快看到这部教科书，那么将以 GLIMMIX 为基础，而不是 R 语言。”第三点也是最重要的一点，GLIMMIX 有一种语法，一旦学会，就能以一致的（并且我认为是相当直接的）方式应用于本书中涵盖的所有 GLMMs. 另一方面，R 针对不同方法有不同的程序，对于某些方法是否具备相应程序也不够清晰，所有这些都没有形成一种将模型转化为软件指令的连贯思维架构。本书的目标是将包含随机模型效应和非高斯数据的线性模型的主要思想和方法论介绍给广大读者，这包括统计学研究生、寻求更新的统计学专业人士以及刚接触广义线性模型思维过程的研究人员。我希望思想是核心，而软件只是辅助而不是成为阻碍或问题本身。SAS 仍然是统计计算世界的通用语言。特别是 GLIMMIX，它是减少对计算细节的关注，并最大限度地关注 GLMM 概念和方法的最佳方式。顺便说一句，我忍不住要发表一下个人看法，我现在认为 GLIMMIX 既是一款软件，也是一件艺术品。它显然是为了辅助、支持和强化 GLMM 的学习过程而编写的。","code":""},{"path":"secpre.html","id":"目标读者以及如何使用本书","chapter":"前言","heading":"目标读者以及如何使用本书","text":"就目标受众而言，我只能重复我前两段写的内容：我的目标受众是统计学研究生、希望跟上时代步伐的统计学专业人士，以及刚开始接触广义线性模型思维过程的研究人员。读者应具备线性代数的基础知识，对概率分布理论和基于似然的估计与推断有所了解，以及具备应用统计学的基础知识，特别是方差分析和回归分析。特别是，我写这本书的目的就是为了作为线性模型的第一门课程，我认为我们需要重新思考当前向学生灌输“\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 思维定式”的范式，然后——可能的话——再开设一门添加了“广义”和“混合”概念的线性模型课程。如果你的研究生课程（就像我曾经的一样）——被锁定在“\\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 优先”的路径中，这本书也可用于第二门以 GLMM 为重点的线性模型课程。此外，我也将本书写为一部参考书和继续教育教材，面向那些希望初步了解或想要深入了解广义线性模型、混合效应模型以及广义线性混合模型的研究者、实践者以及具备定量素养的非统计学家群体。如果你用这本书作为线性模型的第一门课程，我认为前两篇是必不可少的。如果你想偷工减料，那就在第三篇里偷工减料吧。如果你把这本书作为 GLMM 的参考，我认为第一篇和第三篇是必不可少的。专注于估计和推断理论的第二篇，可按需参阅。我认为在任何情况下都不可省略的是第一篇。正如我之前提到的，我们才开始意识到，当 \\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 是“一般”线性模型时，我们从未意识到存在一些基本的线性模型问题。我们才开始意识到，我们需要解决这些问题。我认为可以肯定地说，即使在统计学界内，我们也没有对这些问题形成普遍的认识，更不用说在更广泛的研究界了。在我看来，前三章可能是本书最重要的章节。玩的开心！数据集 0.1 配对比较的二项数据","code":"/* Binomial response variable */\n/* Treatment_0 is # favorable outcomes out of 100 for Trt=0 */\n/* Treatment_1 is # favorable outcomes out of 100 for Trt=1 */"},{"path":"secpre.html","id":"sec2","chapter":"前言","heading":"第二版前言","text":"距本书第一版的编写已经过去十二年了。第一版的编写有两个主要目标：作为 GLMM 线性模型家族的全面介绍，用于研究生水平的统计建模和统计方法课程，以及新接触广义线性混合模型的统计学家。为那些使用 GLMMs 进行数据分析或研究的人员提供参考。在本版中，我们力求忠于这些目标，同时还纳入：使用本书教授 GLMM 相关课程的经验教训。使用 GLMMs 的经验，特别是通过大学研究和工业研发早期阶段常见的小型数据集所获得的关于 GLMM 估计和推断行为的知识。在第一版编写时还未受到较高关注的与 GLMMs 相关的主题。该版本得益于两位合著者的加入，而他们也曾是受益于第一版的学生。他们不仅从学生视角出发，利用本书学习统计建模，还在毕业后积累了丰富的教授统计方法课程的经验，因此他们的贡献极具价值。为了更新本书所涵盖的材料并使其对学生和读者更加友好，我们添加了新的章节并大幅重写了其他章节。首先介绍一下新的章节。第 4 章。该章是对经典线性模型术语、估计和推断理论以及方法论的概述。在编写第一版时，我们假定读者已修读过线性模型的第一门课程。这个假定与我们的初衷相悖，即让本书成为对 GLMM 家族中所有线性模型——仅含经典固定效应的线性模型、广义线性模型以及高斯和非高斯数据的混合模型——的全面介绍。在将第一版用于统计学专业研究生课程的全套建模序列时，很明显我们需要在介绍 GLMM 估计和推断之前，增加一章用于介绍基础知识。第 ?? 章。第一版在计数数据章节中提供了零膨胀和栅栏 (hurdle) 模型的示例。在接下来的几年里，我们遇到了更多不同的例子，包括使用计数以外数据类型的应用。一个特别重要的应用是连续比例。我们决定在单独的章节中介绍零膨胀和栅栏模型。第 ?? 章。一位对第一版提出批评的评论家遗憾地指出我们遗漏了加性模型。在使用第一版的过程中，我们逐渐认同了他的观点。从技术上讲，加性模型并不是 GLMM 家族的一个子集，但涉及平滑样条的应用会以重要方式增强 GLMM 的方法论。我们增加了本章来满足这一需求。第 ?? 章和第 ?? 章。在编写第一版时，我们有意排除 GLMMs 的贝叶斯方法。但现在忽视 GLMMs 的贝叶斯方法变得越来越困难。在许多 GLMM 的应用场景中，贝叶斯方法可为基于似然的方法的局限性提供了有效的解决方案。虽然我们并不打算深入介绍“贝叶斯与 GLMM”，但我们确实提供了一个介绍和一些示例，以让读者了解贝叶斯与 GLMM 之间的联系。我们鼓励学生通过参加专门讲解贝叶斯统计学的课程来深入学习。经大幅修订的章节如下。第 1 章和第 2 章。第 1 章介绍了统计建模，并定义了 GLMM 家族的成员。第 2 章向读者介绍了将数据集描述转化为适当模型的过程。这两章经过了大量的修订，以体现我们在运用本书对统计学专业的学生以及作为统计方法应用者的各学科的研究生教授统计方法和统计建模时，哪些教学内容效果良好，哪些不尽如人意。在修订的第 2 章中，我们特别强调了一个过程，我们发现该过程能使用户可靠地将数据集描述转化为统计模型——这通常是初学者面临的一个大问题（有时即使是经验丰富的数据分析师也会觉得这是一个棘手的问题）。我们还增加了一个更清晰的准则列表，用以区分“合理”的模型与明显不适合特定情境的模型。第 ?? 章和第 ?? 章。这两章分别介绍了计数数据和比例数据，共同构成了 GLMM 应用入门的核心介绍。在编写第一版后的十多年里，我们积累了大量的经验。这包括一些在 2011 年普遍认为“最佳”的实践，但经验和持续的研究证明并非如此。一个核心问题是何时使用 SAS 默认的伪似然算法，何时使用积分近似算法，尤其是 Laplace 算法和自适应求积算法。2011 年的普遍观点是：“尽可能使用自适应求积算法。”但自从 Elizabeth Claassen 的论文 (2014) 开始，我们遇到了越来越多不符合这种情况的案例，以 Stroup 和 Claassen 于 2020 年发表在 Journal Agricultural, Biological, Environmental Statistics 上的文章 “Pseudo-Likelihood Quadrature? Thought Knew, Think Know, Still Trying Figure ” 作为高潮。虽然有许多情况可以使用求积法，但绝对不是最佳方法。第 ?? 章和第 ?? 章已经过修订，以反映我们目前的理解。有两章增加了额外的例子，以补充我们认为在第一版中的遗漏。第 ?? 章增加了一个例子，基于 Green, Fellers Stroup 于 2017 年在 Statistics Public Policy 上发表的文章 “Defining Program Effects: Distribution-Based Perspective”. 第 ?? 章增加了两个关于多项数据的例子，一个是有序数据的例子，其中的标准比例几率模型——通常是 GLM 软件中处理多项数据的唯一选项——显然不合适；另一个是名义数据的例子，在第一版中遗漏了。回想起来，这是一个错误。关于软件，有一个反复出现的问题：为什么所有的例子都使用 SAS 程序？为什么在第二版中继续使用 SAS 而不是 R 呢？有两个回答。首先，SAS 有一个名为 PROC GLIMMIX 的单一程序，其功能允许我们实现本书中讨论的大多数应用，并允许用户选择伪似然、Laplace 和自适应求积算法。PROC GLIMMIX 无法实现的一些示例可以使用 PROC NLMIXED 来实现。这些程序的语法是一致的——一旦掌握，就能让读者专注于 GLMM 的概念、方法和应用。相反，R 拥有超过 80 个包，每个包仅实现了 GLMM 家族中的一部分模型，而非全部。至今为止，没有哪个 R 包能接近 PROC GLIMMIX 的多功能性。不同的 R 包采用不同的语法，其中一些语法相当独特。尽管存在 80 多个 R 包，但仍有一些 GLMM 模型目前无法通过任何 R 包来实现但可通过 SAS 的 PROC GLIMMIX 实现。有关此问题的深入探讨，读者可参考 Julia Piaskowski 的网站，提供了题为 “Mind Gap: Mixed Models R - done sprawling mixed model ecosystem R?” 的文章。在介绍贝叶斯方法的章节中，SAS 的 BGLIMM 和 MCMC 等程序分别采用了 GLIMMIX 和 NLMIXED 的语法结构，从而进一步减轻了读者应对软件语法的负担。简言之，我们之所以没有选择使用 R，是因为不想让众多具有各种不同语法约定和模型限制的 R 包成为读者理解和学习的障碍以及分散注意力的因素。同时，我们也不希望因为 R 中缺乏实现某些模型的能力而不得不省略相关实例。最后，尽管 Stroup Claassen 在 2020 年发表于 JABES 上的文章提出了发现和建议，但至今为止，据作者所知，仍未有 R 包实现了伪似然算法。我们鼓励对 R 感兴趣的读者为本书中的示例编写 R 程序。若能将这些程序公开共享，将会是对 GLMM 世界的一大贡献。最后，有读者询问我们是否能为本书中的习题提供答案。我们的回答是“否”，这主要是因为，正如我们在本书中多次提到的那样，基于 GLMM 的习题通常有多种合理的答案。我们担心答案会限制读者的想象力。我们非常欢迎读者就“这种方法是否合理”等问题与我们联系，但我们不想提供可能导致思维受限的东西。","code":""},{"path":"chap1.html","id":"chap1","chapter":"第 1 章 建模基础","heading":"第 1 章 建模基础","text":"","code":""},{"path":"chap1.html","id":"sec1-1","chapter":"第 1 章 建模基础","heading":"1.1 什么是模型","text":"模型是对一组观测背后的产生过程的一种数学描述。统计模型包括一个描述解释变量假定影响的方程，并描述了我们假设的具有随机变异性质的过程各个方面相关的概率分布。遵循 McCullagh Nelder (1989)，我们可以使用如下的通用形式来编写模型\n为了解其工作原理，请考虑所有初学统计学的学生都熟悉的两个模型：两处理均值模型以及线性回归模型。","code":""},{"path":"chap1.html","id":"sec1-1-1","chapter":"第 1 章 建模基础","heading":"1.1.1 两处理均值模型","text":"比较两均值的统计模型可写为\\[\\begin{align}\ny_{ij}=\\mu_i+e_{ij}\n\\tag{1.1}\n\\end{align}\\]其中各项定义如下：下标 \\(= 1,2\\) 标识处理\\(j= 1,2, \\ldots,n_i\\) 标识第 \\(j\\) 个观测\\(n_i\\) 是第 \\(\\) 个处理的观测数\\(y_{ij}\\) 表示第 \\(\\) 个处理的第 \\(j\\) 个观测\\(\\mu_i\\) 表示第 \\(\\) 个处理的均值\\(e_{ij}\\) 表示与第 \\(ij\\) 个观测相关的随机残差\n许多统计文章和教科书将 \\(e_{ij}\\) 称为随机“误差” (error). 我们更喜欢“残差” (residual) 这个词，因为它避免了“错误” (error) 的贬义含义，即观测受到数据收集过程中所犯错误的影响。“残差”一词在开发广义线性模型构造的逻辑时也很有帮助。模型的第一个组成部分，即式子的左侧，为观测 (observation). 对于模型 (1.1)，它是 \\(y_{ij}\\)。统计模型的另外两个组成部分构成了式子的右侧。模型 (1.1) 的系统性 (systematic) 部分是 \\(\\mu_i\\)，即观测的均值。或者，我们可以将 \\(\\mu_i\\) 称为模型的确定性 (deterministic) 部分，因为它是由处理 \\(\\) 的应用决定的。无论我们称之为确定性还是系统性，模型的这一部分都是数学定律，不受随机变异性的影响。在混合模型术语中，模型的确定性部分称为固定效应 (fixed effects).模型 (1.1) 的随机 (random) 部分是 \\(e_{ij}\\)。它告诉我们，假定观测围绕其均值随机变异。项 \\(e_{ij}\\) 是对第 \\(\\) 次观测的独特性的全面表征。理论上，至少根据一些哲学流派的说法，如果我们知道每个观测的独特之处，我们就可以写出一个完全确定的模型。然而，我们不能。相反，我们描述了这些观测的概率分布。在经典线性模型中，我们假设高斯分布，表示为 \\(e_{ij}\\mathrm{~..d.~}N(0,\\sigma^2)\\)3。对于广义线性模型，我们对模型的随机部分进行了更通用的刻画。","code":""},{"path":"chap1.html","id":"sec1-1-2","chapter":"第 1 章 建模基础","heading":"1.1.2 线性回归模型","text":"现在考虑线性回归模型\\[\\begin{align}\ny_{ij}=\\beta_0+\\beta_1X_i+e_{ij}\n\\tag{1.2}\n\\end{align}\\]其中\\(\\beta_0\\) 为截距\\(\\beta_1\\) 为斜率\\(X_i\\) 为预测变量（又称自变量）的值\\(y_{ij}\\) 表示 \\(X_i\\) 第 \\(\\) 个水平的第 \\(j\\) 次观测\\(e_{ij}\\) 为残差\n线性回归模型类似于 (1.1)。唯一的区别是 \\(\\beta_0+\\beta_1X_i\\) 取代 \\(\\mu_i\\) 成为模型的系统部分。项 \\(y_{ij}\\) 和 \\(e_{ij}\\) 保留了相同的定义和假设。","code":""},{"path":"chap1.html","id":"sec1-1-3","chapter":"第 1 章 建模基础","heading":"1.1.3 最终评注","text":"请记住，统计模型都是近似值。著名爵士乐手 Thelonius Monk 曾说: “wrong note, resolve .” 我们可以这样转述: “right model, work .” 或者，正如 George Box 的名言: “models wrong, useful” (Box, 1976). 即使在这些简单的例子中，也有很多关于我们所观察的单元的信息是我们不知道且不打算深究的。此外，我们也没有关于未从目标总体中抽取的单元的信息。我们必须满足于使用概率分布来近似这些个体之间的变异，并假定我们样本中的单元准确地代表了目标总体。这错了吗？从技术上讲是错的。但它有用吗？用我父亲最喜欢的一句话来回答: “Good enough ’s .”如果目标是比较两个均值（如模型 (1.1)），或者估计斜率和截距，如模型 (1.2)，并且 \\(y_{ij}\\) 是连续的、独立的、关于其均值或多或少地对称分布，并且方差不取决于均值，则本节定义的模型是“足够好的”。统计模型至少必须包括这两个例子中给出的三个元素，即观测；产生观测过程中的系统性或确定性部分；随机部分，包括假定概率分布的陈述。\n介绍性文本往往将重点局限于这三个要素。但正如我们将在以下章节中看到的那样，仅凭这些要素不足以描述当代统计中常用的模型。1.1 节的关键思想：模型的系统性/确定性与随机部分","code":""},{"path":"chap1.html","id":"sec1-2","chapter":"第 1 章 建模基础","heading":"1.2 模型的替代形式","text":"上一节的两个例子使用了我们所说的统计模型的模型方程 (model equation) 形式。也就是说，基本形式是“观测 = 系统部分 + 随机部分”。从历史上看，线性模型的文献中主要采用模型方程的形式。尽管这对于相对简单的模型来说已经足够了，并且对于第一次看到模型的学生来说可能是可以接受的，但对于当代统计中常见情况有用的模型需要额外的考虑。我们将在接下来的两节中对此进行描述。要了解这一点，我们将重新访问模型 (1.1) 和 (1.2)。","code":""},{"path":"chap1.html","id":"sec1-2-1","chapter":"第 1 章 建模基础","heading":"1.2.1 两种线性预测器形式：单元格均值和效应","text":"从两处理模型 (1.1) 开始，即 \\(y_{ij}=\\mu_i+e_{ij}\\)，它可重写为 \\(y_{ij}=\\mu+\\tau_i+e_{ij}\\)，其中 \\(\\mu +\\tau_i= \\mu_i\\)。前者将模型的系统部分表示为处理均值，称为单元格均值 (cell means) 模型；后者将模型的系统部分表示为由处理或预测器效应修正的截距，称为效应 (effects) 模型。两个模型都隐式地描述了产生观测数据的过程。效应模型明确说明了生成观测的所有过程。即，个体是从总体中抽取的，我们假定这些个体在响应变量方面的概率分布是 \\(\\mathrm{..d.~}N\\left(\\mu,\\sigma^2\\right)\\)。然后，我们将个体（可能是随机地）分配到处理 1 或处理 2 中。分配到第 \\(\\) 种处理会使平均响应改变 (“bumps”) \\(\\tau_i\\)。这导致观测的分布为 \\(NI\\left({\\mu}+{\\tau}_i,{\\sigma}^2\\right)\\)，我们将 \\(NI\\) 读作“正态且独立”。在本书的后续内容中，我们将这种分布表示为 \\(y_{ij} \\sim N(\\mu + \\tau_i, \\sigma^2)\\)，我们假定其是独立的，除非另有说明。单元格均值模型意味着分布 \\(y_{ij} \\sim N(\\mu_i, \\sigma^2)\\) 在过程的描述中并不那么明确。它只是说明了将处理结果应用于均值为 \\(\\mu_i\\) 和方差为 \\(\\sigma^2\\) 的高斯分布。虽然不是那么明确，但单元格均值模型在计算上更简单，这在本书后面介绍的许多应用中都是一个很大的优势。无论我们使用单元格均值模型还是效应模型，我们都可看出系统部分都是观测期望值的模型，即 \\(E(y_{ij})=\\mu_i=\\mu+\\tau_i\\)。类似地，线性回归模型 (1.2) 可表示为 \\(y_{ij}\\thicksim N(\\beta_0+\\beta_1X_i,\\sigma^2)\\)。线性回归的系统部分将观测的期望值建模为 \\(E(y_{ij})=\\mu_{}=\\beta_{0}+\\beta_{1}X_{}\\)。请注意，线性回归是一个用 \\(\\beta_0\\) 取代 \\(\\mu\\)、\\(\\beta_1X_i\\) 取代 \\(\\tau_i\\) 的效应模型。","code":""},{"path":"chap1.html","id":"sec1-2-2","chapter":"第 1 章 建模基础","heading":"1.2.2 两种模型形式：模型方程和概率分布","text":"此处的一个要点是，只要我们假定高斯（正态）分布——这是一个重要的规定——就有两种方法来表达统计模型。模型方程形式，即 \\(y_{ij}=\\mu_i+e_{ij}\\) 或等价地 \\(y_{ij}=\\mu+\\tau_i+e_{ij}\\)概率分布形式，即 \\(y_{ij}\\sim N\\left(\\mu_i,\\sigma^2\\right)\\) 或等价地 \\(y_{ij}\\sim N\\big(\\mu+\\tau_i,\\sigma^2\\big)\\)\n模型方程形式在统计文献中更为常见，但当我们尝试对非高斯数据建模时，其局限性使其不可接受。根据本节末尾显而易见的原因，我们在本书的剩余部分强调概率分布形式。","code":""},{"path":"chap1.html","id":"sec1-2-3","chapter":"第 1 章 建模基础","heading":"1.2.3 说明模型方程形式缺点的转折","text":"回到线性回归示例——模型 (1.2) ——但假设对第 \\(ij\\) 个个体的观测来自 \\(n_{ij}\\) 个独立的伯努利试验。也就是说，我们观测到“0/1”、“要么/或者”、“成功/失败”的结果。将“成功”的数量表示为 \\(y_{ij}\\)。令 \\(\\pi_i\\) 表示预测变量为 \\(X_i\\) 的个体每次伯努利试验“成功”的概率。因此，\\(y_{ij}\\thicksim\\mathrm{Binomial}(n_{ij},\\pi_i)\\)。现假设我们欲使用线性回归将 \\(\\pi_i\\) 的变化建模为 \\(X_i\\) 的函数。模型方程法给出 \\(y_{ij}=\\beta_0+\\beta_1X_i+e_{ij}\\)。我们便立马发现了问题：如何解释 \\(e_{ij}\\)？对于高斯观测，在模型 (1.2) 中假定 \\(e_{ij}\\mathrm{~..d.~}N(0,\\sigma^2)\\) 得到 \\(y_{ij}\\thicksim N\\left(\\beta_0+\\beta_1X_i,\\sigma^2\\right)\\)。然而，假定 \\(e_{ij}\\sim\\mathrm{Binomial}\\) 并将其加入 \\(\\beta_0+\\beta_1X_i\\) 并不会得到合理的结果，更不用说 \\(y_{ij}\\thicksim\\mathrm{Binomial}(n_{ij},\\pi_i)\\) 了。从数理统计中我们知道只有高斯分布是可加的。高斯分布有两个不同的参数，均值和方差。类似地，在模型方程中，系统部分和随机部分起着不同的作用：\\(\\beta_0+\\beta_1X_i\\) 代表均值 \\(\\mu_i\\)，\\(e_{ij}\\) 代表方差 \\(\\sigma^2\\)。对于二项观测，\\(\\beta_0+\\beta_1X_i\\) 很可能涉及到成功概率 \\(\\pi_i\\) 的估计，但因为对于二项分布 \\(Var\\left(y_{ij}\\right)\\propto\\pi_i\\left(1-\\pi_i\\right)\\)，我们知道均值的同时也就知道了方差。没有单独的方差可估计，因此残差项在模型中不起作用。这就给我们留下了一个问题：\\(\\beta_0+\\beta_1X_i\\) 是否提供了 \\(\\pi_i\\) 的精确估计？我们可将响应变量重新定义为第 \\(ij\\) 个个体的样本比例 \\(p_{ij}=\\frac{y_{ij}}{n_{ij}}\\)。这给出候选模型 \\(p_{ij}=\\beta_0+\\beta_1X_i+e_{ij}\\)。假定每个观测都有足够的 \\(N_{ij}\\)，我们可援引中心极限定理，并声称 \\(p_{ij}\\) 具有近似的高斯分布。然而，除非 \\(\\beta_1=0\\)——也就是说，对于所有 \\(X_i\\)，\\(\\pi_i\\) 相同——否则我们不能假定等方差 \\(\\sigma^2\\)（为什么不能？）。鉴于我们的目标是估计 \\(X\\) 对 \\(\\pi_i\\) 的影响，这似乎事与愿违。必然存在 \\(\\beta_1\\ne 0\\) 的可能性，否则最初可能根本就不会收集到这些数据。如果我们仍然继续呢？请考虑表 1.1 中所示的示例数据。\n使用模型 \\(p_{ij}=\\beta_0+\\beta_1X_i+e_{ij}\\) 我们得到 \\(\\hat{\\beta}_0=-0.089\\) 以及 \\(\\hat{\\beta}_1=0.1115\\)，与检验 \\(H_0:{\\beta}_1=0\\) 相关的 \\(p<0.0001\\) 以及决定系数 \\(R^2=0.917\\)。乍一看很不错。然而，仔细观察，在 \\(X=0\\) 时 \\(p\\) 的预测值为 -0.089，在 \\(X=10\\) 时则为 1.026. 我们如何看待小于 0 或大于 1 的预测概率？处理这种情况的传统方法是使用方差稳定变换。对于二项数据，标准变换为 \\(\\sin^{-1}\\left(\\sqrt{p_{ij}}\\right)\\)。然而，这种方法有两个问题。首先，它本质上相当于“当你只有一把锤子时，试着让每个问题看起来都像钉子”的建模思维。换句话说，模型方程的形式遵循高斯思维；如果数据不是高斯的，我们必须使它们成为“高斯行为”。其二，它不适用于混合模型，除了 \\(e_{ij}\\)，混合模型还有随机效应。如果我们知道我们有一个二项响应变量，那么最好将其作为二项变量来处理，而不是试图强行将其视为正态变量。应该如何做呢？概率分布形式提供了一种更为可行的方法。","code":""},{"path":"chap1.html","id":"sec1-2-4","chapter":"第 1 章 建模基础","heading":"1.2.4 非高斯数据建模的替代方法——初始概念","text":"使用概率分布方法对二项数据进行建模有许多替代方案。目前，我们认为这两种最常见：检查概率分布以确定适合用于建立线性模型的 \\(\\pi_i\\) 的函数定义一个可以合理产生观测的过程，其中 \\(\\pi_i\\) 随 \\(X_i\\) 变化\n对于这个例子，两种方法都预示了我们将在后续章节中充分展开的广义线性模型。目前，我们重点关注基本概念。方法 1：检查概率分布二项随机变量 \\(y_{ij}\\) 的 p.d.f.4 为 \\(\\binom{N_{ij}}{y_{ij}}\\pi_i^{y_{ij}}\\left(1-\\pi_i\\right)^{N_{ij}-y_{ij}}\\)，对数似然为 \\(\\log\\binom{N_{ij}}{y_{ij}}+y_{ij}\\log(\\pi_i)+(N_{ij}-y_{ij})\\log(1-\\pi_i)\\)，它也可重写为 \\(y_{ij}\\log\\left[\\frac{\\pi_i}{\\left(1-\\pi_i\\right)}\\right]+N_{ij}\\log\\left(1-\\pi_i\\right)+\\log\\left(\\frac{N_{ij}}{y_{ij}}\\right)\\)，其关键在于 \\(y_{ij}\\log\\left[\\frac{\\pi_i}{\\left(1-\\pi_i\\right)}\\right]\\) —— 它揭示了随机变量 \\(y_{ij}\\) 在 \\(\\log\\left[\\frac{\\pi_i}{\\left(1-\\pi_i\\right)}\\right]\\) 中是线性的。在概率论中，均值的函数（此时为 \\(\\log\\left[\\frac{\\pi_i}{\\left(1-\\pi_i\\right)}\\right]\\)）乘以 \\(y_{ij}\\) 后称为典型参数 (canonical parameter). 在统计建模中，\\(\\log\\left[\\frac{\\pi_i}{\\left(1-\\pi_i\\right)}\\right]\\) 称为 \\(\\text{logit}\\)。一个可能的模型是令典则参数等于模型的系统部分，因此\\[\\mathrm{logit}(\\pi_i)=\\log\\biggl[\\frac{\\pi_i}{\\left(1-\\pi_i\\right)}\\biggr]=\\beta_0+\\beta_1X_i\\]这便是 logistic 回归模型，在许多统计应用中都很常见。使用第 5 章充分讨论的方法，截距和斜率的估计分别为 \\(\\hat{{\\beta}}_0=-4.109\\) 和 \\(\\beta_1=0.764\\)。给定 \\(X_i\\)，我们可以将 \\(\\pi_i\\) 的估计确定为\\[\\hat{\\pi}_i=\\frac1{1+e^{-\\left(\\hat{\\beta}_0+\\hat{\\beta}_1X_i\\right)}}\\]\n对于 \\(X=0\\)，\\(\\hat\\pi=0.016\\)；对于 \\(X=10\\)，\\(\\hat\\pi=0.972\\)。图 1.1 显示了线性模型对 \\(p_{ij}\\) 的拟合与 logistic 回归拟合的比较。\n拟合 \\(p_{ij}\\) 的线性模型中观测值和预测值之间的相关性为 0.957，而对于 logistic 回归模型为 0.982。根据任何适用的准则，logistic 模型都提供了更好的拟合，更重要的是，它避免了试图解释不可能的预测值的问题。方法 2：定义一个合理的过程设想存在一个由 \\(X\\) 驱动的不可观测过程。我们无法看到该过程，只能看到其结果。若该过程超过某个值，我们将看到一次“失败”，否则我们将看到一次“成功”。现在，令 \\(\\eta\\) 表示成功与失败之间的边界，并假设它随 \\(X\\) 线性变化，即 \\(\\eta=\\beta_0+\\beta_1X\\)。这是门槛模型 (threshold model) 的一个简单示例。如果我们进一步设想不可观测过程具有标准正态分布，那么在 \\(X\\) 处“成功”的概率可建模为 \\(\\pi_i=\\Phi\\big(\\beta_0+\\beta_1X_i\\big)\\)，其中 \\(\\Phi(\\cdot)\\) 为逆正态或称为 probit. 图 1.2 说明了基本思想。如图所示，对于 \\(X = 3\\)（右轴），\\(\\eta\\cong-1\\)5（水平轴）（一开始你可能会有些不适应——你可能习惯于看到 \\(X\\) 在水平轴上，而响应变量在垂直（Y）轴上）。曲线 \\(\\Phi(-1)\\) 下的阴影面积是概率 \\(\\pi\\)。随着 \\(X\\) 的增加，\\(\\eta\\) 也随之增加，与 \\(\\pi\\) 对应的正态曲线下面积也随之增加。图 1.2 显示了正斜率（\\(\\beta>0\\)）。请注意，负斜率也是可能的，此时，随着 \\(X\\) 的增加，\\(\\eta\\) 会减小，因此 \\(\\pi\\) 也会减少。回归线的位置和斜率取决于 \\(\\beta_0\\) 和 \\(\\beta_1\\)。\n这就是 probit 回归模型。与 logistic 回归一样，它通常用于二项数据。logistic 模型和 probit 模型都是线性模型概率分布形式的示例。两者有相同的步骤：确定观测数据的概率分布。在 logistic 模型和 probit 模型中为 \\(y_{ij}\\thicksim\\mathrm{Binomial}(n_{ij},\\pi_i)\\)模型焦点为响应的期望值，即 \\(E\\left(y_i\\right)=N_{ij}\\pi_i\\)。由于 \\(N_{ij}\\) 已知，模型将有效地关注于 \\(\\pi_i\\)说明线性预测器。在本例中，线性预测器为 \\(\\beta_0+\\beta_1X_i\\)。现在，你可以把线性预测器看作模型方程形式的左手边，但不含 \\(e_{ij}\\)。在这个例子中，线性预测器对应于模型的系统部分请注意对于“现在”的强调。当我们在 1.3 节中引入随机模型效应时，事情变得更加复杂。确定将期望值连接到线性预测器的函数。在 logistic 模型中，该函数为 \\(\\log\\biggl(\\frac{\\pi_i}{1-\\pi_i}\\biggr)=\\beta_0+\\beta_1X_i\\)。在线性模型语言中，这称为连接函数 (link function). 你还可以将 logistic 模型写为\\[\\pi_i=\\frac1{\\left\\{1+\\exp\\left[-\\left(\\beta_0+\\beta_1X_i\\right)\\right]\\right\\}}\\]这称为逆连接函数 (inverse link function)。在连接函数中，线性预测器是独立的，将期望值嵌入函数；而在逆连接函数中，期望值是独立的，将线性预测器嵌入函数。两者都是有效且充分的表达式。请注意，我们使用逆连接描述 probit 模型 \\({\\pi}_i={\\Phi}\\big({\\beta}_0+{\\beta}_1X_i\\big)\\)。你可以将相同的步骤应用于 1.1 节介绍的高斯模型。分布：\\(y_{ij}\\sim NI\\left(\\mu_i,\\sigma^2\\right)\\)模型焦点：\\(E\\Big(y_{ij}\\Big)=\\mu_i\\)线性预测器：\\(\\beta_0+\\beta_1X_i\\)连接：对于高斯模型，连接函数称为恒等连接 (identity link)，即线性预测器本身预测了 \\(E\\Big(y_{ij}\\Big)=\\mu_i=\\eta_i=\\beta_0+\\beta_1X_i\\)1.2 节的关键思想：模型方程形式与概率分布形式；线性预测器；连接和逆连接函数。","code":""},{"path":"chap1.html","id":"sec1-3","chapter":"第 1 章 建模基础","heading":"1.3 模型效应的类型","text":"在本节中，我们将介绍模型效应类型的两个重要区分。第一个区分，如 1.3.1 节所述，为分类 (classification) 效应（也称为类别 (class) 效应）和直接 (direct) 效应之间的区分。第二个区分，如 1.3.2 节所述，为固定 (fixed) 效应和随机 (random) 模型效应之间的区分。随机模型效应是线性预测器的一部分，勿与统计模型的模型方程形式中的残差项混淆。","code":""},{"path":"chap1.html","id":"sec1-3-1","chapter":"第 1 章 建模基础","heading":"1.3.1 分类与直接效应","text":"在前几节中，我们考虑了两处理均值模型和线性回归模型。线性预测器对于前者为 \\(\\mu_i=\\mu+\\tau_i\\)，对于后者为 \\(\\beta_0+\\beta_1X_i\\)。在两处理模型中，预测变量为处理，相应的模型效应为 \\(\\tau_i\\)。在回归模型中，预测变量为 \\(X_i\\)，相应的模型效应为 \\(\\beta_1\\)。这说明了统计模型中的两种类型的效应。在前一个模型中，处理是作为分类变量的预测变量的一个例子；在后一个模型中，\\(X_i\\) 是作为直接变量的预测变量的一个例子。分类变量按类别定义，例如处理 1 和处理 2，而直接变量由其确切的量定义。这种区分有时可能会令人困惑，因为将预测变量定义为直接变量或分类变量，具体取决于我们的目标以及我们想要的推断。例如，对于表 1.1 中的数据，我们可以将 \\(X_i\\) 的水平定义为分类变量并拟合效应模型 \\(\\mu+\\tau_i,= 1,2,\\ldots,10\\)，其中效应定义在 \\(X\\) 的 10 个水平上。如果我们不知道响应是否根据 \\(X\\) 的任何定义函数而变化，并且我们不期望响应遵循这样的函数，我们可能会这样做。如果我们对 \\(y\\) 和 \\(X\\) 之间的任何可能关系都没有先入为主的想法，那么将 \\(X\\) 定义为分类变量可以让我们进行探索。","code":""},{"path":"chap1.html","id":"sec1-3-2","chapter":"第 1 章 建模基础","heading":"1.3.2 固定与随机效应","text":"分类与直接效应是区分模型效应类型的一种方法。另一个也许是更根本的区分，与如何选择效应的水平以及预期推断的范围有关。要了解其工作原理，请考虑以下内容。线性回归示例的扩展，以说明模型效应类型之间的重要区分。在表 1.1 中，\\(X\\) 的每个水平只有一个观测。相反，假设在多个位置 (locations) 或多个批次 (batches) 观测到 \\(X\\) 的水平。表 1.2 显示了一组假想的数据。\\(X\\) 有 11 个水平，取值为 0 到 48 不等。对于 4 个批次中的每一个，在 \\(X\\) 的每个水平上都观察到一个连续变量 \\(Y\\) 和一个二项变量，其中 \\(N\\) 是伯努利试验的次数，\\(Fav\\) 是“成功”的次数。\n这些数据该如何建模？图 1.3 显示了叠加线性回归线的数据图。\n实线显示了所有批次的平均线性回归；虚线显示了每个单独批次的回归。该图表明假定线性回归是合理的，但假定所有批次具有一个共同的回归可能是不合理的。假定按批次进行单独的线性回归得到线性预测器 \\(\\beta_{0i}+\\beta_{1i}X_{ij}\\)，其中 \\(\\beta_{0i}\\) 和 \\(\\beta_{1i}\\) 分别为第 \\(\\) 个批次的截距和斜率，\\(=1,2,3,4\\)，\\(X_{ij}\\) 为第 \\(\\) 个批次 \\(X\\) 的第 \\(j\\) 个值。这可以等价地表示为\\[\\begin{align}\n\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\n\\tag{1.3}\n\\end{align}\\]其中\\(\\beta_0\\) 和 \\(\\beta_1\\) 是总体截距和斜率（用实心黑线表示）\\(b_{0i}\\) 和 \\(b_{1i}\\) 分别是与特定于批次的截距和斜率（\\(\\beta_0\\) 和 \\(\\beta_1\\)）的偏差。我们如何对 \\(b_{0i}\\) 和 \\(b_{1i}\\) 进行估计和推断取决于产生批次的过程以及我们希望做出的推断。一方面，这些批次可以代表更多批次的样本。在这种情况下，我们可以任意抽取四个批次——至少在理论上是这样。由此可见，\\(b_{0i}\\) 和 \\(b_{i1}\\) 是随机变量，因此具有概率分布。在这种情况下，模型 (1.3) 称为随机系数线性回归模型。另一方面，该数据集中的四个批次也可能是整个总体。例如，如果有四个批次的供应商，并且我们将“批次 \\(\\)”理解为“供应商 \\(\\)”就说得通了。这与根据单个供应商随机抽取四个批次不同。此时，模型 (1.3) 称为不等斜率线性回归模型。如果批次代表更大的总体，我们自然希望将推断重点放在总体上，而不仅仅是我们观察到的四个批次。我们需要考虑各批次之间的变异。如果批次是总体，我们希望将推断重点放在批次效应及其对关于 \\(X\\) 的回归的影响。此时，我们将 \\(b_{0i}\\) 和 \\(b_{1i}\\) 视为模型参数，且与 \\(\\beta_0\\) 和 \\(\\beta_1\\) 具有相同的意义。作为样本的批次（因此将 \\(b_{0i}\\) 和 \\(b_{1i}\\) 作为随机变量）与作为总体的批次（因此将 \\(b_{0i}\\) 和 \\(b_{1i}\\) 作为模型参数）之间的区别引入了随机模型效应与固定模型效应的概念。如果批次是样本，\\(b_{0i}\\) 和 \\(b_{1i}\\) 是随机效应；如果批次是总体，因此 \\(b_{0i}\\) 和 \\(b_{1i}\\) 是参数，那么 \\(b_{0i}\\) 和 \\(b_{1i}\\) 就是固定效应。线性预测器仅包含固定效应的模型称为固定效应模型或仅固定效应模型 (fixed-effect-models). 线性预测器同时包含固定和随机效应的模型称为混合模型 (mixed models). 理论上，混合模型中的随机效应可具有任何合理的分布。例如，Lee Nelder (1996) 以及 Lee et al. (2006) 讨论了双广义线性模型 (doubly-generalized linear models)，其中随机模型效应以及响应变量可能具有非高斯分布。在本书中，我们几乎只关注高斯分布的随机效应。这出于两个原因：具有高斯随机效应的混合模型的计算方法和可用软件得到了更好的开发。专注于高斯案例使我们能够专注于概念，并最大限度地减少因计算问题分散注意力的风险。绝大多数具有混合模型的实际应用都假定了高斯随机效应——至少在编写本书时如此。统计建模的最先进技术一直在进步。356 年前，广义线性模型基本上是未知的，线性混合模型被认为超出了最先进的水平。现在它们已经变得司空见惯，任何关于建模有见地的讨论都必须包括它们。在未来，非高斯随机效应可能也会同样普遍。但目前我们还未达到那个阶段。总之，多批次回归例子产生了线性预测因子 \\(\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\\)。如果我们假设固定的批次效应，那么 \\(\\beta_0,\\beta_1,b_{i0}\\) 和 \\(b_{i1}\\) 都是模型参数。如果我们假定随机的批次效应，那么只有 \\(\\beta_0\\) 和 \\(\\beta_1\\) 是模型参数，并且我们必须说明随机变量 \\(\\beta_1,b_{i0}\\) 和 \\(b_{i1}\\) 的假定概率分布。通常，来自不同批次的 \\((b_{0i},b_{1i})\\) 对假定为独立的，并且在每个批次中，\\((b_{0i},b_{1i})\\) 对是二元正态的，即\\[\\begin{bmatrix}b_{0i}\\\\b_{1i}\\end{bmatrix}\\thicksim N\\left(\\begin{bmatrix}0\\\\0\\end{bmatrix},\\begin{bmatrix}\\sigma_0^2&\\sigma_{01}\\\\\\sigma_{01}&\\sigma_1^2\\end{bmatrix}\\right)\\]其中 \\(\\sigma^2_0\\) 和 \\(\\sigma^2_1\\) 分别为 \\(b_{0i}\\) 和 \\(b_{1i}\\) 的方差，\\(\\sigma_{01}\\) 是 \\(b_{0i}\\) 和 \\(b_{1i}\\) 之间的协方差。一旦我们确定了线性预测器，并确定了随机效应及其假定概率（如果有），那么我们就能够像在 1.2 节中那样完成模型构建。也就是说，对于假定服从高斯分布的数据，线性预测器是给定随机效应 \\(b_{0i}\\) 和 \\(b_{1i}\\) 的条件下，数据期望值的估计值。正式地，我们将其表述为观测：\\(y_{ij}\\mid b_{0i},b_{1i}\\thicksim N\\left(\\mu_{ij}\\mid b_{0i},b_{1i},\\sigma^2\\right)\\)模型焦点：\\(E\\Big(y_{ij}\\mid b_{0i},b_{1i}\\Big)=\\mu_{ij}\\mid b_{0i},b_{1i}\\)线性预测器：\\(\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\\)关于 \\(b_{0i}\\) 和 \\(b_{1i}\\) 的假定（如果是随机的）——例如上例连接：恒等的，即 \\(\\mu_{ij}\\mid b_{0i},b_{1i}\\) 由 \\(\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\\) 建模\n如果我们对表 1.2 中的变量 \\(Fav\\) 建模，那么我们将相应地调整观测的分布以及连接。即观测：\\(Fav_{ij}\\mid b_{0i},b_{1i}\\sim\\text{Binomial}\\bigg[N_{ij},\\bigg(\\pi_{ij}\\mid b_{0i},b_{1i}\\bigg)\\bigg]\\)模型焦点：\\(E\\Big(Fav_{ij}\\mid b_{0i},b_{1i}\\Big)=\\pi_{ij}\\mid b_{0i},b_{1i}\\)线性预测器：\\(\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\\)关于 \\(b_{0i}\\) 和 \\(b_{1i}\\) 的假定（如果是随机的）连接：Logist 或 Probit，例如，对于 Logit：\n\\[\\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right)\\]\n由 \\(\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\\) 建模，其中 \\(\\pi_{ij}\\) 为 \\(\\pi_{ij}\\mid b_{0i},b_{1i}\\) 的缩写\n假定高斯数据的模型是我们对线性模型的介绍。在本书中，我们使用以下缩写。如果批次效应是固定的，我们使用线性模型 (Linear Model) 的首字母缩写 LM 来指代。如果批次效应是随机的，则它是线性混合模型 (Linear Mixed Model)，其缩写为 LMM.\n假定二项数据的模型是我们对广义线性模型 (Generalized Linear Model, GLM) 的介绍。如果线性预测器中的所有效应都是固定的，例如模型 (1.4) 具有固定批次效应，则我们有一个仅固定效应的广义线性模型。这些模型使用缩写 GLM.如果批处理效应是随机的，则它是一个广义线性混合模型 (generalized linear mixed model)，缩写为 GLMM.\n请注意，GLMM 是一般情况；LM, GLM 和 LMM 都是特殊情况。在 1.4 节中，我们将看到如何以矩阵形式编写这些模型，这对于开发第 4, 5 和 6 章中的估计和推断理论至关重要。在第 2 章，我们将介绍一些技巧，以帮助将数据集描述和目标转化为合理的模型。在第 3 章中，我们将讨论推断的基本概念以及在开始正式开发估计和推断理论之前需要理解的一些细节。到那时，我们将准备好充分发展用于处理 GLMMs（以及因此而来的 LM, GLM 和 LMM）所需的底层理论和方法论。1.3 节的关键思想：分类与直接预测变量；固定与随机模型效应；线性模型 (LM)；线性混合模型 (LMM)；广义线性模型 (GLM)；广义线性混合模型 (GLMM)","code":""},{"path":"chap1.html","id":"sec1-4","chapter":"第 1 章 建模基础","heading":"1.4 以矩阵形式编写模型","text":"在前几节中，我们推导了八个模型。表 1.3 对它们进行了总结。\n请注意，表 1.3 中的前两个模型是 1.1 节中介绍的模型 (1.1) 和 (1.2)。为方便起见，在本章的剩余部分中，我们将此表中的型号称为模型 1 至模型 8，如表中首列所示。我们可以用矩阵形式来表达这些。事实上我们必须这样做，这有两个重要的原因。这些模型估计和推断理论的开发——在第 4, 5 和 6 章中介绍——需要矩阵代数。这些模型的统计计算程序——在“黑匣子”中——本质上是矩阵代数处理器。当你在软件包（在本书中为 SAS® PROC GLIMMIX）中指定模型时，你实际做的就是指定定义响应变量、其线性预测器、其连接及其概率分布的矩阵。\n本节的目的是向你展示如何操作。我们从仅固定效应模型开始，然后考虑混合模型。","code":""},{"path":"chap1.html","id":"sec1-4-1","chapter":"第 1 章 建模基础","heading":"1.4.1 仅固定效应模型","text":"我们从第一个模型开始，即两处理 LM. 假定每种处理有三个观测。我们可以将与每个观测及其线性预测器对应的连接函数写为方程组：\\[\\begin{gathered}\n\\mu_{11} =\\eta_{11}=\\mu+\\tau_1 \\\\\n\\mu_{12} =\\eta_{12}=\\mu+\\tau_1 \\\\\n\\mu_{13} =\\eta_{13}=\\mu+\\tau_1 \\\\\n\\mu_{21} =\\eta_{21}=\\mu+\\tau_2 \\\\\n\\mu_{22} =\\eta_{22}=\\mu+\\tau_2 \\\\\n\\mu_{23} =\\eta_{23}=\\mu+\\tau_2\n\\end{gathered}\\]这又可用矩阵形式表示为\\[\\begin{bmatrix}\\eta_{11}\\\\\\eta_{12}\\\\\\eta_{13}\\\\\\eta_{21}\\\\\\eta_{22}\\\\\\eta_{23}\\end{bmatrix}=\\begin{bmatrix}1&1&0\\\\1&1&0\\\\1&1&0\\\\1&0&1\\\\1&0&1\\\\1&0&1\\end{bmatrix}\\begin{bmatrix}\\mu\\\\\\tau_1\\\\\\tau_2\\end{bmatrix}\\]这显示了一个仅考虑固定效应的线性模型的一般结构–使用前面定义的首字母缩写 LM 或 GLM. 仅固定效应线性预测器的一般形式为\\[\\begin{align}\n\\symbf\\eta=\\symbf X\\symbf\\beta\n\\tag{1.4}\n\\end{align}\\]其中 \\(\\symbf\\eta\\) 是数据集中 \\(N\\) 个观测中每个观测的连接函数的 \\(N × 1\\) 值向量，\\(\\symbf X\\) 是 \\(N × p\\) 矩阵，\\(p\\) 是线性预测器中的参数数量，\\(\\symbf\\beta\\) 是固定效应模型参数的 \\(p × 1\\) 向量。此时，为了建立一些一般原则，将 \\(\\symbf X\\symbf\\beta\\) 划分为多个分量是有用的。具体地\\[\\symbf X\\symbf{\\beta}=\\begin{bmatrix}\\symbf X_\\mu&\\symbf X_\\tau\\end{bmatrix}\\begin{bmatrix}\\mu\\\\\\symbf\\tau\\end{bmatrix}=\\symbf X_\\mu\\mu+\\symbf X_\\tau\\symbf{\\tau}\\]其中\\[\\symbf{\\tau}=\\begin{bmatrix}{\\tau}_1\\\\{\\tau}_2\\end{bmatrix},\\quad\\symbf{X}_\\tau=\\begin{bmatrix}1&0\\\\1&0\\\\1&0\\\\0&1\\\\0&1\\\\0&1\\end{bmatrix}\\]请注意，\\(\\symbf X_\\mu=\\symbf 1_6\\)，即一个 \\(6 × 1\\) 向量。换句话说，矩阵 \\(\\symbf X\\) 被划分为 “\\(\\mu\\)” 或“截距部分”和 “\\(\\tau\\)” 或“效应部分”，向量 \\(\\symbf\\beta\\) 也是如此。矩阵 \\(\\symbf X\\) 的截距部分始终是一个 N × 1 的全一向量，矩阵的效应部分始终涉及每个效应水平的一列——本例中为两列，因为有两种处理。对于每个效应水平的列，在对应该水平的行中填入 1，否则填入 0 ——在本例中，\\(\\symbf X_\\tau\\) 的第一列在前三行（对应接受处理 1 的观测）填入 1，而 \\(\\symbf X_\\tau\\) 的第二列在接下来的三行（第 4 到 6 行，对应接受处理 2 的观测）填入 1. 回想一下，处理是一种分类效应——\\(\\symbf X_\\tau\\) 展示了如何为分类效应构建矩阵 \\(\\symbf X\\) 的一部分。矩阵 \\(\\symbf X\\) 通常称为设计矩阵 (design matirx)，特别是当模型包含分类效应时。这是因为你可以通过查看矩阵 \\(\\symbf X\\) 准确地知道哪些处理分配给了哪些观测——因此，矩阵 \\(\\symbf X\\) 描述了设计。矩阵 \\(\\symbf X\\) 又称为导数矩阵 (derivative matrix)，因为 \\(\\symbf X\\) 的每个元素都是关于相应模型参数的导数。也就是说，\\(\\symbf X\\) 的每一行都由\\[\\begin{equation}\\begin{bmatrix}\\frac{\\partial \\symbf X_i\\symbf{\\beta}}{\\partial\\mu}&\\frac{\\partial \\symbf X_i\\symbf{\\beta}}{\\partial\\tau_1}&\\frac{\\partial \\symbf X_i\\symbf{\\beta}}{\\partial\\tau_2}\\end{bmatrix}\\end{equation}\\]组成。其中 \\(\\symbf X_i\\) 表示矩阵 \\(\\symbf X\\) 的第 \\(\\) 行。将 \\(\\symbf X\\) 视为导数矩阵有助于过渡到非线性模型，虽然这超出了本书的范围，但在最后几章中我们简要介绍了某些非线性建模工具。然而，非线性模型是统计建模在 GLMM 之外的下一个扩展。现在考虑第二个模型，即，在 \\(X_{ij}=0,1,\\ldots,10\\) 的水平上的线性回归。与第一个模型一样，这建立了一组方程\\[\\begin{aligned}\n\\eta_1&=\\beta_0  \\\\\n\\eta_2&=\\beta_0+\\beta_1  \\\\\n\\eta_3&=\\beta_0+2\\beta_1  \\\\\n{\\eta}_{4}&={\\beta}_{0}+3{\\beta}_{1}  \\\\\n{\\eta}_{5}&={\\beta}_{0}+4{\\beta}_{1}  \\\\\n{\\eta}_{6}&={\\beta}_0+5{\\beta}_1  \\\\\n{\\eta}_{7}&={\\beta}_{0}+6{\\beta}_{1}  \\\\\n\\eta_{8}& =\\beta_{0}+7\\beta_{1}  \\\\\n\\eta_9&=\\beta_0+8\\beta_1  \\\\\n{\\eta}_{10}&={\\beta}_0+9{\\beta}_1  \\\\\n\\eta_{11}&=\\beta_{0}+10\\beta  \n\\end{aligned}\\]这可写为矩阵形式\\[\\begin{bmatrix}\\eta_1\\\\\\eta_2\\\\\\eta_3\\\\\\eta_4\\\\\\eta_5\\\\\\eta_6\\\\\\eta_7\\\\\\eta_8\\\\\\eta_9\\\\\\eta_{10}\\\\\\eta_{11}\\end{bmatrix}=\\begin{bmatrix}1&0\\\\1&1\\\\1&2\\\\1&3\\\\1&4\\\\1&5\\\\1&6\\\\1&7\\\\1&8\\\\1&9\\\\1&10\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix}\\]与两处理模型一样，它具有方程 (1.4) 中给出的一般形式 \\(\\symbf\\eta=\\symbf X\\symbf\\beta\\)。对于回归模型，矩阵 \\(\\symbf X\\) 可以划分为“截距部分”和“效应部分”，即 \\(\\symbf X\\symbf{\\beta}=\\symbf X_{\\beta_0}{\\beta}_0+\\symbf X_{\\beta_1}{\\beta}_1\\)。与往常一样，截距部分 \\(\\symbf X_{\\beta_0}\\) 只是一个全一向量。在该模型中，预测器是一个直接变量，因此矩阵 \\(\\symbf X\\) 中相应的列，即效应部分 \\(\\symbf X_{\\beta_1}\\)，实际上是预测器变量的值（0 到 10）的向量。对于二项数据的两处理和回归模型，你可以以相同的方式设置矩阵。也就是说，模型 3 的设置与模型 1 相同，模型 4 的设置与模型 2 相同。仅连接函数 \\(\\symbf \\eta_i\\) 的定义发生了变化。现在考虑固定效应、多批次模型（表 1.3 中的模型 5 和 6）。令连接等于线性预测器得到形如式为 \\(\\eta_{ij}=\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\\) 的一组方程。回想一下我们有四个批次，这就得到了矩阵方程我们可以将这些矩阵方程的逻辑概括如下。固定效应 LM 或 GLM 的一般形式为 \\(\\symbf\\eta=\\symbf X\\symbf\\beta\\)。将 \\(\\symbf X\\symbf\\beta\\) 划分为\\[\\begin{bmatrix}\\symbf X_{\\beta_0}&\\symbf X_{b_0}&\\symbf X_{\\beta_1}&\\symbf X_{b_1}\\end{bmatrix}\\begin{bmatrix} \\beta_0\\\\\\symbf{b}_0\\\\\\beta_1\\\\\\symbf{b}_1\\end{bmatrix}=\\symbf X_{\\beta_0}\\beta_0+\\symbf X_{b_0}\\symbf{b}_0+\\symbf X_{\\beta_1}\\beta_1+\\symbf X_{b_1}\\symbf{b}_1\\]其中，\\(\\symbf X_{\\beta_0}\\) 是 \\(N×1\\) 全一向量，\\(\\symbf X_{b_0}\\) 是一个 \\(N × 4\\) 矩阵，如果相应的观测在第 \\(j\\) 批中，则其第 \\(ij\\) 元素为 1，否则为 0.\\(\\symbf X_{\\beta_1}\\) 是一个 \\(N × 1\\) 向量，其元素等于相应观测的协变量 \\(X_{ij}\\) 的值，\\(\\symbf X_{b_1}\\) 是一个 \\(N × 4\\) 向量，如果相应观测在第 \\(j\\) 批中，则其元素为 \\(X_{ij}\\)，否则为 0，\\(\\symbf b_1\\) 是 \\(4 ×1\\) 向量，其元素为 \\(b_{1i}\\)，\\(= 1, 2, 3, 4\\) 对应四个批次。请注意，矩阵 \\(\\symbf X_{b_1}\\) 是 \\(\\symbf X_{b_0}\\) 和 \\(\\symbf X_{\\beta_1}\\) 的水平直积 (horizontal direct product). 另请注意，预测变量 \\(X_{ij}\\) 是直接变量，因此矩阵 \\(\\symbf X\\) 中的列是由 \\(X_{ij}\\) 确切值组成的向量，而批次是分类变量，因此每个批次有一列，并且这些列具有非零值当且仅当第 \\(ij\\) 个观测位于与该列对应的批次中。该模型本质上包含了建立矩阵形式所涉及的主要决策的缩影。截距参数始终是标量，其在矩阵 \\(\\symbf X\\) 中的对应元素是一个全一列向量。直接变量始终具有标量参数，并且它们在矩阵 \\(\\symbf X\\) 中的相应元素是直接变量确切值的列向量。分类变量每个效应水平有一个参数，因此矩阵 \\(\\symbf X\\) 中每个效应水平都有一列，如果相应的观测在该水平，则其元素为 0，否则为 0. 矩阵 \\(\\symbf X\\) 的 \\(\\symbf X_{b_1}\\) 分量以及向量 \\(\\symbf b_1\\) 具有分类和直接变量的特征：因此，\\(\\symbf X_{b_1}\\) 被构造为元素的直积，在这种情况下，它是由 \\(\\symbf X_{\\beta_0}\\) 和 \\(\\symbf X_{\\beta_1}\\) 组成的。总之，所有仅固定效应模型都具有一般形式 \\(\\symbf\\eta=\\symbf X\\symbf\\beta\\)。所有概率陈述都位于观测向量 \\(\\symbf y\\) 的分布中。因此，仅固定效应模型的完整指定为线性预测器：\\(\\symbf\\eta=\\symbf X\\symbf\\beta\\)\\(\\symbf y\\) 服从某种分布，记 \\(E(\\symbf y)=\\symbf \\mu\\)，\\(Var(\\symbf y)=\\symbf R\\)连接函数：\\(\\symbf{\\eta}=g\\left(\\symbf{\\mu}\\right)\\)\n现在我们转向将批次视为随机效应的多批次模型。","code":""},{"path":"chap1.html","id":"sec1-4-2","chapter":"第 1 章 建模基础","heading":"1.4.2 混合模型：具有固定效应和随机效应的模型","text":"\n表 1.3 中的最后两个模型是混合模型。线性预测器 \\(\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_{ij}\\) 似乎与上面讨论的模型 5 和 6 的线性预测器完全相同。主要区别在于，在模型 5 和 6 中，与批次 \\(b_{0i}\\) 和 \\(b_{1i}\\) 相关的效应是固定参数，而在模型 7 和 8 中，它们具有概率分布。在矩阵表示法中，与随机效应相关的 \\(\\symbf X\\symbf\\beta\\) 的分量从 \\(\\symbf X\\symbf\\beta\\) 中移除并置于新的元素中，表示为 \\(\\symbf{Zb}\\)，其中向量 \\(\\symbf b\\) 由随机模型效应组成。正式地，线性预测器为 \\(\\symbf\\eta=\\symbf X\\symbf\\beta+\\symbf{Zb}\\)，其中 \\(\\symbf b  \\sim N (\\symbf 0,\\symbf G)\\)。对于模型 7 和 8，线性预测器的固定效应分量为 \\({\\beta}_{0i}+{\\beta}_{1i}X_{ij}\\)。因此其矩阵形式为或\\[\\begin{bmatrix}\\symbf X_{\\beta_0}&\\symbf X_{\\beta_1}\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix}=\\symbf X_{\\beta_0}\\beta_0+\\symbf X_{\\beta_1}\\beta_1\\]其中所有项的定义如前。模型 5 和 6 中 \\(\\symbf X\\symbf\\beta\\) 的其余元素变为模型 7 和 8 中的 \\(\\symbf{Zb}\\)，即其中\\[\\symbf{b}=\\begin{bmatrix}{b}_0\\\\{b}_1\\end{bmatrix}\\thicksim N\\left(\\begin{bmatrix}0\\\\0\\end{bmatrix},\\begin{bmatrix}\\symbf{}{\\sigma}_0^2&\\symbf{}{\\sigma}_{01}\\\\\\symbf{}{\\sigma}_{01}&\\symbf{}{\\sigma}_1^2\\end{bmatrix}\\right)\\]请注意，矩阵 \\(\\symbf Z\\) 的元素 \\(\\symbf Z_{b0}\\) 和 \\(\\symbf Z_{b1}\\) 的构造与模型 5 和 6 中的 \\(\\symbf X_{b0}\\) 和 \\(\\symbf X_{b1}\\) 完全相同。唯一的变化是，因为它们在模型 7 和 8 中是随机效应，所以它们置于矩阵 \\(\\symbf Z\\) 中，它们对应的模型效应置于 \\(\\symbf b\\) 中，而 \\(\\symbf b\\) 是随机效应向量，并且必须说明 \\(\\symbf b\\) 的概率分布。总之，所有混合模型都具有一般形式 \\(\\symbf\\eta=\\symbf X\\symbf\\beta+\\symbf{Zb}\\)。需对观测向量 \\(\\symbf y\\mid \\symbf b\\) 以及随机效应向量 \\(\\symbf b\\) 说明概率分布。请注意，在混合模型中，观测是以随机模型效应为条件的。为了更清楚地说明这一点，我们将在第 2 章中给出几个示例。混合模型的完整指定如下：线性预测器：\\(\\symbf\\eta=\\symbf X\\symbf\\beta+\\symbf{Zb}\\)，其中 \\(\\symbf b\\sim N(\\symbf 0,\\symbf G)\\)以随机效应为条件的观测 \\(\\symbf y\\mid \\symbf b\\) 服从某种分布，并记 \\(E(\\symbf y\\mid \\symbf b)=\\symbf \\mu\\mid \\symbf b\\) 以及 \\(Var(\\symbf y\\mid \\symbf b)=\\symbf R\\)。通常，为方便起见，我们会使用简写 \\(E(\\symbf y\\mid \\symbf b)=\\symbf \\mu\\) 来表示观测的条件均值。1.4 节的关键思想：模型的矩阵形式；矩阵 \\(\\symbf X\\) 和向量 \\(\\symbf \\beta\\) 如何形成固定效应模型；分类与直接变量对矩阵组成的影响；在混合模型中 \\(\\symbf X\\symbf\\beta\\) 与 \\(\\symbf Z\\symbf b\\) 有何不同？","code":""},{"path":"chap1.html","id":"sec1-5","chapter":"第 1 章 建模基础","heading":"1.5 小结：模型完整陈述的必要元素","text":"表 1.4 总结了我们在本章中考虑的模型类型。\n我们将表 1.4 中的各列称为统计模型的必要元素。此后，在本书中我们使用以下格式来指定统计模型。统计模型的必要元素——标准格式线性预测器：一般形式为“\\(\\eta=\\)…”，例如 \\(\\eta_i=\\eta+\\tau_i\\)。分布：必须指定观测和所有随机模型效应的分布。在混合模型中，观测的分布取条件于随机模型效应。连接函数：对于高斯模型（LM 和 LMM），连接（通常）是恒等的。对于 GLM 和 GLMM，连接是均值的某个函数，例如\n\\[\\eta_i=\\mathrm{logit}(\\pi_i)=\\log\\biggl[\\frac{\\pi_i}{\\left(1-\\pi_i\\right)}\\biggr]\\]重复之前提出的观点：GLM, LM 和 LMM 都是 GLMM 的特殊情况。这一点在第 4, 5 和 6 章中需要牢记。这些章节开发了线性模型估计和推断的理论与方法。它们专注于 GLMM，因为 GLMM 的理论和方法也适用于 GLM, LM 和 LMM.\n从历史的角度来看，我们在第 4, 5 和 6 章中的发展与传统文本中线性模型理论和方法的开发有所不同，传统文本通常从线性模型 (LM) 开始，然后逐层增加复杂性。为理解这一理念的合理性，我们需要简要回顾线性建模思想的发展历程。线性模型在过去的五十年里经历了长足的发展。20 世纪 70 年代中期是对比本文方法和我们所说的“更传统”方法的一个很好的参照点。彼时，第一款广泛可用且真正全面的线性模型统计软件问世。同时，那十年期间也涌现了许多经典的线性模型教科书。这些教科书以矩阵代数为基础，构建了线性模型的理论和方法；它们重点讨论了二次型及其相关的分布理论。在此期间，本文中所称的 LM 是以模型方程而非概率分布的形式呈现的，并称为“一般线性模型” (“General Linear Model”). 也就是说，“一般线性模型”定义为 \\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\)，其中 \\(\\symbf{e}\\thicksim N\\left(\\symbf 0,\\symbf{}{\\sigma}^2\\right)\\)。SAS® 开发的程序 PROC GLM 实现了“一般线性模型”。该 SAS® 程序现在会引起混乱，因为它实际上只适用于我们使用的术语 “LM”，而不是真正的 “GLMs”——我们在本书中提到的广义线性模型。长期以来，“一般线性模型”一直被认为是线性建模的一个特例。LMM 和 GLM 的概念和方法已经存在很长时间了。在 20 世纪 30 年代，随着不完全区组设计的出现，引入了区组间信息的恢复 (recovery inter-block information) ——即假定随机区组效应的 LMM 分析。自 20 世纪 40 年代以来，方差分量估计一直是线性建模的一部分。多个误差项的使用（例如用于多水平和裂区实验）——一种 LMM 程序——是方差分析 (analysis variance, ANOVA) 的扩展，它在 20 世纪 20 年代 ANOVA 引入后不久出现，并自此成为标准的统计工具。早在 PROC GLM 程序出现之前，计量经济学家和地质统计学家就为专门的应用开发了具有相关误差 (correlated error) 的 LMMs. Harville 于 1976 年发表了 LMM 的统一理论。在 GLM 方面，logistic 回归和 probit 回归模型（与 1.2 节中的二项回归示例类似）在 20 世纪 50 年代被广泛使用。列联表的对数线性模型于 20 世纪 60 年代引入。生存分析方法大约在这个时候出现。Nelder Wedderburn (1972) 发表了 GLM 的统一理论。在 20 世纪 80 年代和 90 年代，两个主要的发展线索同时发生——事实上，它们相互推动和加强——改变了我们对线性模型的看法。首先，计算机变得更小、更快、更强大、更可用——这一点非常明显。LMM 和 GLM 的计算在 20 世纪 70 年代的标准下是令人望而却步的，到 1990 年代的标准下变得容易。其次，随着计算机的发展，GLM 和LMM 理论相互混合，到 20 世纪 90 年代中期，文献中出现了 GLMMs 的综合理论，例如 Breslow Clayton (1993). 到 2000 年代中期，用于实现 GLMMs 的综合软件已经变得广泛可用，这种软件的语法对于任何了解 LMs 的人来说都很熟悉。那么，从 2023 年的角度来看，将 LM（尤其是将 \\(Var(\\symbf e)\\) 限制为 \\(\\symbf \\sigma^2\\) 的 LM）称为“一般”线性模型似乎很奇怪。按照当代标准，LM 根本不是“一般的”，这样称呼它是具有误导性的。此外，模型方程形式——仅固定效应模型的 \\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 以及混合模型的 \\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\symbf{Zb}+\\mathbf{e}\\)——正如我们 1.2 节中看到的那样，对于 GLM 和 GLMM 来说是无用且具有误导性的。因此本书的行文流程是这样的。我们没有先在模型方程形式下的 LM 模型 \\(\\require{symbf}\\mathbf{y}=\\mathbf{X}\\symbf\\beta+\\mathbf{e}\\) 构建一套详尽的理论和方法论，然后再从头开始研究 GLMs, LMMs 和 GLMMs，而是从一开始就从一般情况入手。总之，模型的基本要素如表 1.4 所示。线性模型的任何表述都必须明确列出表 1.4 中每一列的要素。缺少任何一个要素的模型表述都是不完整。在第 2 章中，我们将学习如何将数据集的描述转化为合理的模型表述。实际上，我们在本章的 1.2 节和 1.3 节已经开始了这个过程，但第 2 章将把它提升到一个更高的层次。然后，在第 3 章中，我们将开始开发处理这些模型所需的理论和方法的概念和工具。","code":""},{"path":"chap1.html","id":"exe1","chapter":"第 1 章 建模基础","heading":"练习","text":"考虑两处理的配对比较。考虑两种可能的场景：\n响应变量是连续的，并可以假定具有高斯分布。\n对于第 \\(j\\) 对（\\(j  = 1, 2 ,..., p\\)）的第 \\(\\) 种处理（\\(= 1,2\\)），进行 \\(N_{ij}\\) 个观测。每个观测都有有利或不利的结果。将 \\(y_{ij}\\) 表示为在第 \\(ij\\) 对中观察到的有利结果数。\n对于 , b 两种情况：\n编写模型的完整指定，包括所有必要元素。\n以矩阵形式写出 . 中的模型。包括任何随机模型效应的协方差假定的矩阵指定。\n鉴于配对比较的目的是找出处理之间是否存在差异，请根据你在 . 中定义的模型参数，正式地表述你将要检验的原假设。\n建议在课堂上实施此练习：\n两人一组，分别完成场景 和 b 的工作。\n两人对对方的工作进行同行评审。\n根据同行的建议修订对以上三个问题的回答。\n一旦两人对两个场景中的工作都感到满意，就提交工作。\n以上所有内容最多花 15 分钟。每个场景的演示和同行评审最多 5 分钟。最多修订 5 分钟。\n对于同行评审，重点关注：\n模型陈述是清楚的吗？\n是否定义了所有的项？\n是否提供了所有必要元素？表达是清晰的吗？\n是否说明了假定？\n答案是否简洁（没有废话，没有死记硬背……）？\n\n考虑两处理的配对比较。考虑两种可能的场景：响应变量是连续的，并可以假定具有高斯分布。对于第 \\(j\\) 对（\\(j  = 1, 2 ,..., p\\)）的第 \\(\\) 种处理（\\(= 1,2\\)），进行 \\(N_{ij}\\) 个观测。每个观测都有有利或不利的结果。将 \\(y_{ij}\\) 表示为在第 \\(ij\\) 对中观察到的有利结果数。对于 , b 两种情况：编写模型的完整指定，包括所有必要元素。以矩阵形式写出 . 中的模型。包括任何随机模型效应的协方差假定的矩阵指定。鉴于配对比较的目的是找出处理之间是否存在差异，请根据你在 . 中定义的模型参数，正式地表述你将要检验的原假设。建议在课堂上实施此练习：两人一组，分别完成场景 和 b 的工作。两人对对方的工作进行同行评审。根据同行的建议修订对以上三个问题的回答。一旦两人对两个场景中的工作都感到满意，就提交工作。以上所有内容最多花 15 分钟。每个场景的演示和同行评审最多 5 分钟。最多修订 5 分钟。对于同行评审，重点关注：\n模型陈述是清楚的吗？\n是否定义了所有的项？\n是否提供了所有必要元素？表达是清晰的吗？\n是否说明了假定？\n答案是否简洁（没有废话，没有死记硬背……）？\n模型陈述是清楚的吗？是否定义了所有的项？是否提供了所有必要元素？表达是清晰的吗？是否说明了假定？答案是否简洁（没有废话，没有死记硬背……）？类似于图 1.2 所示的多批次情形，只不过现在有两种处理。每种处理都随机分配了三个批次。与图 1.2 中的情形一样，假定随时间的变化是线性的。考虑响应变量的以下情形：\n高斯响应变量。\n响应变量 \\(y_{ijk}\\) 是第 \\(ijk\\) 个处理-批次-时间组合中 \\(N_{ijk}\\) 个观测中的有利结果数。\n响应变量是计数，假定具有泊松分布。\n对于每种情形：编写完整的模型指定，包括所有必要元素。\n假设随变化时间的线性响应对应于材料（例如食品、药物、农药）随时间的降解（例如失去效力、毒素积聚等）。研究人员想知道“哪种处理更好？”根据 . 中的模型参数，编写一个正式的原假设，为该问题提供一个具有统计可操作性的定义，并回答研究者的问题。\n\n第 ii. 部分只需要做一次。与模型指定不同，这里的答案应该同样适用于每个响应变量情形（如果你正确地完成了第 . 部分）。\n提示：你的“假设”实际上可能是一系列假设以及一个相关的方案，通过这些假设和方案来回答研究问题。请尽可能简洁地完成，并尽可能清晰、简洁地解释。如果你的答案超过半页纸，那就说明你没有做到简洁、清晰或简明扼要。类似于图 1.2 所示的多批次情形，只不过现在有两种处理。每种处理都随机分配了三个批次。与图 1.2 中的情形一样，假定随时间的变化是线性的。考虑响应变量的以下情形：高斯响应变量。响应变量 \\(y_{ijk}\\) 是第 \\(ijk\\) 个处理-批次-时间组合中 \\(N_{ijk}\\) 个观测中的有利结果数。响应变量是计数，假定具有泊松分布。\n对于每种情形：编写完整的模型指定，包括所有必要元素。\n假设随变化时间的线性响应对应于材料（例如食品、药物、农药）随时间的降解（例如失去效力、毒素积聚等）。研究人员想知道“哪种处理更好？”根据 . 中的模型参数，编写一个正式的原假设，为该问题提供一个具有统计可操作性的定义，并回答研究者的问题。\n对于每种情形：编写完整的模型指定，包括所有必要元素。假设随变化时间的线性响应对应于材料（例如食品、药物、农药）随时间的降解（例如失去效力、毒素积聚等）。研究人员想知道“哪种处理更好？”根据 . 中的模型参数，编写一个正式的原假设，为该问题提供一个具有统计可操作性的定义，并回答研究者的问题。第 ii. 部分只需要做一次。与模型指定不同，这里的答案应该同样适用于每个响应变量情形（如果你正确地完成了第 . 部分）。提示：你的“假设”实际上可能是一系列假设以及一个相关的方案，通过这些假设和方案来回答研究问题。请尽可能简洁地完成，并尽可能清晰、简洁地解释。如果你的答案超过半页纸，那就说明你没有做到简洁、清晰或简明扼要。","code":""},{"path":"chap2.html","id":"chap2","chapter":"第 2 章 设计要务","heading":"第 2 章 设计要务","text":"\n译者注：本章涉及实验设计的基础知识，译者提供一个辅助阅读材料：混乱数据分析：设计的实验(Milliken Johnson, 2009) 之第四章：实验设计基础。","code":""},{"path":"chap2.html","id":"sec2-1","chapter":"第 2 章 设计要务","heading":"2.1 将设计和目标转译为模型的入门思想","text":"在第 1 章中，我们看到完整地指定一个统计模型需要线性预测器 \\(\\symbf X\\symbf\\beta+\\symbf{Zb}\\)随机模型效应的分布 \\(\\symbf{b}\\sim N\\begin{pmatrix}\\symbf 0,\\symbf{G}\\end{pmatrix}\\)以随机效应为条件，数据的分布 \\(\\symbf y\\mid \\symbf b\\)连接函数 \\(g(\\symbf \\mu\\mid\\symbf b)=\\symbf X\\symbf\\beta+\\symbf{Zb}\\)此外，在第 1 章中我们看到该模型有两个目的。首先，它描述了产生观测的合理过程；其次，它能够对建模数据收集所追求目标进行所需的估计和推断。在本章中，我们考虑研究设计方式和最终模型之间最重要的桥梁。广义上讲，设计是指研究的组织和数据收集协议。根据语境，本书中的术语“设计”可能指正式设计的实验、调查设计、准实验设计、观察性研究 (formally designed experiments, survey design, quasi-experimental design,\nobservational studies) 等。我们将本章称为“设计要务”，因为将研究设计转译为统计模型是统计建模实践中至关重要的第一步。它也可能是最被低估和被忽视的。本章的目的是定义对统计建模至关重要的设计术语和概念，并介绍有助于准确地将研究设计转译为统计模型的技术。","code":""},{"path":"chap2.html","id":"sec2-1-1","chapter":"第 2 章 设计要务","heading":"2.1.1 设计问题引起的建模问题概述","text":"大多数建模问题都是由于对设计问题理解不足或应用不当。建模问题的主要原因包括：构建线性预测器时的不当决策。固定/随机模型效应的不当决策。观测以及（在较小程度上）随机模型效应的概率分布的不当决策。预期的模型指定与代码之间的不匹配。结果的不良转译，强调统计术语而非学科术语（研究主题所驱动的学科语言）。未能使用模型结果来明确实现目标。通常需要根据研究目标，对模型参数的线性组合进行相关的推断，很少只是估计模型参数。关于最后一点，“获得参数估计后如何处理”，我们将留给第 3 章，其中详细介绍了推断问题，以及从第 8 章开始的第三篇，其中介绍了 GLMM 应用。在本章中，我们重点关注：定义用于可视化和描述研究设计的概念和术语（2.2 节）。将研究设计转译为统计模型（2.3 节）。区分固定效应和随机效应（2.4 节）。决定观测的分布（2.4 节）。将概念扩展到更复杂的研究设计（2.5 节）。编写准确描述我们想要拟合的模型的 SAS 代码（附录 B）。注意：附录 B 专门针对 SAS 软件进行说明。但更广泛的经验教训也适用于其他软件：统计模型从根本上运用了矩阵运算，我们必须了解命令是如何实现预期模型的。","code":""},{"path":"chap2.html","id":"sec2-1-2","chapter":"第 2 章 设计要务","heading":"2.1.2 源于设计要务的建模选择概述","text":"在许多建模情况下，不存在“唯一一个”正确的线性预测器。通常，我们可以选择许多合理的线性预测器。尽管所有这些都可能在技术上准确描述数据，但我们通常发现某个线性预测器能更好地帮助我们实现研究目标。第 1 章中的线性回归示例提供了一个简单的说明。目标是估计响应随时间的线性变化。两个可能的线性预测变量为\\(\\beta_0+\\beta_1X_i\\) 其中 \\(X_i=0,1,\\ldots,10\\) 表示观测时间\\(\\eta+\\tau_i\\) 其中 \\(=1,2,\\ldots,10\\)选项 1 将时间定义为数值预测器——直接线性回归效应。选项 2 将时间定义为分类效应。作为数据的描述，两者在技术上都是正确的。\n但是，选项 1 能直截了当地回答研究目标，而选项 2 使解释变得模糊，这是没有必要的。第 1 章中的两均值模型提供了另一个说明。两个可能的线性预测器为\\(\\eta+\\tau_i\\) 其中 \\(=1,2\\)\\(\\eta+\\tau_i+p_j\\) 其中 \\(=1,2\\) 指处理水平，\\(p_j;j = 1,2,...,J\\) 表示配对效应选项 1 和选项 2 之间的选择显然是一个设计问题。研究是通过随机分配给独立个体的处理进行的，还是在每个配对的个体间随机分配的？在继续进行统计建模之前，我们必须解决该设计问题。在线性回归示例中，我们必须确定上面编写的线性预测器是否充分，或者其多批次形式 \\(\\beta_{0j}+\\beta_{1j}X_{ij}=\\beta_0+b_{0j}+\\left(\\beta_1+b_{1j}\\right)X_{ij}\\) 是否更合适。如果多批次形式更合适，特定于批次的效应 \\(b_{0j}\\) 和 \\(b_{1j}\\) 也有概率分布吗？再次强调，这些都是设计问题。观测是独立的，还是以某种方式分组？如果将它们分组，这些组是整个总体，还是代表更大总体的样本？重复前述内容：我们必须先解决这些设计问题，然后才能继续进行统计建模。本章可能看起来像是属于设计教科书的内容，你可能会对自己说，“我以为这应该是一本关于建模的教科书。” 严格来说，你可以在不了解通常在实验设计课程中教授的思维方式的情况下学习统计模型（实际上，你也完全可以不参考概率分布理论进行学习）。但是，仅仅因为可以这么做并不意味着你应该这样做。正如上述例子所示，如果很少提及设计原理，可能会导致我们机械地遵循一套严格的步骤，缺乏真正的理解。对设计原理和概率论的工作知识对我们在本书的工作至关重要。我们的目标是让你具备灵活性和适应各种场景的能力，真正掌握 GLMM 工作中的复杂细节。本章重点讨论设计问题，即使对于不涉及设计实验的模型也是如此。重复本节中的一句话，经验表明，统计建模中最严重的错误（在给定数据架构或目标的情况下毫无意义的模型）几乎总是由于对设计的理解不足造成的。即使是调查、准实验和观察研究也必须利用设计原则，以便对数据提出的问题获得可信的答案。因此，本章将不可避免地涉及设计概念。特此警告。","code":""},{"path":"chap2.html","id":"sec2-2","chapter":"第 2 章 设计要务","heading":"2.2 描述数据架构以促进模型指定","text":"在简单的场景中，研究设计所遵循的统计模型可能很容易确定。然而，研究越复杂，从设计到模型的转译过程就越困难。从本节开始，我们将介绍一个过程，用于构建与给定研究设计和目标一致的模型。我们通过一个示例开始。假设一个学区正在实施一项专业发展计划。在要求该学区所有学校参与之前，该学区希望看到该计划的有效性证据。为了了解这一点，它进行了一项研究，设定如下：从区内学校中随机抽取十所学校每所学校选出四名教师：两名参加专业发展计划，两名不参加假设所有 40 人都教授相同的年级和相同的科目为评估该计划，在研究结束时对每位学生进行评估，并测量他们学习成绩的提升情况对于这个例子，暂时不要评判研究设计的质量——我们的目的只是通过设计架构来构建一个统计模型。在我们为这个例子构建适当的模型之前，我们需要先了解一些基本概念和术语。","code":""},{"path":"chap2.html","id":"sec2-2-1","chapter":"第 2 章 设计要务","heading":"2.2.1 每项研究都有一个蓝图","text":"我们首先将研究设计可视化：一图胜千言。当登山者描述他们的登顶路线时，总会有一个他们称为关键 (crux) 的部分——攀登过程中最困难的部分。如果你能渡过关键部分，就能到达顶峰。在将数据和目标转译为模型的过程中，关键是确保线性预测器准确描述研究设计对你观察到的数据的影响。要解决关键问题，图片可能是最有价值的工具。在对研究设计可视化时，我们借鉴了农业实验的设计。当农业研究人员在田间进行实验时，他们会在所谓的平面图\n(plot plan) 中勾勒出实验的布局。平面图只是一张地图，显示所有观测的位置、应用处理水平的位置、是否存在配对或区组等。虽然这起源于农业，我们可以为任何与农业无关的研究绘制平面图。我们使用研究蓝图 (study blueprint) 一词来指代研究的可视化。蓝图的组成部分包括：单元布局。待分配的处理。随机化方案，即将处理分配给单元的方式。这三个元素结合起来构成了研究蓝图。蓝图有助于将研究的文字描述转译为图形，从而区分研究中使用的处理和应用处理的单元结构。","code":""},{"path":"chap2.html","id":"sec2-2-2","chapter":"第 2 章 设计要务","heading":"2.2.2 两个研究蓝图示例","text":"在本节中，我们使用两个场景来说明如何根据研究设计描述构建研究蓝图。在 2.3 节中，我们使用生成的可视化结果来构建适合每种场景的统计模型。","code":""},{"path":"chap2.html","id":"sdc2-2-2-1","chapter":"第 2 章 设计要务","heading":"2.2.2.1 场景 1 的研究蓝图","text":"考虑上面介绍的学校示例：十所学校，每所学校四名教师，我们要评估专业发展计划的有效性。让我们使用上面定义的组成部分构建此场景的蓝图。单元布局：在本研究中，有多个不同尺寸的单元。最大的单元是学校。中型单元是每位参与教师使用的课堂。最小单元是学生（每个班级的学生）。每所学校内有四个课堂，每个课堂有 \\(s\\) 名学生。\n图 2.1.的左侧显示了研究中的三种不同尺寸的单元，图 2.1.的右侧显示了学校 1 的这些单元的布局。相同的布局适用于十所学校中的每一所。\n当我们将布局扩展到学校 1 之外时，我们就得到了研究蓝图的单元布局，如图 2.1.B 所示。这里我们只包含了四所学校的布局以避免冗余。其余六所学校的布局相同。\n图 2.1.的左侧显示了研究中的三种不同尺寸的单元，图 2.1.的右侧显示了学校 1 的这些单元的布局。相同的布局适用于十所学校中的每一所。当我们将布局扩展到学校 1 之外时，我们就得到了研究蓝图的单元布局，如图 2.1.B 所示。这里我们只包含了四所学校的布局以避免冗余。其余六所学校的布局相同。待分配的处理：我们有一个处理因素，有两个水平（参与者或非参与者）。图 2.1.C 说明了处理水平。随机化方案。每所学校选出四名教师参与这项研究。每个处理水平（发展计划或对照）随机分配两名教师。\n通过使用随机化方案将图 2.1.B（单元布局）与图 2.1.C（处理）结合，我们已准备好所有组成部分来创建图 2.1.D 所示的研究蓝图。现在我们已经完成了场景 1 的研究蓝图，我们可以在图 2.1.D 中看到每所学校都经历了两种处理。参与的课堂按学校分组。课堂是分配处理的单元。在同一个老师的课堂里所有的学生都接受同样的处理。观测对学生进行，但个别学生接受处理的分配并不是独立的。","code":""},{"path":"chap2.html","id":"sdc2-2-2-2","chapter":"第 2 章 设计要务","heading":"2.2.2.2 场景 2 的研究蓝图","text":"相反，假设我们只能在随机选择的一半的学校试行专业发展计划，并且研究方案要求特定学校参与研究的所有教师要么参加专业发展 (professional development, PD)，要么不参加。这导致研究设计略有不同。考虑如下的场景 2：从该地区的学校中随机抽取了十所学校。其中五所学校被随机分配参加专业发展计划，另外五个作为非参与的对照。从每所学校中随机挑选两名教师。假设所有 20 名学生都教授相同的年级和相同的科目。为评估该计划，在研究结束时对每位学生进行评估，并测量他们学习成绩的提升情况。使用本节介绍的组成部分，我们可以构建研究蓝图，如图 2.2 所示。每所学校只经历一种处理。换句话说，一所学校要么参与专业发展计划，要么不参与。现在，处理被分配到的单元是学校，而不是课堂。\n尽管只在场景 1 和 2 的背景下概述了构建研究蓝图的过程，但所有研究描述都可以遵循相同的一般过程。这种将研究的所有元素连接在一起的图形对于模型构建至关重要。接下来我们定义一些关键术语，使我们能够将研究蓝图与统计模型联系起来。","code":""},{"path":"chap2.html","id":"sec2-2-3","chapter":"第 2 章 设计要务","heading":"2.2.3 研究关键要素的术语","text":"既然我们有了研究的蓝图，我们就需要通过语言来描述与构建合适的线性预测器相关的研究关键要素。这些关键要素如下：因素 (factor)：我们想要估计或检验其对响应效应的自变量。因素可以是分类（类别）或直接（数值）变量。水平 (levels)：因素的具体类别（针对分类变量）或数量（针对直接变量）。重复单元 (unit replication)：被独立分配一个因素水平的最小实体，关键在于独立分配。在设计的实验中，这通常称为实验单元 (experimental unit). 我们通过随机化实现了对因素水平的单元的独立分配。在观察性研究中，重复单元通常称为观察单元 (observational unit). 在观察性研究中，我们不控制因素水平的单元分配，而是由自然进行分配。从建模的角度来看，实验和观察研究得到的线性预测器是相似的。抽样单元 (sampling unit)：我们收集数据的最小实体。我们还将抽样单元称为观察的单元 (unit observation) 但不要与上面提到的观察单元 (observational unit) 混淆。请注意，抽样单元总是包含在重复单元中。如果抽样单元与重复单元相同，我们说抽样单元是真正的重复。伪重复 (pseudo replicate)：被错误地识别为重复单元的单元。这通常发生在当没有独立分配给处理因素的单元被当作真正的重复进行分析时。例如，在本节中的学校例子中，学生是抽样单元而不是重复单元。如果在数据分析中将学生视为重复，就会出现伪重复。伪重复会给出不准确，甚至是严重不准确的推断统计结果。分组的单元 (grouped units)：当重复单元被组合在一起时——理想情况下是因为它们在某些相关方面相似时，就会发生这种情况。在实验设计中，这些组称为区组 (blocks). 在抽样调查中，这些组可能称为层或集群 (strata clusters)。术语有所不同，但从建模的角度来看，思想是相同的。确定分组单元（例如区组）的一个有用的方法是确定因素水平如何分配给单元。如果研究设计中存在区组，则必须在模型中考虑它。为了确保我们构建的统计模型适用于研究设计，正确识别真实重复并识别任何单元分组至关重要。这确保了我们定义的任何模型都能准确地测量结果。为了进一步阐明我们的术语，请考虑以下示例。示例 2.1  考虑第 1 章中讨论的两处理示例，包括两独立样本的情况和配对观测的情况。因素：处理。水平：处理 1 和处理 2.重复单元：\n两处理：分配给特定处理的个体。\n配对：配对的单个成员。\n两处理：分配给特定处理的个体。配对：配对的单个成员。抽样单元：与重复单元相同。伪重复：无。分组的单元：\n两处理：因素水平被随机分配给每个个体。没有单元分组在一起，因此不存在区组。\n配对：因素水平随机分配给一对中的单元。因此，每对代表一个区组。\n两处理：因素水平被随机分配给每个个体。没有单元分组在一起，因此不存在区组。配对：因素水平随机分配给一对中的单元。因此，每对代表一个区组。示例 2.2  考虑第 1 章中讨论的多批次线性回归示例，我们有一个二项响应变量，并观测一个定量解释变量 \\(X\\)，其中我们测量四个批次 \\(X\\) 随时间的变化。因素：回归方程中使用的预测变量 \\(X\\)。水平：\\(X=0,3,6,9,12,18,24,36,48\\) 月。重复单元：每个批次 \\(X\\) 的每个水平下的观测结果。抽样单元：与重复单元相同。伪重复：无。分组的单元：因素水平分配给批次内的单元。因此，每个批次代表一个区组。示例 2.3  考虑本节开头介绍的场景 1。有十所学校，每所学校都有四名教师被选来参加这项研究：两名教师被分配参加专业发展计划，两名则没有。收集每位教师课堂上所有学生的信息，以评估学生的成绩提升。因素：专业发展计划。水平：参与者和非参与者。重复单元：随机分配到某个计划水平的教师。抽样单元：每个老师的课堂里的学生。伪重复：同一个老师的课堂里的所有学生都暴露在相同的环境中。因此，学生是伪重复，而不是真正的重复。分组的单元：因素水平随机分配给每所学校的教师。因此，每所学校代表一个区组。示例 2.4  考虑本节前面的场景 2。有十所随机选择的学校，其中五所学校被随机分配参加专业发展计划，五所学校被分配不参加。收集每位教师课堂上所有学生的信息，以评估学生的成绩提升。因素：专业发展计划。水平：参与者和非参与者。重复单元：随机分配到某个计划水平的学校。抽样单元：每个老师的课堂里的学生。伪重复：与示例 2.3 一样，学生是伪重复而不是真正的重复。此外，教师现在也成为伪重复，因为同一所学校的所有教师都分配了相同的处理。分组的单元：因素水平随机分配给每所学校。因为学校是重复单元，并且相同的因素水平应用于整个学校，所以我们不再有区组。本节讨论的术语以及本书后续部分使用的术语均源自实验设计的视角。请注意，其他学科对类似概念使用不同的术语。强调这些差异有助于明确概念。例如，在社会科学中，多水平模型 (multi-level models) 使用“水平” (level) 一词来指代观测的结构、分组的单元（如区组）和重复单元。我们重申，在本书中，“水平”一词指的是因素水平；区组、重复单元和观察单元指的是观测的结构。最后一点：请注意，当在一个单元内分配因素水平时，就会出现某种形式的区组。","code":""},{"path":"chap2.html","id":"sec2-3","chapter":"第 2 章 设计要务","heading":"2.3 从研究蓝图到统计模型","text":"2.2 节中的研究蓝图和术语为我们从数据集描述过渡到统计模型奠定了坚实的基础。在本节中，我们将介绍实现此转换的过程。这涉及使用调整的方差分析 (repurposed ANOVA) 表作为工具。我们从变异源 (sources variation) 开始，研究上一节中创建的蓝图（或平面图）有助于我们可视化。我们将它们列出来，就像在 ANOVA 表中列出变异源一样。然后，我们将这些变异源与旨在线性预测器中表示它们的效应以及相关的概率分布联系起来（如果有意义的话）。我们将这种“从设计到调整的 ANOVA 再到模型”的工具称为“Fisher 会怎么做？” (Fisher , WWFD)，原因将在下一节中解释。WWFD 回到了建模的 ANOVA 根源，将 ANOVA 的思考过程用于广义线性混合模型 (GLMM) 的设定中。","code":""},{"path":"chap2.html","id":"sec2-3-1","chapter":"第 2 章 设计要务","heading":"2.3.1 Fisher 会怎么做？","text":"Speed (2010) 发表在 IMS Bulletin 的一篇专栏间接地启发了本节的标题。Fisher 在 Yates 题为 Complex Experiments (1935) 文章之后的评论以及 Fisher 对早期建模工作的反应（Speed 使用术语“困惑”和“愤怒”）提供了直接的推动力。Fisher 表示，复杂的研究设计可以分解为地形 (topographical) 和处理部分，然后结合起来形成合理的分析方法。Fisher 使用“地形”一词是因为他谈论的是农业实验和土地地形。我们可以扩大地形一词的使用范围，以涵盖研究中所有单元的组织方式，而不仅仅是为了设计的实地实验。本着同样的精神，Federer (1955) 拒绝使用术语 “experimental design” (“design actually use experimental”)，而使用术语实验设计 (experiment design) 和处理设计 (treatment design). Milliken Johnson (2008) 则提到了研究设计 (study design) 和处理设计 (treatment design). 本书自此将遵循 Milliken Johnson 的术语。传统上，ANOVA 被理解为算术平方和、均方及其相关二次形的集合。对于 GLMM 来说，以这种方式看待 ANOVA 是一种无益的干扰。这种局限性的观点忽略了使用 GLMM 时必不可少的关键组成部分。从 GLMM 的角度理解 ANOVA 有助于思考产生数据的过程。通过增加这种必要的视角，我们现在有了一个构建 GLMM 所需元素的有力工具。虽然我们永远无法真正知道 Fisher 会怎么做，但这种方法试图根据当代统计建模的发展来遵循他的想法。具体怎么做呢？我们将研究设计的视角划分为单元的组织方式、地形（研究设计）结构、处理结构以及处理如何应用于研究设计的单元。请注意，这与 2.2.1 节中介绍的研究蓝图的组成部分紧密相关。对于设计的每个方面，地形和处理，我们只写出变异源和相关的自由度。我们将这些调整的 ANOVA 表称为框架 ANOVA 表 (skeleton ANOVA table). WWFD 是把这些表组合起来并使用它们构建合理 GLMM 的过程。我们使用 WWFD 的目标是构建一个统计模型，该模型考虑了研究要素以及影响预期响应的处理设计（线性预测器），并考虑数据变异的概率分布。在框架 ANOVA 表中：我们纳入：\n变异源\n相关自由度\n变异源相关自由度我们排除：\n平方和\n符号表示\n平方和符号表示变异源确定了研究的关键要素。参考研究蓝图来确定相应的自由度应该很简单。回想一下，研究蓝图的组成部分为单元布局。待分配的处理。随机化方案，即将处理分配给单元的方式。WWFD 使用相同的组成部分。我们按如下方式使用它们。WWFD 的第一部分是确定地形结构。在这一部分，我们确定了单元的组织结构和相应的自由度 (degree freedom, DF). 我们的重点完全放在单元本身上。这一步排除了任何涉及处理的内容；我们在这一部分的重点完全放在单元的布局上。地形结构中列出的所有术语都将延续到完整的模型中。","code":""},{"path":"chap2.html","id":"sec2-3-1-1","chapter":"第 2 章 设计要务","heading":"2.3.1.1 从设计到模型过程图示：场景 1","text":"要了解其工作原理，请考虑 2.2.2 节中的示例 2.3。WWFD 的第一部分如图 2.3.所示。在图中，我们包括了研究蓝图中相关的组成部分以及地形结构。此后我们将简单引用研究蓝图，而不将其包含在图中。\n在地形结构中，我们确定了仅归因于单元的所有变异源。这些变异源是学校、分组的单元和校内的课堂，使用约定 classroom(school) 列出。在研究中，有 10 所学校，每所学校内有 4 个课堂。我们有 10 × 4 = 40 个重复单元，由此 40 - 1 = 39 个总自由度。请注意，将地形结构中变异源的自由度相加也等于 39。该数学检查可用于验证是否已正确识别变异源。从技术上讲，我们应该在图 2.3.中增加一行，用于描述课堂内的学生。简便起见，我们假设每个课堂都有一个学生表现的度量，例如学生成绩的班级均值。因此，在这个例子中，classroom(school) 是观察单元和重复单元。WWFD 的第二部分是识别处理结构。这包括识别与处理相关的所有变异源，并确定其相应的自由度。在我们的示例中，图 2.3.B 说明了 WWFD 过程的第二部分。与之前一样，我们包含了研究蓝图中相关的构组成部分以供参考。\n在处理结构中，我们确定了仅与处理相关的所有变异源。在该研究中，一种处理有两个水平，因此有 1 个自由度。同样，我们总共有 39 个自由度，这意味着剩下的 38 个自由度属于 Fisher 称为“平行” (“parallels”) 的总类别。或者，我们可以将它们称为残余 (leftovers). 无论如何，该行本质上是一个占位符，不会延续到最终模型。WWFD 的第三部分是组合结构 (combined structure). 这整合了地形和处理结构，并确定了研究的所有变异源及其相应的自由度。图 2.4 显示了这样的安排。\n行的排列很重要，区组划分—— School ——出现在第一行，处理因素—— development program ——出现在第二行。classroom(school) 出现在第三行，表示 classroom(school) 作为 development program 的重复单元。箭头强调了这种关系。始终将与给定处理因素相关的重复单元置于处理效应正下方的行中。“Combined Structure” 是通过将地形结构表和处理表中的变异源沿着它们所在行向右移动得到的。组合表对应于经典 ANOVA 表的变异源和 DF 列。请注意左表中的 classroom(school) 在合并表中修改为 “classroom(school) |\nDP”，将其解读为“考虑专业发展后的校内课堂”。这反映了处理对重复单元的分配。另请注意，在组合表的 DF 列中，DP 的考虑使得 classroom(school) 的 DF 减去相应值。处理分配前 classroom(school) 有 30 个 DF，考虑 DP 后有 30 - 1 = 29 个 DF. 处理的 DF 总是以这种方式从其相应的重复单元的 DF 中移除。从组合 ANOVA 的结构可以看出，来自 School, DP 和 classroom(school) |\nDP 的项需要纳入线性预测器。WWFD 过程的第四步是将组合表中的变异源与模型效应及其假定的概率分布（如果有意义）相关联。表 2.1 说明了这一点。\n根据表 2.1，我们现在可以编写统计模型。在模型方程形式中，我们有\\[y_{ijk}=\\eta+\\tau_i+s_j+u_{ijk};\\quad u_{ijk}\\mathrm{~iid~}N\\big(0,\\sigma_u^2\\big)\\]在概率分布形式中—— GLMM 系列的所有模型的首选形式——模型必需元素为线性预测器 \\(\\eta_{ij}=\\eta+\\tau_i+s_j\\)分布 \\(y_{ijk}\\sim N\\left(\\mu_{ij},\\sigma_u^2\\right)\\)连接函数：恒等 \\(\\eta_{ij}=\\mu_{ij}\\)请注意，对于此模型，classroom(school) |\nDP 变异源对应于观察单元。与此来源相关的模型中的项此后在本书中称为单元效应 (unit effect). 对于具有高斯数据的模型，我们可以使用 “Model Effect” 列中的元素以模型方程形式构建适当的模型。在具有非高斯响应变量的模型中，“Resulting\nObservation” 列对于列出模型的所有必需元素至关重要。三条要点：第一条是一个定义。定义：合理的模型是指变异源与模型中解释这些源的项之间存在一一对应关系的模型。不符合上述定义的模型，无论是没有解释它的项的变异源，还是具有多个混淆 (confounded) 的项，根据定义都不是合理的模型。一个合理的模型不一定就是最好的模型——对于一个给定的数据集，可能有不止一个合理的模型，但是不合理的模型应取消作为候选模型的资格，并被认为不合适且不使用，参见第 ?? 章和第 ?? 章。除单元水平之外的其他效应可能具有相关的概率分布。我们在第 1 章的 LMM 和 GLMM 示例中看到了这一点。有关此问题的更多信息请参见 2.4.1 节。","code":""},{"path":"chap2.html","id":"sec2-3-1-2","chapter":"第 2 章 设计要务","heading":"2.3.1.2 从设计到模型过程图示：场景 2","text":"图 2.2 显示了学校研究的另一种设计，将 DP 处理分配给学校而不是每所学校内的课堂。为了开发模型，我们使用与场景 1 相同的策略。图 2.5 显示了并排的 WWFD 框架 ANOVAs.\n请注意其相对于表 2.1 的重新安排。对于场景 1 设计，DP 处理被分配到教师的课堂，而在嵌套设计中，处理被分配到学校水平。因此，DP 置于学校上方而不是课堂上方，这表明了分配的位置以及自由度受到的影响。在合并表中，重复单元 school | DP，解读为“考虑专业发展后的学校”，出现在处理因素 DP 所在行的正下方。表 2.2 显示了合并表，其中包含变异源及其相关的模型效应。\n模型方程形式的结果模型为\\[y_{ijk}=\\eta+\\tau_{}+s_{ij}+c_{ijk};\\quad s_{ij}\\mathrm{~iid~}N{\\left(0,\\sigma_{s}^{2}\\right)};\\quad c_{ijk}\\mathrm{~iid~}N{\\left(0,\\sigma_{c}^{2}\\right)}\\]在首选的 GLMM 形式中，模型必需元素为线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+s_{ij}\\)分布：\n\\(s_{ij}\\mathrm{~iid~}N\\left(0,\\sigma_s^2\\right)\\)\n\\(y_{ijk}\\mid s_{ij}\\sim N\\left(\\mu_{ij},\\sigma_c^2\\right)\\)\n\\(s_{ij}\\mathrm{~iid~}N\\left(0,\\sigma_s^2\\right)\\)\\(y_{ijk}\\mid s_{ij}\\sim N\\left(\\mu_{ij},\\sigma_c^2\\right)\\)连接函数：恒等 \\(\\eta_{ij}=\\mu_{ij}\\)","code":""},{"path":"chap2.html","id":"sec2-4","chapter":"第 2 章 设计要务","heading":"2.4 分布要务","text":"在 2.3 节中，我们专注于将线性预测器完全构建为将连接划分为其组成部分的方程。在本节中，我们转向模型构建中所需的最终问题。它们为：区分固定模型效应和随机模型效应，即 \\(\\symbf{X\\beta}\\) 中包含什么，\\(\\symbf{Zb}\\) 中包含什么？指定随机效应的概率分布。指定观测的概率分布，即 \\(\\symbf y \\mid\\symbf b\\) 的分布尽管 Lee Nelder (1996, 2006) 讨论了双广义线性模型 (doubly generalized linear models)，其中 \\(\\symbf y \\mid\\symbf b\\) 和 \\(\\symbf b\\) 可能呈非高斯分布。我们在本书中将注意力限制在高斯随机效应上，即 \\(\\symbf{b}\\sim N(\\symbf{0},\\symbf{G})\\)。出于我们的目的，我们需要决定哪些效应包含随机效应向量 \\(\\symbf b\\) 以及 \\(\\symbf G\\)——\\(\\symbf b\\) 的方差-协方差阵——的形式。以下两个小节中的示例说明了所需的决策过程。","code":""},{"path":"chap2.html","id":"sec2-4-1","chapter":"第 2 章 设计要务","heading":"2.4.1 模型效应：固定或随机","text":"为了说明所涉及的决策及其制定方式，请考虑学校专业发展干预示例。图 2.1 所示设计的线性预测器为 \\(\\eta_{ij}=\\eta+\\tau_i+s_j\\)。根据定义，效应 \\(\\eta\\) 是固定截距，因此它自动成为 \\(\\symbf {X\\beta}\\) 的一部分。对于其余项，关键问题是每个效应的水平是代表更大关注总体的样本，还是其本身就是明确的关注所在，从而代表了争整个总体？是否存在与所讨论的效应相关的概率分布？对于专业发展效应 \\(\\tau_i\\) ，这两个水平（参与者或非参与者）显然是仅有的两个关注水平——它们是研究的明确动机，并不代表更大的总体。因此 \\(\\tau_i\\) 是固定的并且属于 \\(\\symbf {X\\beta}\\)。另一方面，该研究的描述称，“随机选择了十所学校。”这一点似乎也很明确。研究中观察到的学校代表了该学区的所有学校（通过外推也可能代表来自类似学区的所有学校）；本研究完全有可能选择其他的学校，而这里所观察到的这十所学校仅代表了一种随机抽样情况。由此可见，学校效应是随机的，应当纳入 \\(\\symbf {Zb}\\) 部分。这会影响我们编写组合 WWFD ANOVA 以及模型效应表的方式。假定随机学校效应，我们重写表 2.1，如表 2.3 所示。\n省略假定的分布隐式地将效应定义为固定的。因此，如果我们假定研究中的十所学校是全部总体，将得到固定学校效应，那么表 2.1 和由此产生的模型是合适的。根据表 2.3，假定学校效应是随机的，混合模型所需的元素为线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+s_j\\)分布：\n\\(s_j\\mathrm{~iid~}N\\left(0,\\sigma_s^2\\right)\\)\n\\(y_{ij}\\mid s_j\\sim N\\left(\\mu_{ij},\\sigma_u^2\\right)\\)\n\\(s_j\\mathrm{~iid~}N\\left(0,\\sigma_s^2\\right)\\)\\(y_{ij}\\mid s_j\\sim N\\left(\\mu_{ij},\\sigma_u^2\\right)\\)连接函数：恒等 \\(\\eta_{ij}=\\mu_{ij}\\)\n在开发混合模型估计和推断理论时，我们必须使用矩阵形式的模型。对于上述模型，线性预测器为 \\(\\symbf \\eta=\\symbf{X\\beta}+\\symbf{Zb}=\\begin{bmatrix}\\symbf 1&\\mathbf{X}_{\\mathrm{\\rho}}\\end{bmatrix}\\begin{bmatrix}\\eta\\\\\\symbf{\\rho}\\end{bmatrix}+\\mathbf{Z}_s\\mathbf{s}\\)，其中 \\(\\symbf{s}\\sim N\\left(0,\\symbf{}{\\sigma}_s^2\\right)\\)，\\(\\symbf 1\\) 为 \\(N × 1\\) 全一向量，\\(\\mathbf{X}_{\\mathrm{\\rho}}\\) 为计划参与者的 \\(N × 2\\) 设计矩阵，\\(\\symbf{\\rho}=\\begin{bmatrix}{\\rho}_1\\\\{\\rho}_2\\end{bmatrix}\\) 为参与者效应向量，\\(\\symbf s\\) 为学校效应向量以及 \\(\\symbf Z_s\\) 为学校效应的设计矩阵（提问：\\(\\symbf Z_s\\) 维度为何？）。","code":""},{"path":"chap2.html","id":"sec2-4-2","chapter":"第 2 章 设计要务","heading":"2.4.2 响应变量分布","text":"关于模型的其余决策涉及观测的分布。如果我们有一个仅固定效应模型，这意味着指定 \\(\\symbf y\\) 的分布。如果我们有一个混合模型，我们指定 \\(\\symbf y\\mid\\symbf b\\) 的分布。请注意，从 2.2 节开始的“从设计到模型过程”中，在我们完全确定了变异源、用于解释这些变异源的模型项、线性预测器以及假定为随机的任何模型效应的分布之前，我们不会对观测数据展开任何讨论。（初学者经常想跳过一些步骤——这肯定会导致模型构建出现问题）。继续我们的学校、课堂、专业发展示例，一旦我们确定了 WWFD ANOVA 表的组合变异源和模型效应列，我们需要确定以下内容观察的单元 (unit observation) 是什么？通常，它是表中的最后一行，如表 2.1 和表 2.2 中所示。具体对学生进行了哪些观察？到底如何测量“成绩提升”？这是成绩测试的结果吗？前测和后测有什么区别吗？是否是分类的，例如“熟练/不熟练”，测量的类型决定了我们对分布的选择，或者至少限制了我们对候选分布的选择。在表 2.1 和 2.2 中，这些问题的答案是测量在课堂水平上进行我们可以假定测量服从高斯分布相反，假设对每个学生的测量是“熟练”或“不熟练”，这是伯努利变量。假设课堂内的学生分数是独立的，则课堂水平的测量是每个班级 \\(s\\) 名学生中的 \\(y_{ijk}\\) 名学生，其中 \\(y_{ijk}\\) 是课堂 \\(k\\) 和学校 \\(j\\) 中接受 DP 处理 \\(\\) 的熟练学生人数。在这种情况下，\\(y_{ijk}\\thicksim\\text{Binomial}\\left(s,\\pi_{ij}\\right)\\)，其中 \\(\\pi_{ij}\\) 表示在教师分配到处理 \\(\\) 并在学校 \\(j\\) 任教条件下，学生达到熟练程度的概率。表 2.4 显示了修订的 WWFD ANOVA.\n基于表 2.4 编写的一个典型模型是具有以下元素的 GLM线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+s_j\\)分布：\\(y_{ijk}\\sim\\text{Binomial}\\left(s,\\pi_{ij}\\right)\\)连接函数：Logit，\\(\\eta_{ij}=\\log\\biggl[\\pi_{ij}/\\biggl(1-\\pi_{ij}\\biggr)\\biggr]\\)请注意，线性预测器不受 \\(y_{ijk}\\) 分布变化的影响。变化的元素是分布和连接函数。仔细观察表 2.4 和所得模型就会发现一个问题。如上所述，没有任何效应或参数可以独立解释观察单元水平的变异。对于高斯数据，\\(\\sigma_u^2\\) 解释了单元水平的方差。上面的二项 GLM 隐式地将单元水平方差定义为 \\(\\pi_{ij}\\left(1-\\pi_{ij}\\right)\\) 的函数，这意味着单元水平方差没有相应的参数——它完全由线性预测器决定，而线性预测器只是学校和 DP 处理的函数。上述模型不符合合理模型的要求：变异源和模型效应之间必须存在一一对应关系。无法考虑所有变异源的模型通常会产生不准确（通常是严重不准确）的推断统计量。统计建模中的专业术语是过度分散 (overdispersion)：数据的变异性比模型能解释的要大。在过度分散的模型中，标准误往往向下偏差，而检验统计量则向上偏差。有关过度分散的深入讨论，请参阅第 ?? 章和第 ?? 章。消除表 2.4 中模型过度分散的方法是添加随机单元水平的效应及其假定的分布。表 2.5 显示了两种常见方法。\n版本 1 得到了本书称为 Logit-Normal GLM 的模型。其所需元素为线性预测器：\\(\\eta_{ijk}=\\eta+\\tau_i+s_j+u_{ijk}\\)分布：\n\\(y_{ijk}\\mid u_{ijk}\\sim\\mathrm{Binomial}\\left(s,\\pi_{ijk}\\right)\\)\n\\(u_{ijk}\\mathrm{~iid~}N\\left(0,\\sigma_u^2\\right)\\)\n\\(y_{ijk}\\mid u_{ijk}\\sim\\mathrm{Binomial}\\left(s,\\pi_{ijk}\\right)\\)\\(u_{ijk}\\mathrm{~iid~}N\\left(0,\\sigma_u^2\\right)\\)连接函数：\\(\\eta_{ijk}=\\log\\left[\\pi_{ijk}/\\left(1-\\pi_{ijk}\\right)\\right]\\)\n版本 2 为 Beta-Binomial GLM。其所需元素包括线性预测器：\\(\\eta_{ijk}=\\eta+\\tau_{}+s_{j}\\)分布：\n\\(y_{ijk}\\mid p_{ijk}\\thicksim\\mathrm{Binomial}{\\left(s,p_{ijk}\\right)}\\)\n\\(p_{ijk}\\sim\\mathrm{Beta}\\left(\\mu_{ijk},\\varphi\\right);0<\\mu_{ijk}<1\\)\n\\(y_{ijk}\\mid p_{ijk}\\thicksim\\mathrm{Binomial}{\\left(s,p_{ijk}\\right)}\\)\\(p_{ijk}\\sim\\mathrm{Beta}\\left(\\mu_{ijk},\\varphi\\right);0<\\mu_{ijk}<1\\)连接函数：\\(\\eta_{ijk}=\\log\\left[\\pi_{ijk}/\\left(1-\\pi_{ijk}\\right)\\right]\\)请注意，在 Beta-Binomial GLM 中，二项概率是随机变量。假定 \\(p_{ijk}\\)（二项概率）的贝塔分布解释了单元水平的变异。遵循 Ferrari Cribari-Neto (2004)，贝塔分布以模型友好的形式给出。或者，我们可以使用数理统计教科书的形式，\\(p_{ijk}\\sim\\mathrm{Beta}\\left(\\alpha_{ijk},\\beta_{ijk}\\right)\\) 其中 \\(\\alpha_{ijk}=\\varphi\\mu_{ijk}\\) 以及 \\(\\beta_{ijk}=\\varphi\\Big(1-\\mu_{ijk}\\Big)\\)。Logit-Normal GLM 和 Beta-Binomial GLM 都是“合理”模型，因为它们都满足模型中的项解释了所有变异源的要求。忽略用于解释单元水平变异的项是不合理模型的一个例子。在线性预测变量中包含 \\(u_{ijk}\\)，也就是说，在 Beta-Binomial 模型中使用 Logit-Normal 线性预测变量，是非合理模型的另一个例子：在线性预测变量中包含 \\(u_{ijk}\\) 会将其方差分量与 \\(p_{ijk}\\) 的贝塔分布的参数混淆，在这样过度指定 (overspecified) 的模型中，估计和推断效果不佳。如果我们像 2.4.1 节中所做的那样假定学校效应是随机的，那么我们会将 \\(s_j\\mathrm{~iid~}N\\left(0,\\sigma_s^2\\right)\\) 添加到分布列表中，并将观测的分布修改为 \\(y_{ijk}\\mid s_j,u_{ijk}\\)（对于 Logit-Normal）或 \\(y_{ijk}\\mid s_j,p_{ijk}\\)（对于 Beta-Binomial）。这些模型分别是 Logit-Normal GLMM 和 Beta-Binomial 混合模型。第 ?? 章和第 ?? 章介绍了 Logit-Normal GLMM 实现的详细示例。第 ?? 章介绍了 Beta-Binomial GLMM 示例。最后一点。表 2.4 中具有过度分散性的模型是这样的例子：说明了当我们将具有高斯数据的模型逐字翻译成用于非高斯响应变量的 GLM 或 GLMM 时，只改变 \\(\\symbf y \\mid\\symbf b\\) 的假定分布时会发生什么。WWFD ANOVA 过程迫使我们关注我们正在构建模型的研究背景，而不仅仅是通过死记硬背的类比来构建模型。总之，编写模型的关键步骤是：构建组合框架 ANOVA.指定模型效应，并在有意义时假定这些效应的概率分布，确保存在一对一的对应关系——没有过度分散（指定不足的模型）或混淆（过度指定的模型）。确定线性预测器。确定观测数据的分布。确定连接函数。","code":""},{"path":"chap2.html","id":"sec2-4-3","chapter":"第 2 章 设计要务","heading":"2.4.3 固定或随机——艰难抉择","text":"决定一种效应是固定的还是随机的并不总是明确的。许多“固定或随机”的争议已经持续了几十年。例子包括多中心临床试验中的地点、长期农学试验中的年份和随机区组设计中的区组。大多数争议不太可能得到彻底解决，因为不存在单一的“正确答案”。唯一包罗万象的正确答案是“这取决于每项研究的具体情况”。与我们在本章中的讨论相关的是我们如何在给定实验的背景下思考与“固定或随机”决策相关的问题。为了说明这一点，我们使用一个看似极端的不完全区组设计，并提出问题：“对于区组效应而言，将其视为固定效应还是随机效应更有意义呢？还是说，这是否重要？”考虑以下假设（但也是现实）的场景。我们想要比较六种处理——表示为 0, 1, 2, 3, 4 和 5. 使用区组有一个自然的理由，但区组的大小是有限的。任何给定区组只能分配三种处理。假设有十个可用的区组。在这些限制条件下设计研究的方法有很多。图 2.6 显示了可能会受到认真考虑的三种替代方案。第一个是真正的均衡不完全区组 (balanced incomplete block, BIB) 设计。它的主要优点是所有处理的比较都具有相等的精度。如果其中一种处理（例如处理 0）是对照或参考标准，并且目标是最大限度地提高每种处理与对照之间成对比较的精度，则可以考虑第二种设计。第三种设计称为不连通 (disconnected) 设计，因为处理 0, 1 和 2 总是一起出现在相同的区组中，而不会与处理 3, 4 或 5 一起出现在同一区组中。令人惊讶的是，不连通设计在实践中得到了广泛的应用（可能是这里显示的三种设计中使用最广泛的一种）。从建模的角度来看，这也是最有趣的，当然也是与本讨论最相关的。\n对于这三个设计中，明显的线性预测器为 \\(\\eta_{ij}=\\eta+\\tau_i+b_j\\)，其中 \\(\\tau_i\\) 表示第 \\(\\) 个处理效应，\\(b_j\\) 表示第 \\(j\\) 个区组效应。对于前两种设计，就处理效应的推断而言，假定固定或随机区组效应的影响相对较小。然而，对于不连通设计来说，影响是巨大的。如果我们假定固定区组效应，则只能在处理 0, 1 和 2 之间或在处理 3, 4 和 5 之间进行比较。所有处理均值都不可估计，我们将在第 3 章开始明确讨论这一概念（这与基于最小二乘法的线性模型理论基本一致）。该示例还解释了为什么 Fisher 对早期线性模型工作的反应是“困惑”和“愤怒”（用 Speed 的描述）。如果我们将区组效应视为随机的，使用正式的可估性 (estimability) 标准，则所有处理均值都是可估的。此外，所有的均值差异都可估，尽管精度不同，具体取决于我们比较的是在同一区组内观察到的不同处理的均值，还是不同的不连通区组的均值。对于不连通设计，固定或随机区组的决策会产生极端后果。为什么会出现这种差异？我们精心实施的 WWFD ANOVA 程序有助于解释这一现象。表 2.6 显示了地形和处理框架 ANOVAs.\n请注意，我们在指定处理框架 ANOVA 方面比我们构建线性预测器 \\(\\eta_{ij}=\\eta+\\tau_i+b_j\\) 时显然更加精确。处理分为两组 (sets)，{0,1,2} 和 {3,4,5}，并且在整个设计中保持组的完整性。因此，我们必须将其作为处理设计中的变异源。表 2.7 显示了组合框架 ANOVA 和相应的模型效应。\n所得线性预测器为 \\(\\eta_{ijk}=\\eta+\\alpha_i+\\tau(\\alpha)_{ij}+b(\\alpha)_{ik}\\)，其中 \\(\\alpha_i\\) 表示第 \\(\\) 组的效应，\\(\\tau(alpha)_{ij}\\) 表示第 \\(\\) 组内第 \\(j\\) 种处理的效应，\\(b(\\alpha)_{ij}\\) 表示第 \\(\\) 组内第 \\(k\\) 个区组的效应。模型的完整编写取决于我们假定 \\(b(\\alpha)_{ij}\\)—— block(set) 效应——是固定的还是随机的，以及观测值的分布，如果我们假定 block(set) 效应是随机的，则可以写 \\(y_{ijk}\\mid b\\left(\\alpha\\right)_{ik}\\sim\\,??\\)。在我们讨论固定或随机区组效应的问题之前，有两点评论需要先提出来。通过框架 ANOVA，我们看到不连通设计也可以理解为嵌套析因 (nested factorial). 事实上，如果我们通过将 2 个“组”定义为处理因子 的 2 个水平，将处理 0, 1 和 2 作为与 第一个水平交叉的因子 B 的 3 个水平，将处理 3, 4 和 5 作为与 第二个水平交叉的因子 B 的相同的三个水平，从而为处理设计增加额外的结构，那么我们有一个 2 × 3 析因处理设计，我们的不连通设计有一个更熟悉的名字：裂区 (split-plot).回顾 Fisher 等人如何评估“组”和组内处理效应的问题是很有指导意义的。作为一般原则，我们知道我们通过比较处理（或处理组）之间的差异与接受这些处理（或处理组）的单元之间的自然差异来计算处理（或处理组）的效应。在此设计中，组应用于区组。因此，我们将组之间的差异与即使没有应用处理也会自然发生的区组之间的差异进行比较。正式地说，我们通过取应用了第一组的五个区组的均值来估计第一组的均值，通过取应用了第二组的五个区组的均值来估计第二组的均值。然后，我们计算组 1 和组 2 之间的平均差异相对于差异的标准误，后者基于组 1 的五个区组的均值与组 2 的五个区组的均值之差的方差\\[\\begin{bmatrix}Var\\left(\\text{mean set}1\\text{ blocks}\\right)+Var\\left(\\text{mean set}2\\text{ blocks}\\right)\\end{bmatrix}/5\\]对于同一组内任意两个处理之间的比较，例如处理 0 和处理 1，我们对应用处理 0 的区组内的 5 个单元和应用处理 1 的区组内的 5 个单元进行平均。所得标准误取决于\\[\\begin{bmatrix}Var\\left(\\text{mean trt 0 units}\\right)+Var\\left(\\text{mean trt 1 units}\\right)\\end{bmatrix}/5\\]对于不在同一区组中的两种处理之间的平均差异，例如对于处理 0 和处理 4，我们对应用处理 0 的组 1 区组内的 5 个单元和应用处理 4 的组 2 区组内的 5 个单元进行平均。所得方差包括区组间方差和单元间方差。从历史上看，本示例说明的裂区结构是混合模型理论发展的主要推动力。Speed (2010) 将裂区设计描述为检验一个人对这场讨论所涉及思维过程理解的“试金石”。仅包含固定效应的线性模型理论无法充分解决这种设计类型的问题——在实践中，这种设计类型可以说是规则而非例外——这让我们理解了为什么 Fisher 对早期建模工作的反应如此负面，这些工作现在大多被我们称为 LM（仅包含固定效应，高斯观测）。回到手头的任务，当我们完成这个例子时，我们本质上规定区组效应必须是随机的。当我们说“我们通过比较应用于不同区组的组间自然发生的变异来评估组间的差异”时，我们就这样做了。因此，区组必须具有概率分布。因此它们一定是随机效应。那么，决定模型效应是固定效应还是随机效应的一个准则是，它是否具有可用作评估模型中另一个效应统计显著性（或在没有假设检验的情况下构建区间估计）的可靠标准的自然发生的变异。请注意，这并没有解决所有模棱两可的“它是固定的还是随机的？”情况。然而，在这里，设计的地形方面和处理方面的相互作用使得这一特定情况下的决策变得明确无误。顺便说一句，这也引发了一个有趣的问题：如果无论使用图 2.6 中显示的三个设计选项中的哪一个，区组的物理性质都是相同的，那么“区组效应是固定的”的倡导者如何解释在第三种设计中将区组效应视为随机效应（这是裂区实验中的普遍情况），但在前两个设计中却不视为随机的呢？","code":""},{"path":"chap2.html","id":"sec2-5","chapter":"第 2 章 设计要务","heading":"2.5 更复杂的设计蓝图：多水平多因素研究","text":"我们现在考虑更复杂的设计蓝图，这些设计具有多个处理因素和不同尺寸的重复单元。这些可能是转译为适当模型时最具挑战性的设计结构，如果不这样做，后果可能是最严重的。对于统计建模的学生来说，重要的是要注意这些设计的结构。虽然 2.5.1 节中的例子是农业的，但它的结构出现在许多环境中——医学、工程、教育等。这是一个值得我们仔细研究的好例子，因为它说明了建模学生必须掌握的许多重要特征。这也是一种经常使用错误指定的模型进行分析的结构——正如我们在本章前面所说的，建模出错的最常见原因是对设计原则的理解不足。","code":""},{"path":"chap2.html","id":"sec2-5-1","chapter":"第 2 章 设计要务","heading":"2.5.1 裂区示例","text":"这个示例将作为贯穿全书的示例，它来自一个旨在评估不同种子组合 (seed mixes) 在各种种植方法下生长的实验，以确定恢复受损草地的有效方法。数据集包含四个区组。每个区组被划分为七个矩形区域。有七种种植方法。每个田地的七个区域中的一个被随机分配给七种种植方法中的每一种。每个区域被划分为四个子区域。有四种种子组合。每个子区域被随机分配接收四种种子组合中的一种。图 2.7A 显示了一个样本区组的平面图。\n为便于阅读，种植方法按顺序显示，从 Trt 1 到 Trt 7；在实际研究中，这些方法的顺序在每个四个区组内都是随机的。与学校示例一样，绘制平面图或蓝图是构建线性预测器的第一步。接下来的步骤是构建地形和处理的并排表，然后是包含模型效应和分布的合并表。根据这张最终表格，我们写出模型，列出其所需元素。在这样做时，我们引入了数据集具有两个因素这一事实所需的额外细节。我们需要处理每个因素，并正确识别其重复单元。图 2.7 B 和 2.7 C 分别说明了与 Trt 和 Mix 相关的重复单元。\n表 2.8 显示了构建 WWFD 框架 ANOVA 表的第一步。\n种植方法 (Trt) 的重复单元（区组内的区域）和种子组合 (Mix) 的重复单元（区域内的区）决定了行的顺序，处理因素紧跟在其重复单元上方。Trt × Mix 必须应用于与 Mix 相同的单元；区域内的区 (plot) 也是其重复单元。表 2.9 显示了下一步：组合表和建议的模型效应。\n我们将“区组内区域和考虑 trt 后的处理” Area(block) | trt 的变异源重写为 block × trt，我们这样做有两个原因。首先，block 和 trt 一起唯一地定义了一个区域。其次，大多数软件包要求将区域指定为 block × trt，以便识别 Area(block) | trt 效应并适当地执行计算。“Resulting Observation” 栏中显示的两个分布是示例，遵循 2.3 节和 2.4 节的先例。根据响应变量的不同，其他分布也是可能的，并将在后续章节中讨论。假定高斯响应变量，所得混合模型为线性预测器：\\(\\eta_{ijk}=\\eta+\\tau_i+\\gamma_j+\\tau\\gamma_{ij}+b_k+bt_{ik}\\)分布：\n\\(bt_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_w^2\\right)\\)\n\\(y_{ijk}\\mid bt_{ik}\\sim N\\left(\\mu_{ijk},\\sigma_u^2\\right)\\)\n\\(bt_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_w^2\\right)\\)\\(y_{ijk}\\mid bt_{ik}\\sim N\\left(\\mu_{ijk},\\sigma_u^2\\right)\\)连接函数：恒等 \\(\\eta_{ijk}=\\mu_{ijk}\\)\n假定二项数据，logit-normal GLMM为线性预测器：\\(\\eta_{ijk}=\\eta+\\tau_i+\\gamma_j+\\tau\\gamma_{ij}+b_k+bt_{ik}+u_{ijk}\\)分布：\n\\(bt_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_w^2\\right)\\)\n\\(u_{ijk}\\mathrm{~iid~}N\\left(0,\\sigma_u^2\\right)\\)\n\\(y_{ijk}\\mid bt_{ik},u_{ijk}\\sim\\mathrm{Binomial}\\left(n_{ijk},\\pi_{ijk}\\right)\\)\n\\(bt_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_w^2\\right)\\)\\(u_{ijk}\\mathrm{~iid~}N\\left(0,\\sigma_u^2\\right)\\)\\(y_{ijk}\\mid bt_{ik},u_{ijk}\\sim\\mathrm{Binomial}\\left(n_{ijk},\\pi_{ijk}\\right)\\)连接函数：logit，\\(\\eta_{ijk}=\\log\\left[\\frac{\\pi_{ijk}}{\\left(1-\\pi_{ijk}\\right)}\\right]\\)请注意本书中使用的两个约定：在线性预测器中，我们首先列出与处理设计相关的所有模型效应，然后列出与研究设计相关的元素（地形结构）。我们使用希腊字母表示固定效应，使用拉丁字母表示随机效应。通常，与处理设计相关的效应是固定的，而与研究设计相关的效应是随机的。在这种情况下，我们也可以说希腊字母表示处理效应，拉丁字母表示研究（地形）设计效应。对于上述模型，我们使用拉丁字母来表示区组效应。上述模型没有给出区组效应的假定分布，这隐式表明它们是给定模型中的固定效应。回到 2.4 节中的讨论，更常见的是假定区组代表更大的总体，因此区组效应必须是随机的。若如此，我们需要添加一个区组效应的假定分布，例如 \\(b_k\\mathrm{~iid~}N\\left(0,\\sigma_b^2\\right)\\)。","code":""},{"path":"chap2.html","id":"sec2-5-2","chapter":"第 2 章 设计要务","heading":"2.5.2 多水平、多因素主题中的变异","text":"刚才讨论的栽培 × 种子混合研究的方法就是裂区实验的一个例子。尽管裂区一词起源于农业，但这些类型的设计在许多学科中都很常见，包括医学、工程学、分子生物学、社会科学（称为多水平设计）等。它们的定义特征有：1) 两个或更多的处理因素（或者更一般地说，两个或更多需要评估其对响应效应的因素）以及 2) 对于各种因素或因素组合，至少有两种不同尺寸的重复单元。不存在一种“裂区设计”。Federer King (2007) 出版了一本完整的教科书，描述了裂区的变体。 Stroup et al. (2018) 描述了进行 2 × 2 析因（两个因子，每个因子有两个水平）的七种方法，其中的四种是裂区设定的不同方法。这仅为冰山一角。换句话说，记住这些设计的通用“食谱”是徒劳的。相反，最好的方法是坚持绘制平面图以完成表 2.1，然后构造线性预测器。\n在本节中，我们将研究 Stroup et al. (2018) 讨论的三种变体，以了解其工作原理。在每种情况下，都有两个处理因素，每个因素都有两个水平。我们将 A1 和 A2 表示为因子 ，将 B1 和 B2 表示为因子 B。场景 1从目标总体中随机抽取八个单元。其中四个单元随机分配接受 A1，另外四个接受 A2。每个单元细分为两个子单元。一个子单元随机分配接受 B1，另一个子单元接受 B2。图 2.8 显示了蓝图。\n实验设计的学生将意识到图 2.8 是一个具有完全随机整区设计的裂区设计 (split-plot design completely randomized whole-plot design). 表 2.10 显示了地形、处理和组合框架 ANOVA 表。\n我们现在可以构建一个类似于表 2.9 的表格来识别模型效应，unit 和 plot(unit) 以及所得观测的假定分布，并使用所得的表格编写适当的模型。我们将其作为练习留给读者。场景 2与场景 1 类似，不同之处在于从原始总体中抽取的 8 个单元被配对成为区组。通常，配对基于某些有意义的标准。在每个区组中，一个单元得到 A1，另一个单元得到 A2，分配是随机进行的。每个分配到 的水平的单元被进一步细分，每个子单元被分配到 B 的一个水平，这与场景 1 中的情况很相似。图 2.9 显示了所得的蓝图。\n实验设计学生将意识到图 2.9 是一个裂区设计 (split-plot design)，其中整区 (whole-plot) 单元安排在随机完全区组中。表 2.11 显示了地形、处理和组合框架 ANOVA 表。场景 3此场景的开始与场景 2 类似。有四个区组，每个区组由一个 2 × 2 的区网格组成。图 2.10A 显示了给定区组的区蓝图。因子 的每个水平随机分配到每个区组中的一行。因子 B 的每个水平分配到一列。图 2.10B 和 2.10C 进行了说明。\n实验设计学生将意识到图 2.10 中的设计是条裂区设计 (strip-split plot design)，也称为裂区组设计 (split-block design). 表 2.12 显式了并排的 WWFD 框架 ANOVA 表。按照本节示例所示的方式继续，我们可以确定一个有效描述任何数据架构的模型，无论它有多复杂。从这些示例中可以吸取的主要教训是不要试图死记硬背食谱；你的数据集很可能有一些违背任何可记忆的食谱的怪癖。此时不必担心设计的名称。正如我们刚刚看到的，“裂区”可以表示本质上无限多种布局中的任何一种。即使是看似定义明确的设计格式，如随机完全区组设计 (randomized complete block) 或完全随机设计 (completely randomized designs)，也有许多理解和误解的方式，因此以图形形式展示种植计划对于准确理解数据收集的具体方式至关重要。由于设计名称的理解或误解得不尽相同，因此设计名称可能更多的是造成混乱，而不是促进沟通。绘制准确的蓝图（平面图）。确定研究（地形）设计中的所有单元、所有处理变异源（所有因素及其相互作用），并在并排表格的适当行中正确定位处理变异源及其相关的重复单元。要小心：此时关注细节至关重要。跳过这些步骤并不是件好事。指定假定为随机的任何效应的分布以及观测单元水平的观测分布（如表 2.1, 2.2, 2.3, 2.4, 2.9 所示）。\n在接下来的四章中，我们将详细介绍使用这些模型的方法论及其基本理论。","code":""},{"path":"chap2.html","id":"exe2","chapter":"第 2 章 设计要务","heading":"练习","text":"针对 2.5 节中的场景 2 和场景 3，填写类似于表 2.9 的表格。你应该指定与每个变异源相对应的模型效应列表以及所得模型所需的元素。给出假定高斯观测的 LMM 和假定二项观测和 logit-normal 模型的 GLMM.针对 2.5 节中的场景 2 和场景 3，填写类似于表 2.9 的表格。你应该指定与每个变异源相对应的模型效应列表以及所得模型所需的元素。给出假定高斯观测的 LMM 和假定二项观测和 logit-normal 模型的 GLMM.考虑以下六种场景。每项都描述了一项研究——研究是如何进行的、感兴趣的响应变量以及研究的目标（在某些情况下，目标是隐含的，没有用太多的文字来说明）。对于每个场景完成接下来所述的练习。\n场景 ：从总体中随机抽取十家诊所。在每个诊所中，志愿者被随机分为两组。一组接受治疗 1；另一组接受治疗 2. 令 \\(N_{ij}\\) 表示在第 \\(j\\) 个诊所接受第 \\(\\) 种治疗的患者人数。感兴趣的响应是 \\(Y_{ij}\\)，即 \\(N_{ij}\\) 中显示出良好治疗结果的患者数。\n场景 B：抽取七个地点。每个地点被分为四个部分。每个部分被随机分配一个“环境危害暴露水平”（水平为 \\(0,0.5X,X\\) 和 \\(2X\\)，其中 \\(X\\) 为“名义最大安全暴露水平”）。响应变量是受该危害影响的物种的存活生物体数（理论上随着暴露水平的增加，生物体的数量线性减少）。\n场景 C：Statsylvania 郡拥有三种不同的土壤类型。为每种土壤类型选择一个位置。每个地点被分成十个部分，称为“区”。其中 5 个随机分配了处理 1，另外 5 个随机分配了处理 2. 每一块地都种植玉米：感兴趣的响应是适合生产乙醇的玉米产量：获得每块土地的总产量测量。目的是评估土壤类型和处理对产量的效应。\n场景 D：法院希望确定该州对谋杀定罪的量刑做法是否存在种族歧视的证据。法院认为相关的一段时间的记录被汇总到一个三向 23 列联表中。第一类是受害者的种族（白人或非白人）。第二类是罪犯的种族（白人或非白人）。第三类是刑罚类型（死刑或不判处死刑）。\n场景 E：从“美国中部城市”县的总体中随机选择了 12 个县。将比较两种疫苗。六个县分配 型疫苗；另外六个县分配 B 型疫苗。在每个县，患者被随机分配到两组。第一组接种低剂量的指定疫苗；第二组接种高剂量的指定疫苗。令 \\(N_{ijk}\\) 表示 第 \\(k\\) 个县接受第 \\(\\) 个疫苗类型（或 B）的第 \\(j\\) 个剂量水平（低或高）的患者人数。令 \\(Y_{ijk}\\) 为第 \\(ijk\\) 组中表现出疫苗旨在预防的疾病症状的患者数量。目的是评估疫苗类型和剂量水平对保护效果的效应（通过出现症状的可能性来衡量）。\n场景 F：从场景 E 定义的同一总体中抽取了 12 所学校，每县一所。六所学校参加了数学教师专业发展计划；六所学校没有参加（学校被随机分配到发展组或无发展组）。这些学校数学课中的学生在学年期间进行了四次数学能力测试（开学时、第一学期后、第二学期后和学年结束时）。目的是看专业发展计划是否提高了学生的数学学习成绩。\n对于每个场景，确定模型所需的元素：\n响应变量及其假定的分布。\n连接函数。\n线性预测器。\n对于线性预测值中假定为随机的任何效应，其假定的分布。\n额外：在你的模型中，哪些参数会告知你与这些目标相关的信息？它们是如何做到的？（不需要讨论太多细节——只需想想这将如何运作）\n注意：\\(\\symbf{X\\beta}\\) 和 \\(\\symbf{X\\beta}+\\symbf{Zb}\\) 不是线性预测器的充分陈述——你需要将写其出来，这样才能清楚每个符号的含义以及每个下标的含义。\n还请注意：这里没有一个正确的答案——给出你认为合理的模型，并准备好解释/捍卫你的论证。考虑以下六种场景。每项都描述了一项研究——研究是如何进行的、感兴趣的响应变量以及研究的目标（在某些情况下，目标是隐含的，没有用太多的文字来说明）。对于每个场景完成接下来所述的练习。场景 ：从总体中随机抽取十家诊所。在每个诊所中，志愿者被随机分为两组。一组接受治疗 1；另一组接受治疗 2. 令 \\(N_{ij}\\) 表示在第 \\(j\\) 个诊所接受第 \\(\\) 种治疗的患者人数。感兴趣的响应是 \\(Y_{ij}\\)，即 \\(N_{ij}\\) 中显示出良好治疗结果的患者数。场景 B：抽取七个地点。每个地点被分为四个部分。每个部分被随机分配一个“环境危害暴露水平”（水平为 \\(0,0.5X,X\\) 和 \\(2X\\)，其中 \\(X\\) 为“名义最大安全暴露水平”）。响应变量是受该危害影响的物种的存活生物体数（理论上随着暴露水平的增加，生物体的数量线性减少）。场景 C：Statsylvania 郡拥有三种不同的土壤类型。为每种土壤类型选择一个位置。每个地点被分成十个部分，称为“区”。其中 5 个随机分配了处理 1，另外 5 个随机分配了处理 2. 每一块地都种植玉米：感兴趣的响应是适合生产乙醇的玉米产量：获得每块土地的总产量测量。目的是评估土壤类型和处理对产量的效应。场景 D：法院希望确定该州对谋杀定罪的量刑做法是否存在种族歧视的证据。法院认为相关的一段时间的记录被汇总到一个三向 23 列联表中。第一类是受害者的种族（白人或非白人）。第二类是罪犯的种族（白人或非白人）。第三类是刑罚类型（死刑或不判处死刑）。场景 E：从“美国中部城市”县的总体中随机选择了 12 个县。将比较两种疫苗。六个县分配 型疫苗；另外六个县分配 B 型疫苗。在每个县，患者被随机分配到两组。第一组接种低剂量的指定疫苗；第二组接种高剂量的指定疫苗。令 \\(N_{ijk}\\) 表示 第 \\(k\\) 个县接受第 \\(\\) 个疫苗类型（或 B）的第 \\(j\\) 个剂量水平（低或高）的患者人数。令 \\(Y_{ijk}\\) 为第 \\(ijk\\) 组中表现出疫苗旨在预防的疾病症状的患者数量。目的是评估疫苗类型和剂量水平对保护效果的效应（通过出现症状的可能性来衡量）。场景 F：从场景 E 定义的同一总体中抽取了 12 所学校，每县一所。六所学校参加了数学教师专业发展计划；六所学校没有参加（学校被随机分配到发展组或无发展组）。这些学校数学课中的学生在学年期间进行了四次数学能力测试（开学时、第一学期后、第二学期后和学年结束时）。目的是看专业发展计划是否提高了学生的数学学习成绩。对于每个场景，确定模型所需的元素：响应变量及其假定的分布。连接函数。线性预测器。对于线性预测值中假定为随机的任何效应，其假定的分布。额外：在你的模型中，哪些参数会告知你与这些目标相关的信息？它们是如何做到的？（不需要讨论太多细节——只需想想这将如何运作）注意：\\(\\symbf{X\\beta}\\) 和 \\(\\symbf{X\\beta}+\\symbf{Zb}\\) 不是线性预测器的充分陈述——你需要将写其出来，这样才能清楚每个符号的含义以及每个下标的含义。还请注意：这里没有一个正确的答案——给出你认为合理的模型，并准备好解释/捍卫你的论证。对于练习 1 中的每个场景，完成下面给出的 PROC GLIMMIX 语句，这是开始分析所需的最低限度对于练习 1 中的每个场景，完成下面给出的 PROC GLIMMIX 语句，这是开始分析所需的最低限度对于本练习，只关注这三个语句，不要包括任何其他语句。对于第 1 章表 1.1 和 1.2 中使用的数据集，编写 PROC GLIMMIX 语句来实现第 1 章中描述的分析。\n对于表 1.1，两项分析均使用线性预测器 \\(\\eta_i=\\beta_0+\\beta_1X_i\\)。其一假定样本比例近似正态；另一个使用假定二项响应的广义线性模型。\n对于表 1.2，有四项分析，均假定线性预测器 \\(\\eta_{ij}=\\begin{pmatrix}\\beta_0+b_{0i}\\end{pmatrix}+\\begin{pmatrix}\\beta_1+b_{1i}\\end{pmatrix}X_j\\) 但做出了不同的假定，如下\n响应变量 \\(Y\\) 假设高斯，批量效应为固定的\n响应变量 \\(Y\\) 假设高斯，批量效应为随机的\n响应变量假定为二项，由 Fav 和 N 指定，批次效应为固定的\n响应变量假定为二项，由 Fav 和 N 指定，批次效应为随机的\n\n对于表 1.1，两项分析均使用线性预测器 \\(\\eta_i=\\beta_0+\\beta_1X_i\\)。其一假定样本比例近似正态；另一个使用假定二项响应的广义线性模型。对于表 1.2，有四项分析，均假定线性预测器 \\(\\eta_{ij}=\\begin{pmatrix}\\beta_0+b_{0i}\\end{pmatrix}+\\begin{pmatrix}\\beta_1+b_{1i}\\end{pmatrix}X_j\\) 但做出了不同的假定，如下\n响应变量 \\(Y\\) 假设高斯，批量效应为固定的\n响应变量 \\(Y\\) 假设高斯，批量效应为随机的\n响应变量假定为二项，由 Fav 和 N 指定，批次效应为固定的\n响应变量假定为二项，由 Fav 和 N 指定，批次效应为随机的\n响应变量 \\(Y\\) 假设高斯，批量效应为固定的响应变量 \\(Y\\) 假设高斯，批量效应为随机的响应变量假定为二项，由 Fav 和 N 指定，批次效应为固定的响应变量假定为二项，由 Fav 和 N 指定，批次效应为随机的","code":"CLASS ...; (if needed)\nMODEL ... = ... / (essential options, if any);\nRANDOM ...; (if needed)"},{"path":"chap2.html","id":"sec2-A","chapter":"第 2 章 设计要务","heading":"附录 A：响应变量 \\(y\\mid b\\) 的常见分布","text":"","code":""},{"path":"chap2.html","id":"sec2-B","chapter":"第 2 章 设计要务","heading":"附录 B：将你的模型传达给软件或“SAS® PROC GLIMMIX 是如何‘思考’的”","text":"我们在 SAS® 程序语句中编写的语句（或在任何其他统计软件包中编写的类似语句）本质上是从统计符号模型描述（以下称为“模型语言”）到计算机软件理解的语言（在此称为“计算机语言”或适当时称为“SAS 语言”）的翻译。在本节中，我们重点介绍 SAS GLIMMIX 程序。为什么？回想一下第 1 章，GLMM 是最通用的线性模型。GLM 不太通用，因为它可以处理非高斯响应变量，但不能处理随机模型效应。LMM 也是一种特殊情况：处理随机模型效应，但不处理非高斯数据。LM 是最特定的：仅处理高斯数据，无随机模型效应。SAS 中的线性模型程序在历史上以相反的顺序发展。 PROC GLM 是为 LM 编写的（在编写时称为“一般”线性模型，尽管“一般”现在是一个过时的形容词）。严格来说，PROC GLM 不能执行任何特定于 LMM 的计算；GLM 确实有几个“补丁”可以满足一些 LMM 需求，但其对LMM的处理最多只能算是部分的和不充分的。而 PROC MIXED 程序是专门为 LMM 设计编写的，因此也能胜任 LM 分析。PROC GENMOD 是为 GLM（广义线性模型——当代缩写中的真正 GLM）编写的，并且由于 LM 是 GLM 和 LMM 的子集，因此 GENMOD 也可以进行 LM 分析。最后，PROC GLIMMIX 是为 GLMMs 编写的，由于 GLM, LMM 和 LM 都是其特例，GLIMMIX 可以处理其他 PROC 可以处理的任何模型，甚至更多。所有这些过程都有一个共同的语法方法。在开发这些程序时，MIXED 和 GENMOD 尽可能使用了 PROC GLM 的语法，只为诸如随机模型效应或 LM 中不存在的连接函数等术语创建了新项。除少数例外，GLIMMIX 使用了 MIXED 的 LMM 语法和 GENMOD 的 GLM 语法。出于这些原因，如果你是初学者，请从 PROC GLIMMIX 开始。一旦你掌握了 GLIMMIX，SAS 的其他线性模型程序就很容易学习了，因为除了少数例外，语法都是相同的。GLM, GENMOD 和 MIXED 程序确实具有特定于某些 LM, GLM 或 MIXED 应用程序的特殊功能，因此 GLIMMIX 并不能完全取代其他程序。但是，对于大多数情况，包括本教科书中涵盖的几乎所有示例，GLIMMIX 都可以满足你的所有需求。","code":""},{"path":"chap2.html","id":"sec2-B-1","chapter":"第 2 章 设计要务","heading":"一般原理","text":"回想一下 GLMM 由以下部分组成：观测响应向量 \\(\\symbf{y}\\mid \\symbf u\\)\\(\\symbf{y}\\mid\\symbf u\\) 的假定分布满足 \\(\\symbf{\\mu}|\\symbf{u}=E(\\symbf{y}\\mid\\symbf{u})\\) 且 \\(\\symbf{R}=Var(\\symbf{y}\\mid\\symbf{u})\\)连接函数 \\(\\symbf\\eta=g\\left(\\symbf\\mu\\mid\\symbf u\\right)\\)线性预测器 \\(\\symbf{X\\beta}+\\symbf{Zu}\\) 对 \\(\\symbf\\eta\\) 直接建模随机向量 \\(\\symbf u\\) 的假定分布，特别地 \\(\\symbf{u}\\sim N\\left(\\symbf{0},\\symbf{G}\\right)\\)\nPROC GLIMMIX 中的三个基本语句共同指定上述列表中的每个元素：CLASS, MODEL 和 RANDOM 语句。从这个角度来看，我们可以将这些语句视为矩阵定义和组织语句。它们的作用如下CLASS 语句列出线性预测器中视为分类变量的所有因素。如果线性预测器中的任何因素（固定或随机）未出现在 CLASS 语句中，则将其视为直接（回归）变量。MODEL 语句具有基本形式：观测变量 \\(\\symbf y\\) 由 response variable 标识。list fixed model effects 指定哪些项包含固定效应参数向量 \\(\\symbf\\beta\\) 和 \\(\\symbf X\\) 矩阵。options 包括 dist= 和 link=。分布假定为高斯，除非我们使用 dist= 选项另行指定。分布有一个默认连接（例如 logit 是二项的默认连接）。如果我们不包含 link= 选项，则使用默认连接。因此，例如，如果我们想要拟合 probit 回归模型，则必须在 MODEL 语句中包含选项 dist=probit。RANDOME 语句具有基本形式：这指定了组成随机向量 \\(\\symbf u\\)、\\(\\symbf Z\\) 矩阵和方差—协方差阵 \\(\\symbf G\\) 形式的项。有几个选项允许我们更改 \\(\\symbf G\\) 的形状和相关结构 (correlation structure). 最常用的两个选项是 subject=，它允许我们形成块对角 \\(\\symbf G\\) 矩阵，以及 type=，它允许我们指定相关结构。RANDOM 语句的变体为或等价地RESIDUAL 选项指定 \\(\\symbf R\\) 矩阵的形状和结构。如果你以前使用过 PROC MIXED 或 PROC GENMOD，则需要知道这些 PROC 中的 REPEATED 语句在 PROC GLIMMIX 中不存在。random _residual_ 语句取代了 MIXED 或 GENMOD 中的 REPEATED.","code":"model response variable = list of fixed model effects/ optionsrandom list of random model effects / optionsrandom _residual_ / optionsrandom effect / options residual"},{"path":"chap2.html","id":"sec2-B-2","chapter":"第 2 章 设计要务","heading":"示例","text":"示例 2.1  (简单线性回归，高斯数据) 线性预测器为 \\(\\alpha+\\beta X\\)。观测为高斯，即 \\(\\symbf{y}\\sim N\\left(\\symbf{\\mu},\\symbf{}{\\sigma}^2\\right)\\)。连接为恒等，\\(\\eta=\\mu\\)。PROC GLIMMIX 语句为没有出现 CLASS 语句，因为线性预测器中的唯一变量是 \\(X\\)，一个直接变量。y 标识响应变量。默认情况下假定截距（本例中为 \\(\\alpha\\)），因此一个全一列会自动放入 \\(\\symbf X\\) 矩阵中，并且截距参数会自动放入 \\(\\symbf \\beta\\) 向量中，除非我们特别关闭截距默认值（后面的示例将演示何时需要执行此操作以及如何执行此操作）。变量 \\(X\\) 是直接变量，因此 \\(X\\) 的元素在 \\(\\symbf X\\) 矩阵中显示为一列。没有随机模型效应，因此不需要 RANDOM 语句。PROC GLIMMIX 中唯一的强制性语句是 PROC 和 MODEL 语句。为了可视化这些语句的效果，请考虑第 1 章表 1.1 中的数据。\n语句 model y = x 执行以下操作：列变量 \\(\\symbf y\\) 中的元素置于如下向量中\n\\[\\symbf{y}=\\left[\\begin{array}{c}0\\\\0\\\\2\\\\2\\\\2\\\\5\\\\7\\\\12\\\\10\\\\16\\\\9\\end{array}\\right]\\]\n请注意，如果响应变量具有不同的名称，例如 weight，则模型语句将是 model weight = x。其次，\\(\\symbf X\\) 矩阵由一列 1 开始，\\(\\symbf \\beta\\) 向量中的第一个参数是截距——这些都是默认出现的。然后列变量 \\(\\symbf x\\) 的内容成为 \\(\\symbf X\\) 矩阵的第二列—— \\(\\symbf x\\) 的元素按其确切值插入，因为 \\(\\symbf x\\) 是直接变量。得到\n\\[\\symbf{X\\beta}=\\begin{bmatrix}1&0\\\\1&1\\\\1&2\\\\1&3\\\\1&4\\\\1&5\\\\1&6\\\\1&7\\\\1&8\\\\1&9\\\\1&10\\end{bmatrix}\\begin{bmatrix}\\alpha\\\\\\beta\\end{bmatrix}\\]示例 2.2  (简单 logistic 线性回归，二项数据) 如同示例 2.1，线性预测器为 \\(\\alpha+\\beta X\\)。观测为 \\(\\symbf y\\thicksim\\text{Binomial}(\\symbf N,\\symbf{\\pi})\\)。logit 连接为 \\(\\eta=\\log\\left(\\frac{\\pi}{1-\\pi}\\right)\\)。PROC GLIMMIX 语句为请注意，在 MODEL 语句中，缺少 CLASS 变量并且仅在等号右侧存在变量 x 的效果与示例 2.1 中的效果相同：它们都设置了与线性预测器一致的 \\(\\symbf{X\\beta}\\)。唯一的区别是响应变量 \\(Y/N\\)。以比率形式给出响应变量意味着分布是二项的。logit 是二项的默认连接，因此不需要 link= 选项。如果我们想使用不同的连接，我们必须指定它。例如，如果我们想使用 probit 连接来拟合模型，则 PROC GLIMMIX 语句为示例 2.3  (单向处理设计，高斯数据) 线性预测器为 \\(\\mu+\\tau_i\\)，其中 \\(\\mu\\) 为截距，\\(\\tau_i\\) 表示第 \\(\\) 个处理效应。观测为 \\(\\symbf{y}\\sim N\\left(\\symbf{\\mu},\\symbf{}{\\sigma}^2\\right)\\)，其中 \\(\\symbf\\mu\\) 是处理均值 \\(\\mu_i=\\mu+\\tau_i\\) 的向量（勿将处理均值向量 \\(\\symbf\\mu\\) 与截距参数（标量）\\(\\mu\\) 混淆！）。连接是恒等 \\(\\symbf \\eta=\\symbf\\mu\\)。PROC GLIMMIX 语句为与示例 2.B.1 一样，模型语句的 y= 部分根据列变量 \\(\\symbf y\\) 的内容创建响应向量。此外，与前两个示例一样，\\(\\symbf X\\) 矩阵中的第一列是全一列。变量 trt 是一个 CLASS 变量，因此当它出现在 MODEL 语句中时，将为每个不同的 trt 值创建一列虚拟变量。表 2.13 显示了单向设计的数据，以说明 CLASS 和 MODEL 语句的效果。\nMODEL 语句读取 TRT 列向量：\\[\\begin{bmatrix}1\\\\1\\\\2\\\\2\\\\3\\\\3\\end{bmatrix}\\]CLASS 语句将其转换为“设计”矩阵\\[\\begin{bmatrix}1&0&0\\\\[1ex]1&0&0\\\\[1ex]0&1&0\\\\[1ex]0&1&0\\\\[1ex]0&0&1\\\\[1ex]0&0&1\\\\[1ex]\\end{bmatrix}\\]在第一列中，当行对应的观测值为 trt = 1 时会出现 1；在第二列中，当行对应的观测值为 trt = 2 时会出现 1，以此类推。因此，最终的 \\(\\symbf X\\) 矩阵和 \\(\\symbf \\beta\\) 向量分别为：\\[\\begin{bmatrix}1&1&0&0\\\\1&1&0&0\\\\1&0&1&0\\\\1&0&1&0\\\\1&0&0&1\\\\1&0&0&1\\end{bmatrix}\\begin{bmatrix}\\mu\\\\\\tau_1\\\\\\tau_2\\\\\\tau_3\\end{bmatrix}\\]示例 2.4  (单向处理设计，二项数据) 观测 \\(y_{ij}=\\text{Binomial}\\Big(N_{ij},\\pi_i\\Big)\\)。线性预测器与示例 2.3 相同。默认连接为 logit 连接。表 2.14 显示了说明 SAS 所需表格的数据\n所需的 PROC GLIMMIX 语句为请注意，在数据集中，\\(N_{ij}\\) 的变量名称为 N_ij，\\(y_{ij}\\) 的变量名称为 Y_ij，因此在模型语句中创建二项响应变量和默认的 logit 连接。CLASS 语句中的变量 trt 以及 MODEL 语句中等号右侧的变量与示例 2.3 中的效果相同。示例 2.5  (多批次线性回归 1) 如 1.3 节所讨论的。数据示于表 1.2. 依次考虑两种线性预测器 \\(\\alpha_i+\\beta_iX_{ij}\\) 和 \\(\\alpha+a_i+\\left(\\beta+b_i\\right)X_{ij}\\)。\n表 1.3 给出了该模型的 LM 和 GLM 形式，其中 LM 的响应变量假定服从 \\(N\\left({\\mu}_i,{\\sigma}^2\\right)\\) 分布并具有恒等连接，而 GLM 则假定服从 \\(\\text{Binomial}\\left(N_{ij},\\pi_i\\right)\\) 分布并具有 logit 连接。首先考虑 LM. 为了让计算机理解我们想要的模型，我们需要做两件事：我们需要给每个批次一个唯一的截距（\\(\\alpha_i\\)），并且我们需要为每个批次指定一个单独的斜率。对于后者，我们引入嵌套效应命令，在本例中为 X()。SAS 将此命令理解为“X 对于因子 的每个水平都有不同的参数值”。它设置的矩阵命令是效应 \\(x\\) 所调用的向量或矩阵与效应 所调用的向量或矩阵的直积。在这种情况下，\\(x\\) 是一个直接变量，因此 \\(\\symbf X\\) 矩阵是 \\(x\\) 值（0, 3, 6,…, 48）的 9 × 1 矩阵，\\(\\symbf \\beta\\) 向量很简单：由斜率系数 \\(\\beta\\) 组成。在本例中，“因子 ”是 Batch，它是一个具有四个水平的分类变量。对于第 1 章 1.4 节中 Batch 的 \\(\\symbf X\\) 矩阵和 \\(\\symbf \\beta\\) 向量的形式，取直积得到考虑到这一点，此模型的 SAS 语句为请注意 MODEL 语句中的新选项：noint。回想一下，MODEL 语句默认创建截距。使用默认值（即不使用 noint 选项），上述语句将生成线性预测器 \\(\\mu+\\alpha_i+\\beta_iX_{ij}\\) ——即带有不需要的和多余的截距项 \\(\\mu\\) 的线性预测器。noint 选项会抑制截距的创建，从而产生所需的模型 \\(\\alpha_i+\\beta_iX_{ij}\\)。对于使用 logit 连接函数的二项数据，将 MODEL 语句中等号左侧的 y 替换为 Fav/N_ij。如果我们想使用 probit 连接，请在 MODEL 语句中添加 link=probit 作为选项，就像我们在示例 2.2 中所做的那样。现在考虑模型的另一种形式 \\(\\alpha+a_i+\\left(\\beta+b_i\\right)X_{ij}\\)。在这里，我们希望默认激活截距项，因为截距参数是 \\(\\alpha\\)。我们还希望将斜率分为总体的 \\(\\beta\\) 和特定于批次的 \\(b_i\\) 部分。我们使用以下语句来实现这一点：此处，缺少 noint 选项允许创建截距 \\(\\alpha\\)。变量 batch 由语句 class batch 定义为分类变量，因此为 \\(\\symbf \\beta\\) 向量中的四个批次参数对应的 \\(\\symbf X\\) 矩阵创建一个 36 × 4 设计矩阵（如第 1 章 1.4 节所示）。变量 \\(x\\) 是直接变量，因此会创建一个 36 × 1 向量，其中包含每个观测的 \\(x\\) 值，对应于总体斜率参数 \\(\\beta\\)。项 x*batch 创建一个 36 × 4 矩阵，等于批次设计矩阵与 \\(x\\) 向量的直积。该模型的完整矩阵形式如第 1 章 1.4 节所示。如果响应变量是二项，我们将进行与上面所示相同的更改。提问：以下语句将创建什么模型？答案：模型：\\(\\alpha_i+\\beta_iX_{ij}\\)。请注意，项 x(batch) 和 x*batch 都创建矩阵直积，因此它们实际上创建相同的模型。提问：考虑到这一点，以下语句将创建什么模型？示例 2.6  (多批次线性回归 2) 与示例 2.5 相同的线性预测方程，但批次效应 \\(a_i\\) 和 \\(b_i\\) 是随机的。到目前为止，我们只看到了固定效应模型，因此只看到了 CLASS 和 MODEL 语句。回想一下，MODEL 语句定义了线性预测变量的 \\(\\symbf{X\\beta}\\) 部分。RANDOM 语句定义 \\(\\symbf{Zu}\\) 部分。RANDOM 语句的基本语法与 MODEL 语句中等式右侧的语法本质上相同。编写具有随机批次效应的模型 \\(\\alpha+a_i+\\left(\\beta+b_i\\right)X_{ij}\\) 的最简单方法是这将创建与示例 2.5 所示模型的固定效应版本相同的矩阵。但请注意，\\(\\alpha_i\\) 和 \\(\\beta_i\\) 效应会移至 RANDOM 语句。这同时创建了 \\(\\symbf Z\\) 矩阵和 \\(\\symbf u\\) 向量的适当分量——注意到\\[\\symbf{u}=\\begin{bmatrix}\\symbf{}\\\\\\symbf{b}\\end{bmatrix}=\\begin{bmatrix}a_1\\\\a_2\\\\a_3\\\\a_4\\\\b_1\\\\b_2\\\\b_3\\\\b_4\\end{bmatrix}\\]它创建隐含的 \\(\\symbf G\\) 矩阵，回想\\[\\symbf{G}=Var\\begin{pmatrix}\\symbf{u}\\end{pmatrix}=Var\\left(\\begin{bmatrix}\\symbf{}\\\\\\symbf{b}\\end{bmatrix}\\right)=\\begin{bmatrix}\\symbf{}\\sigma_A^2&\\symbf{}\\sigma_{AB}\\\\\\symbf{}\\sigma_{AB}&\\symbf{}\\sigma_B^2\\end{bmatrix}\\]定义随机部分的另一种（也是更好的）方法是，线性预测器的 \\(\\symbf {Zu}\\) 部分使用 SUBJECT= 选项。此选项定义 \\(\\symbf Z\\) 和 \\(\\symbf G\\) 矩阵的分块对角结构。那么它是如何工作的，为什么它更可取？首先，它是如何工作的？请注意，每个批次都有一个截距和一个斜率。用矩阵术语来说，第 \\(\\) 批的 \\(\\symbf {Zu}\\) 为\\[\\begin{bmatrix}1&0\\\\1&3\\\\1&6\\\\1&9\\\\1&12\\\\1&18\\\\1&24\\\\1&36\\\\1&48\\end{bmatrix}\\begin{bmatrix}a_i\\\\b_i\\end{bmatrix}\\]第 \\(\\) 批的 \\(\\symbf G\\) 矩阵为\\[Var\\begin{bmatrix}a_i\\\\b_i\\end{bmatrix}=\\begin{bmatrix}{\\sigma}_A^2&{\\sigma}_{AB}\\\\{\\sigma}_{AB}&{\\sigma}_B^2\\end{bmatrix}\\]分别表示 \\(\\symbf Z_i,\\symbf u_i\\) 和 \\(\\symbf G_i\\)。现在请注意，我们可以将完整的 \\(\\symbf Z\\) 和 \\(\\symbf G\\) 构造为分别由 \\(\\symbf Z_i\\) 和 \\(\\symbf G_i\\) 形成的分块对角阵。即\\[\\symbf{Z}=\\begin{bmatrix}\\symbf{Z}_1&0&0&0\\\\0&\\symbf{Z}_2&0&0\\\\0&0&\\symbf{Z}_3&0\\\\0&0&0&\\symbf{Z}_4\\end{bmatrix}\\text{}\\symbf{G}=\\begin{bmatrix}\\symbf{G}_1&0&0&0\\\\0&\\symbf{G}_2&0&0\\\\0&0&\\symbf{G}_3&0\\\\0&0&0&\\symbf{G}_4\\end{bmatrix}\\]由此\\[\\symbf{u}=\\begin{bmatrix}\\symbf{u}_1\\\\\\symbf{u}_2\\\\\\symbf{u}_3\\\\\\symbf{u}_4\\end{bmatrix}\\]我们可验证这些只是前面展示的 \\(\\symbf u\\) 向量以及 \\(\\symbf Z\\) 和 \\(\\symbf G\\) 矩阵重新排列的版本。使用这种方法定义线性预测器的 SAS 语句为subject=batch 选项导致 \\(\\symbf Z\\) 和 \\(\\symbf G\\) 矩阵定义分块对角阵，并使用 batch 来定义分块。在矩阵的每个分块内，\\(x\\) 都有一个截距和斜率系数，从而得到上面所示的矩阵 \\(\\symbf Z_i,\\symbf u_i\\) 和 \\(\\symbf G_i\\)。现在第二个问题：为什么要这么麻烦呢？为什么最好使用 SUBJECT= 选项？首先，分块对角结构可以实现更高效的计算。这不仅适用于 SAS PROC GLIMMIX，而且通常适用于基于矩阵的统计计算算法。所有 GLMM 软件，无论是 SAS 还是其他软件，都必然是基于矩阵的。当我们在本文后面的章节中讨论更复杂的 GLMM 时，有些模型根本无法运行，除非我们使用 SUBJECT= 选项来简化所需的计算。当我们开始考虑固定效应和随机效应的线性组合的估计（“最佳线性无偏预测器”，“best linear\nunbiased predictors” 或 BLUP）时，第二个优点将变得清晰。在此示例中，\\(\\alpha+\\beta X_{ij}\\) 给出总体平均 (population averaged) 回归方程，即这些批次所代表的整个总体的线性预测器的期望值。然而，在某些应用中，特定批次的预测值 \\(\\alpha+a_i+\\left(\\beta+b_i\\right)X_{ij}\\)（称为特定于批次的 BLUP）是我们关注的。请注意，这是固定模型效应和随机模型效应的线性组合。如果我们使用 subject=batch 选项，我们就可以让程序相对方便地计算这些 BLUPs.示例 2.7  (× B 析因) 两因素，其中因素 \\(, B\\) 都有 \\(\\ge 2\\) 个水平，观察到所有可能的 \\(× B\\) 组合。在此示例中，我们重点定义线性预测器。这里的示例 SAS 语句都展示了正态分布的响应变量，但与前六个示例一样，我们可以很容易地通过之前演示的相同方法，将这些语句适应于二项或其他非正态分布的响应变量。首先要理解的是——这是实验设计入门课程经常忽略的一点——析因结构不只有一个模型。可能有几种模型。每种模型都有其用途，具体取决于情况和目标。本节将比前面的示例更简洁。SAS 如何“理解”模型形成命令的基本原理已经介绍完毕。在本节中，给出了 SAS 语句及其模型后果。我们鼓励你详尽地研究每个示例，以便你可以可视化并准确描述每个模型的 \\(\\symbf X\\) 矩阵和 \\(\\symbf\\beta\\) 向量的内容。在示例 2.8 中，我们将回顾这些模型，其中包含由各种分块结构和裂区特征产生的额外随机效应。7.1 经典的主效应和交互作用：\\(\\mu+\\alpha_i+\\beta_j+\\left(\\alpha\\beta\\right)_{ij}\\)或等价地 model y=|b;。7.2 将每个 \\(A_i × B_j\\) 组合视为“处理”，并将模型简化为单向处理效应模型：\\(\\mu+\\left(\\alpha\\beta\\right)_{ij}\\)。注意！！！！！在该模型中，效应 \\((\\alpha\\beta)_{ij}\\) 不是交互效应！！！它只是第 \\(\\) 个处理组合的效应（或关于截距的偏离）。7.3 单元格均值模型。抑制上一个模型（7.2）中的截距。这给出 \\(\\left(\\alpha\\beta\\right)_{ij}\\)。请注意，这只是 \\(\\mu_{ij}\\)（即第 \\(ij\\) 种处理的均值）的另一种书写方式，。7.4 嵌套模型。在这种情况下首先给出 的主效应，然后是 中嵌套的 B 的效应。模型为 \\(\\mu+\\alpha_i+\\beta(\\alpha)_{ij}\\)。当不关心 B 的主效应，只关心 B 在 的特定水平内的效应时，我们使用此模型。有时，研究目标会直接要求采用这种模型；通常，一旦建立了不可忽略的 \\(× B\\) 交互作用并且我们的关注点转移到简单效应（如 特定水平下 B 的效应）时，就会使用此模型。回顾我们对示例 2.5 的讨论，请注意以下语句产生完全相同的模型。这是因为两者都会计算相同的直积矩阵和相同的结果参数向量。如果不显示给出，SAS 不会假定 B 主效应。7.5 回归模型。如果 B 是一个定量因素呢？那么就没有理由不将其视为回归（直接）变量——而且这样做也有充分的理由。我们可以使用与示例 2.5 基本相同的建模策略：或等价地这些得到什么模型？小心！一定要准确表述每组语句所隐含的线性预测器！如果我们怀疑 B 存在二次效应，我们将如何修改这些语句？示例 2.8  (学校专业发展成效研究) 根据 2.2 节，回想一下，线性预测器为 \\(\\eta_{ij}=\\eta+\\tau_i+S_j\\)，其中学校效应 \\(s_j\\) 是随机的。该模型假定我们在课堂层面进行观测。如果我们的观测是在学生层面，我们将表 2.1 修改为“Resulting Observation” 分布假定高斯响应变量。我们可以修改非高斯观测的表，例如遵循表 2.4 所示的 Logit-normal 方法。\nMODEL 语句定义了\\(\\symbf X\\) 矩阵和用于截距 \\(\\mu\\) 以及专业发展 \\(\\rho_j\\) 的固定效应向量 \\(\\symbf\\beta\\)，RANDOM 语句定义了学校和教师效应的 \\(\\symbf Z\\) 矩阵、\\(\\symbf u\\) 向量和协方差阵 \\(\\symbf G\\)。专业发展、学校和教师效应都是分类变量。使用与随机批次回归示例（示例 2.6）中相同的逻辑，以下 SAS 语句定义了线性预测器：变量 dp 表示专业发展计划的效应。请注意，与教师效应 teacher(school*dp) 相对应的 \\(\\symbf Z\\) 矩阵分量由分类变量 teacher, school 和 dp 的设计矩阵的直积定义。与前面的示例一样，如果我们将随机语句写为所得模型是相同的，因为 teacher*school*dp 创建了与 classroom(school*dp) 相同的 \\(\\symbf Z\\) 矩阵分量以及 \\(\\symbf u\\) 向量和 \\(\\symbf G\\) 矩阵的相关元素。或者，最好是，我们可以在 RANDOM 语句中使用 SUBJECT= 选项。与示例 2.6 一样，这会创建一种计算效率更高的模型形式，并允许我们计算特定于学校或教师的 BLUPs（如果研究目标需要）。相应的 SAS 语句为回想一下，subject=school 为 \\(\\symbf Z\\) 矩阵定义了一个分块对角结构，每个学校一个分块，每个分块内的列对应于 intercept（全一列）以及 classroom*dp（由 classroom 和 dp 的直积定义的设计矩阵）。我们只能使用一个 MODEL 语句，但可以有多个 RANDOM 语句。为了使特定于课堂的 BLUPs 更加方便，我们可以将 SAS 语句重写为或等价地 random dp / subject=classroom*school;。当我们开始详细讨论 BLUPs 时（从第 6 章开始），我们将看到第二个 RANDOM 语句可以方便地计算特定于每个课堂的效应。示例 2.9  (三个裂区场景) 2.5 节描述了三个裂区场景——进行 \\(×B\\) 处理设计的不同方法。场景 1. 裂区，无区组。线性预测器：\\(\\eta_{ijk}=\\mu+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+u(\\alpha)_{ik}\\)。SAS 语句：或等价地或等价地 random exp_unit*;\n或等价地 random intercept / subject=exp_unit();\n或等价地 random intercept / subject=exp_unit*;变量 exp_unit 表示整区实验单元。请注意，它必须是分类变量。回顾我们在本节前面对 \\(×B\\) 处理设计的讨论（示例 2.7），请记住 \\(×B\\) 处理设计的分解 \\(\\mu+\\alpha_i+\\beta_j+\\left(\\alpha\\beta\\right)_{ij}\\) 不是强制的。我们可以使用单元格均值法 \\(\\mu_{ij}\\)，相关的 SAS 语句为RANDOM 语句的所有等价形式都已被跳过，但我们也可以在这里使用它们中的任何一个。或者，我们可以使用分解 \\(\\mu+\\alpha_i+\\beta(\\alpha)_{ij}\\) 来关注 B 在因子 的特定水平内的简单效应。SAS 语句为或等价地场景 2. 裂区。整区实验单元（相对于因子 水平的重复单元）安排在区组中。线性预测器：\\(\\eta_{ijk}=\\mu+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+r_k+r\\alpha_{ik}\\)。SAS 语句：请注意，这等价于 random block block*;。为什么？变量 block 表示区组因子。与场景 1 一样，我们可以在 MODEL 语句中使用 \\(× B\\) 处理设计的所有替代分解（单元格均值、嵌套等）。此外，还有几种等价的方法来编写 RANDOM 语句。MODEL 语句与场景 1 中所示的语句相同。RANDOM 语句类似，不同之处仅在于它们需要区组效应以及整区误差。留给读者作为练习来确定它们的确切形式。场景 3. 条区。因子 \\(\\)、因子 \\(B\\) 和 \\(× B\\) 组合各自具有不同尺寸的重复单元。线性预测器：\\(\\eta_{ijk}=\\mu+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+r_k+r\\alpha_{ik}+r\\beta_{jk}\\)。SAS 语句：请注意，这等价于 random block block*block*b;。为什么？与场景 1 和 2 一样，我们可以在 MODEL 语句中使用 \\(× B\\) 处理设计的所有替代分解（单元格均值、嵌套等），并且生成的 MODEL 语句与场景 1 中所示的相同。同样，我们可以使用上面所示的 RANDOM 语句的任何一种形式。通过对高斯分布观测值的标准分析，我们使用哪种形式并没有太大区别。对于非高斯响应变量，出于计算原因，带有 subject=block 选项的形式更可取。第二章附录 B 的关键思想：CLASS, MODEL 和 RANDOM 语句的功能。NOINT 选项的功能以及何时使用它？SUBJECT= OPTION 的功能是什么以及为什么使用它？MODEL 和 RANDOM 语句中各项隐含的矩阵运算。B() 和 *B 语法有什么功能？","code":"proc glimmix;\n model y=x;proc glimmix;\n  model y/n=x;proc glimmix;\n  model y/n=x / link=probit;proc glimmix;\n class trt;\n model y = trt;proc glimmix;\n class trt;\n model Y_ij/N_ij = trt;proc glimmix;\n class batch;\n model y=batch x(batch) / noint;proc glimmix;\n class batch;\n model y=batch x x*batch;proc glimmix;\n class batch;\n model y=batch x*batch / noint;proc glimmix;\n class batch;\n model y=batch x*batch;proc glimmix;\n class batch;\n model y = x;\n random batch x*batch;proc glimmix;\n class batch;\n model y = x;\n random intercept x / subject=batch;proc glimmix;\n class a b;\n model y=a b a*b;proc glimmix;\n class a b;\n model y = a*b;proc glimmix;\n class a b;\n model y = a*b / noint;proc glimmix;\n class a b;\n model y = a b(a);proc glimmix;\n class a b;\n model y = a a*b;proc glimmix;\n class a;\n model y =a b(a) / noint;proc glimmix;\n class a;\n model y =a b a*b;proc glimmix;\n class dp school classroom;\n model y=dp;\n random school classroom(school*dp);random school classroom*school*dp;proc glimmix;\n class dp school classroom;\n model y=dp;\n random intercept classroom*dp / subject=school;proc glimmix;\n class dp school classroom;\n model y=dp;\n random intercept / subject=school;\n random dp / subject=classroom(school);proc glimmix;\n class a b exp_unit;\n model y=a|b;model y=a b a*b;\n random exp_unit(a);proc glimmix;\n class exp_unit a b;\n model y = a*b / noint;\n random intercept / subject=exp_unit(a);proc glimmix;\n class exp_unit a b;\n model y = a b(a);model y = a a*b;\n random intercept / subject=exp_unit(a);proc glimmix;\n class block a b;\n model y = a|b;\n random intercept a / subject=block;proc glimmix;\n class block a b;\n model y = a|b;\n random intercept a b / subject=block;"},{"path":"chap3.html","id":"chap3","chapter":"第 3 章 搭建舞台","heading":"第 3 章 搭建舞台","text":"第 1 章介绍了线性统计模型的基本要素。模型至少必须指定线性预测器、观测的分布和连接函数。如果线性预测器是完全确定的，则我们只有固定效应线性预测器 \\(\\symbf{X\\beta}\\)。如果模型还包含随机效应，则线性预测变量为 \\(\\symbf {X\\beta}+\\symbf{Zb}\\)，并且必须指定随机效应 \\(\\symbf b\\) 的分布。第 2 章探讨了从数据集结构到模型构建的过程。所得模型的基本特征是，它必须是对产生观测结果过程的合理描述。这里的措辞很重要：“合理的描述”而不是“唯一真实的描述”。在科学中，我们可以——而且经常会——对一个过程有两个（或多个）合理的描述。统计推断的目的至少在一定程度上是为了确定哪些相互竞争的解释与数据更一致。本章的目的是为线性模型的统计推断奠定基础。这涉及两个一般主题。首先，一旦我们指定了一个模型并估计了它的参数，我们该如何处理这些估计？我们如何使用它们来回答最初促使我们收集数据的问题？这本身就是一个重要且未被充分重视的建模主题。尽管听起来令人惊讶，但统计分析和科学探究往往被划分为两种方式，使两者都变得贫瘠。本章的一个目标是帮助减少这种划分。从科学问题的角度构建参数估计——反之亦然——是一个重要步骤。能够在“科学语言”和“统计语言”之间轻松切换是很重要的。本章的第二个主题涉及在一个只有固定效应、只有高斯数据的世界中我们不会想到的问题。这些是模型尺度与数据尺度、推断空间和条件与边际问题。所有这些都在某种程度上被低估了，但正如我们将看到的那样，这些是至关重要的。它们在 3.2 节中介绍，并在 3.3 节至 3.5 节中详细讨论。","code":""},{"path":"chap3.html","id":"sec3-1","chapter":"第 3 章 搭建舞台","heading":"3.1 模型推断的目标：概述","text":"线性模型类型的一个有用的助记符是 (G)LM(M). 所有模型共有的是 LM，线性模型部分：每个线性模型都存在的线性预测器 \\(\\symbf{X\\beta}\\) 的固定效应分量。(G) 前缀（表示“广义”）表示观测的分布可能是非正态的；(M) 后缀表示线性预测器包括随机效应，即 \\(\\symbf{Zb}\\) 项。固定线性预测器 \\(\\symbf{X\\beta}\\) 很重要，因为正如我们在第 2 章中看到的，固定效应通常描述处理设计，而处理设计又由研究目标决定——这是我们想要回答的基本问题。因此，如果我们在建模方面做得很好，我们应该能够将每个目标表达为关于模型参数或其线性组合的问题。考虑以下示例：动机问题：药物的稳定性限制特性如何随时间变化？\n假定特性 \\(Y\\) 随时间线性变化，则根据线性回归方程 \\(Y={\\beta}_0+{\\beta}_1T\\) 可得 \\(\\symbf{X\\beta}\\)，其中 \\(T\\) 表示时间。关于“是否存在随时间的变化”或“\\(Y\\) 随 \\(T\\) 如何变化”的问题，都可以通过对斜率参数 \\(\\beta_1\\) 的推断来回答。但如果线性回归不能充分描述其随时间的变化该怎么办？我们可以用一个更适合的线性预测器来替代 \\({\\beta}_0+{\\beta}_1T\\)，例如二次多项式或更高阶多项式，甚至是非线性函数。即便如此，推断的重点依然会集中在反映随时间变化的相关参数上。动机问题：药物的稳定性限制特性如何随时间变化？\n假定特性 \\(Y\\) 随时间线性变化，则根据线性回归方程 \\(Y={\\beta}_0+{\\beta}_1T\\) 可得 \\(\\symbf{X\\beta}\\)，其中 \\(T\\) 表示时间。关于“是否存在随时间的变化”或“\\(Y\\) 随 \\(T\\) 如何变化”的问题，都可以通过对斜率参数 \\(\\beta_1\\) 的推断来回答。但如果线性回归不能充分描述其随时间的变化该怎么办？我们可以用一个更适合的线性预测器来替代 \\({\\beta}_0+{\\beta}_1T\\)，例如二次多项式或更高阶多项式，甚至是非线性函数。即便如此，推断的重点依然会集中在反映随时间变化的相关参数上。动机问题：三种或更多关注的处理如何影响响应？这里 \\(\\symbf{X\\beta}\\) 遵循单向模型 \\(\\eta+\\tau_i\\)，其中 \\(\\tau_i\\) 是第 \\(\\) 种处理的效应。科学问题可转译为一个或多个关于处理效应的陈述。关注点可能集中在评估处理的预期响应的总体相等性上，即 \\(\\tau_1=\\tau_2=\\tau_3\\)。或者，我们可能想要刻画各处理的预期平均性能。用模型术语来说，\\(\\eta+ \\tau_i\\) 提供了预期处理性能的起点。如果数据是高斯的，\\(\\eta+ \\tau_i\\) 也给出了处理均值的直接估计。如果数据是非高斯的，\\(\\eta+ \\tau_i\\) 根据连接函数给出估计，但不直接估计处理均值。这预示了 3.3 节中模型尺度与数据尺度的问题。\n对处理差异的关注点通常不仅仅是评估 \\(\\tau_1=\\tau_2=\\tau_3\\) 而是更具体的问题。通常，我们想要评估均值对之差 \\(\\tau_i-\\tau_{'}\\) 或均值的线性组合，例如 \\(\\tau_1-\\frac12\\big(\\tau_2+\\tau_3\\big)\\)。与预期处理效应一样，这些直接估计高斯数据的均值之差。对于非高斯数据，它们是根据连接函数进行估计的，如果需要均值之差，则需要转换。动机问题：三种或更多关注的处理如何影响响应？这里 \\(\\symbf{X\\beta}\\) 遵循单向模型 \\(\\eta+\\tau_i\\)，其中 \\(\\tau_i\\) 是第 \\(\\) 种处理的效应。科学问题可转译为一个或多个关于处理效应的陈述。关注点可能集中在评估处理的预期响应的总体相等性上，即 \\(\\tau_1=\\tau_2=\\tau_3\\)。或者，我们可能想要刻画各处理的预期平均性能。用模型术语来说，\\(\\eta+ \\tau_i\\) 提供了预期处理性能的起点。如果数据是高斯的，\\(\\eta+ \\tau_i\\) 也给出了处理均值的直接估计。如果数据是非高斯的，\\(\\eta+ \\tau_i\\) 根据连接函数给出估计，但不直接估计处理均值。这预示了 3.3 节中模型尺度与数据尺度的问题。对处理差异的关注点通常不仅仅是评估 \\(\\tau_1=\\tau_2=\\tau_3\\) 而是更具体的问题。通常，我们想要评估均值对之差 \\(\\tau_i-\\tau_{'}\\) 或均值的线性组合，例如 \\(\\tau_1-\\frac12\\big(\\tau_2+\\tau_3\\big)\\)。与预期处理效应一样，这些直接估计高斯数据的均值之差。对于非高斯数据，它们是根据连接函数进行估计的，如果需要均值之差，则需要转换。动机问题：在一项两处理生长曲线研究中，处理如何影响响应随时间的变化？\n简便起见，与第一个动机问题一样，假定响应随时间线性变化。\\(\\symbf{X\\beta}\\) 的一般形式为 \\(\\beta_{0i}+\\beta_{1i}T\\)，其中 \\(\\beta_{0i}\\) 表示第 \\(\\) 个处理的截距，\\(\\beta_{1i}\\) 是第 \\(\\) 个处理的斜率。推断可能关注两处理的斜率之差 \\(\\beta_{1,1}-\\beta_{1,2}\\)，或在特定时间下处理预期响应之差 \\(\\beta_{0,1}-\\beta_{0,2}+\\left(\\beta_{1,1}-\\beta_{1,2}\\right)T\\)，等等。动机问题：在一项两处理生长曲线研究中，处理如何影响响应随时间的变化？\n简便起见，与第一个动机问题一样，假定响应随时间线性变化。\\(\\symbf{X\\beta}\\) 的一般形式为 \\(\\beta_{0i}+\\beta_{1i}T\\)，其中 \\(\\beta_{0i}\\) 表示第 \\(\\) 个处理的截距，\\(\\beta_{1i}\\) 是第 \\(\\) 个处理的斜率。推断可能关注两处理的斜率之差 \\(\\beta_{1,1}-\\beta_{1,2}\\)，或在特定时间下处理预期响应之差 \\(\\beta_{0,1}-\\beta_{0,2}+\\left(\\beta_{1,1}-\\beta_{1,2}\\right)T\\)，等等。动机问题：两因素如何单独或作为一个系统来影响响应？\n请注意，实现这一目标需要一个析因处理结构。简便起见，考虑一个 \\(2×2\\) 析因——每个因素有 2 个水平，考虑所有可能的组合。这里 \\(\\symbf{X\\beta}\\) 遵循具有交互作用的双向模型 \\(\\eta_{ij}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\)。与所有模型一样，线性预测器用连接函数表示——\\(\\eta_{ij}\\) 可能直接估计处理组合均值 \\(\\mu_{ij}\\)（与高斯数据一样），也可能不直接估计（与非高斯数据通常的情况一样）。\n推断可能集中在以下一个或多个方面：处理组合均值；主效应均值，例如因子 的水平跨 B 所有水平的均值，反之亦然；主效应之差；简单效应，即在 B 的给定水平上，的两个水平之差；在 的给定水平上，B 的两个水平之差等等。这些目标中的每一个都可以用线性预测器的参数来表示，如表 3.1 所示。\n表 3.1\n\n如果数据是高斯的，那么第二列中的表达式实际上会估计第三列中的期望。对于非高斯数据，只有当模型使用恒等连接时，第二列才直接估计第三列中的期望（通常情况并非如此）。对于非恒等连接函数，第二列中的估计需要转换，具体方法取决于连接函数。\n总之，我们应该从评估这两个因素之间的交互作用开始。根据定义，当简单效应不相等时，就会出现交互作用。如果交互作用可以忽略不计，则主效应提供了有用的信息；否则，主效应往往是误导性的，最好关注简单效应。动机问题：两因素如何单独或作为一个系统来影响响应？\n请注意，实现这一目标需要一个析因处理结构。简便起见，考虑一个 \\(2×2\\) 析因——每个因素有 2 个水平，考虑所有可能的组合。这里 \\(\\symbf{X\\beta}\\) 遵循具有交互作用的双向模型 \\(\\eta_{ij}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\)。与所有模型一样，线性预测器用连接函数表示——\\(\\eta_{ij}\\) 可能直接估计处理组合均值 \\(\\mu_{ij}\\)（与高斯数据一样），也可能不直接估计（与非高斯数据通常的情况一样）。\n推断可能集中在以下一个或多个方面：处理组合均值；主效应均值，例如因子 的水平跨 B 所有水平的均值，反之亦然；主效应之差；简单效应，即在 B 的给定水平上，的两个水平之差；在 的给定水平上，B 的两个水平之差等等。这些目标中的每一个都可以用线性预测器的参数来表示，如表 3.1 所示。表 3.1如果数据是高斯的，那么第二列中的表达式实际上会估计第三列中的期望。对于非高斯数据，只有当模型使用恒等连接时，第二列才直接估计第三列中的期望（通常情况并非如此）。对于非恒等连接函数，第二列中的估计需要转换，具体方法取决于连接函数。\n总之，我们应该从评估这两个因素之间的交互作用开始。根据定义，当简单效应不相等时，就会出现交互作用。如果交互作用可以忽略不计，则主效应提供了有用的信息；否则，主效应往往是误导性的，最好关注简单效应。没有统计规则规定必须进行上述任何检验。为了使上述任何一项成为统计推断的有效目标，我们必须能够根据激发研究的问题和进行研究的学科语言来严格解释它。建议练习：将上面定义的参数的每个线性组合表述为“学科语言”，而不是这里给出的“统计语言”的表述。3.1 节的关键思想：将目标或动机问题表达为参数的线性组合；与回归、单向处理设计、析因处理结构相关的目标；简单效应、主效应、交互作用。","code":""},{"path":"chap3.html","id":"sec3-2","chapter":"第 3 章 搭建舞台","heading":"3.2 推断的基本工具","text":"3.1 节中的所有例子都说明了一个共同主题的变体。每个陈述都涉及估计或检验模型参数的线性组合。在本节中，我们介绍与这些线性组合相关的正式结构和术语。","code":""},{"path":"chap3.html","id":"sec3-2-1","chapter":"第 3 章 搭建舞台","heading":"3.2.1 可估函数","text":"3.1 节所示的参数线性组合的一般形式为 \\(\\symbf{K'\\beta}\\)，其中 \\(\\symbf{K}\\) 是一个 \\(p×k\\) 矩阵，\\(\\symbf\\beta\\) 是固定效应的 \\(p×1\\) 向量。我们要么估计 \\(\\symbf{K'\\beta}\\)，要么检验 \\(H_0:\\symbf{K'\\beta} =\\symbf\\theta\\) ，其中 \\(\\symbf\\theta\\) 是 \\(p × 1\\) 常数向量。通常我们会设计检验使得 \\(\\symbf\\theta=\\symbf 0\\)。在上述展示的大多数例子中，\\(\\symbf{K}\\) 是一个向量——我们将这样的向量记为 \\(\\symbf{k}\\)。在第一个例子中：\\[\\symbf{\\beta}=\\begin{bmatrix}{\\beta}_0\\\\{\\beta}_1\\end{bmatrix}\\]关注检验 \\(H_0\\colon\\beta_1=0\\)，因此 \\(\\symbf{k}'=\\begin{bmatrix}0&1\\end{bmatrix}\\)。在第二个例子中：\\[\\symbf{\\beta}=\\begin{bmatrix}\\mu\\\\\\tau_1\\\\\\tau_2\\\\\\tau_3\\end{bmatrix}\\]为估计处理均值，例如，处理 1，\\(\\symbf{k}'=\\begin{bmatrix}1&1&0&0\\end{bmatrix}\\)。为估计或检验处理差异，例如处理 1 与处理 2，\\(\\symbf{k}'=\\begin{bmatrix}0&1&-1&0\\end{bmatrix}\\)。对于处理 1 与处理 2 和 3 均值的对比7 (contrast)：\\[\\symbf{k}'=\\begin{bmatrix}0&1&-\\frac{1}{2}&-\\frac{1}{2}\\end{bmatrix}\\]对于处理均值的总体相等性，\\(\\symbf K\\) 必须由两个向量组成，这两个向量是保证 \\(\\tau_1=\\tau_2=\\tau_3\\) 的任意向量。为什么是两个？因为有三种处理，因此处理之间的差异有两个自由度——每个自由度一个对比向量。矩阵 \\(\\symbf{K}'=\\begin{bmatrix}0&1&0&-1\\\\[0.3em]0&0&1&-1\\end{bmatrix}\\) 满足要求吗？为什么？矩阵 \\(\\symbf{K}'=\\begin{bmatrix}0&1&-\\frac12&-\\frac12\\\\[0.3em]0&0&1&-1\\end{bmatrix}\\) 呢？建议练习：还有哪些矩阵可用来定义比较 (comparisons)？总之，当研究目标集中在固定因素效应时，推断首先将这些目标转化为有关线性预测器固定参数线性组合的陈述。这些线性组合称为可估函数 (estimable functions)，用 \\(\\symbf{K'\\beta}\\) 表示。顾名思义，\\(\\symbf{K'\\beta}\\) 必须是可估的——第 4 章和第 6 章介绍了与可估性 (estimability) 相关的正式理论。目前，需要注意的是，当线性预测器中的 \\(\\symbf X\\) 矩阵满秩时，总是满足可估性要求。当 \\(\\symbf X\\) 不满秩时，条件 \\(\\symbf{K}'=(\\symbf{X'X})^-(\\symbf{X'X})\\mathbf{K}'\\) 必须成立，其中 \\((\\symbf{X'X})^-\\) 表示广义逆。","code":""},{"path":"chap3.html","id":"sec3-2-2","chapter":"第 3 章 搭建舞台","heading":"3.2.2 固定效应和随机效应的线性组合：可预测函数","text":"参考 3.1 节中的示例 2，比较三个或更多处理。现在我们增加一点：假设我们在多个地点应用每种处理。这些地点可以是不同的诊所、学校、实验室设施或农场——笼统地说，可以把它们当作不同的地点。此时线性预测器为 \\(\\eta+\\tau_i+L_j\\)，其中 \\(L_j\\) 表示第 \\(j\\) 个地点效应。假设有 \\(L\\) 个地点，因此 \\(j=1,2,...,L\\)。此时在构建模型时，我们需要决定地点效应是固定的还是随机的。这一决定对可估性标准以及我们如何解释由此产生的估计和检验具有重要意义。这些影响很重要，所以我们需要理解它们。为此两种情况都加以考虑，从固定的地点效应开始。如果地点效应是固定的，则线性组合 \\(\\eta+\\tau_i\\) 不符合上面给出的可估性标准。估计处理的预期响应（处理均值）需要所有地点的均值——因此相应的可估函数为 \\(\\eta+\\tau_i+\\bar{L}_\\cdot\\)，其中 \\(\\bar{L}_\\cdot=\\frac1L\\sum_jL_j\\)。对于处理之间的比较，例如处理 1 与处理 2 或处理 1 与处理 2 和 3 均值的比较，所有地点的均值将会抵消，得到与该处理设计先前所示的相同的可估函数。地点效应出现在处理均值的可估函数中，但不出现在处理差异的可估函数中。如果地点效应是随机的，则 \\(L_j\\) 不再是模型参数；它们是假定 \\(\\mathrm{..d.}\\quad N\\left(0,\\sigma_L^2\\right)\\) 的随机变量。因此，\\(L_j\\) 不再参与可估性议题：在具有随机地点效应的情况下，\\(\\eta+\\tau_i\\) 是可估的，是评估处理均值的起点。然而，我们可能会问，如果我们确实试图估计 \\(\\eta+\\tau_i+\\bar{L}_\\cdot\\)，那么在随机地点效应模型中会发生什么？这是可能的吗？如果可能，我们该如何解释？答案是可能的。函数 \\(\\eta+\\tau_i+\\bar{L}_\\cdot\\) 现在是固定效应和随机效应的线性组合。线性模型推断中的一般表示法是 \\(\\symbf{K^{\\prime}\\beta+M^{\\prime}b}\\)，这称为可预测函数 (predictable function)，前提是 \\(\\symbf {K'\\beta}\\) 满足上面给出的可估性要求。对于“我们该如何解释？”，我们推迟到第 3.4 节讨论推断空间 (inference space) 时再回答。当数据允许将地点 × 处理项添加到线性预测器时，即 \\(\\eta+\\tau_i+L_j+\\left(\\tau L\\right)_{ij}\\)，多地点多处理示例变得更加有趣，其中 \\((\\tau L)_{ij}\\) 表示地点 × 处理交互效应。现在可以根据跨所有地点的平均处理差异（例如处理 1 与处理 2）或特定地点的处理差异来设定目标。前者的可估函数，对于固定地点效应模型为 \\(\\tau_1-\\tau_2+\\overline{\\left(\\tau L\\right)}_{1\\cdot}-\\overline{\\left(\\tau L\\right)}_{2\\cdot}\\)，对于随机地点效应模型为 \\(\\tau_1-\\tau_2\\)。对于这两种模型，\\(\\tau_1-\\tau_2+\\left(\\tau L\\right)_{1j}-\\left(\\tau L\\right)_{2j}\\) 估计地点 \\(j\\) 处的处理 1 与处理 2 的差异，但对于固定地点模型，这是一个可估函数，而对于随机地点模型，它是固定和随机效应的线性组合——一个可预测函数。我们将在 3.4 节中探讨其具体含义。","code":""},{"path":"chap3.html","id":"sec3-2-3","chapter":"第 3 章 搭建舞台","heading":"3.2.3 推断的三个问题","text":"在使用我们在本节中讨论的函数类型时，有三个主要问题。他们是","code":""},{"path":"chap3.html","id":"sec3-2-3-1","chapter":"第 3 章 搭建舞台","heading":"3.2.3.1 模型尺度与数据尺度","text":"这是具有非恒等连接函数的模型独有的问题。对于高斯数据，或者更普遍地说，对于恒等连接，可估函数实际上估计的是期望值或期望之差。但是，对于通常与非高斯数据一起使用的非恒等连接函数，这并不成立。例如，在二项数据 logit 模型中，可估函数 \\(\\eta+\\tau_i\\) 估计的是 logit 值，或等价地，是对数几率 (log odds). 将这表达为接受第 \\(\\) 种处理的个体的预期估计，意味着将以概率而非 logit 的形式表达 \\(\\eta+\\tau_i\\)。这需要使用逆连接将估计转换为概率，即\\[\\pi_i=1/\\left[1+e^{-(\\eta+\\tau_i)}\\right]\\]因此，对于非恒等连接，有两种表达估计的方法：直接用模型参数（模型尺度）或响应变量的期望（数据尺度）。我们在 3.3 节更详细地考虑这个问题。","code":""},{"path":"chap3.html","id":"sec3-2-3-2","chapter":"第 3 章 搭建舞台","heading":"3.2.3.2 推断空间","text":"只有当线性预测器包含随机效应时，才会出现这个问题。在这些模型中，仅由固定效应的线性组合产生的估计 \\(\\symbf{K'\\beta}\\)，表示适用于随机模型效应所代表的整个总体的推断。这称为广义推断 (broad inference) ——之所以如此命名，是因为它广泛适用于整个总体。一些作者也称之为“总体平均” (“population averaged”, PA) 推断，但是广义推断和 PA 推断并不相同。正如我们将在 3.5 节中看到的，PA 推断是广义推断的子集，但并非所有广义推断都是 PA 推断。可预测函数呢？对于 \\(\\symbf{K'\\beta}+\\symbf{M'b}\\)，在 \\(\\symbf M\\ne\\symbf 0\\) 的情况下，\\(\\symbf M\\) 矩阵中的非零系数专门将推断限制在由 \\(\\symbf M\\) 矩阵定义的 \\(\\symbf b\\) 的那些水平上。三处理多地点示例说明了这种情况。可估函数 \\(\\tau_1−\\tau_2\\) 为处理 1 和 2 之间的差异提供了广义推断，而可预测函数 \\(\\tau_1-\\tau_2+\\left(\\tau L\\right)_{1j}-\\left(\\tau L\\right)_{2j}\\) 将处理 1 和处理 2 的推断范围限制在地点 \\(j\\)。这称为狭义推断 (narrow inference)，之所以如此命名，是因为 \\(\\symbf M\\) 的非零系数将推断范围从整个总体缩小到 \\(\\symbf M\\) 所确定的水平。在某些应用中，狭义推断称为特定个体 (subject-specific, SS) 推断。","code":""},{"path":"chap3.html","id":"sec3-2-3-3","chapter":"第 3 章 搭建舞台","heading":"3.2.3.3 基于条件模型和边际模型的推断","text":"这个问题是混合模型独有的，对于非高斯数据的混合模型来说更是如此，即使是有经验的统计从业者也普遍误解（或根本不理解）这个问题。我们根据两个概率分布指定了广义线性混合模型：给定随机效应的观测 \\(\\symbf y\\mid \\symbf b\\) 的条件分布和随机效应 \\(\\symbf b\\) 的分布。如此指定的 GLMM 称为条件模型 (conditional models). 条件模型中的两种分布都无法直接观察到。根据概率论可知，数据 \\(\\symbf y\\) 的边际分布可通过对 \\(\\symbf y\\) 和 \\(\\symbf b\\) 的联合密度关于 \\(\\symbf b\\) 进行积分来得到，其概率密度函数可表示为\\[f\\left(\\symbf{y}\\right)=\\int_{\\symbf{b}}f\\left(\\symbf{y}\\mid\\symbf{b}\\right)f\\left(\\symbf{b}\\right)d\\,\\symbf{b}\\]边际分布是我们唯一能直接观察到的分布。在非高斯混合模型设定中，许多看似合理的模型没有明确考虑 \\(\\symbf b\\) 的分布。这些模型隐式地将推断限制在 \\(\\symbf y\\) 的边际分布上。这些称为边际模型 (marginal models). 边际模型产生的估计与条件模型的估计具有不同的期望——两模型估计的目标不同。决定哪种推断适合自己的目标是 GLMMs 工作的强制性部分。对边际/条件模型推断区分的无知常常会带来不良后果。当非高斯数据源于存在随机效应的场景时，观测数据服从边际分布。除条件 GLMM 之外的所有模型都会得到边际估计的某种变体，通常是边际分布的均值或均值的函数。然而，无论使用何种模型，数据分析师很可能按照条件 GLMM 的方式来理解问题。即使是为了“避免 GLMM 相关问题”而不使用 GLMM，情况也是如此。实际估计的与数据分析师认为自己估计的完全不同，这样的情况十分常见。如果这一点尚不清楚，3.5 节给出了一个熟悉的例子以澄清该问题。3.2 节的关键思想：可估函数；可估性标准；可预测函数；GLMM 推断的三个问题：数据/模型尺度；推断空间；条件/边际。","code":""},{"path":"chap3.html","id":"sec3-3","chapter":"第 3 章 搭建舞台","heading":"3.3 问题 1：数据尺度与模型尺度","text":"正如我们在上一节中看到的，推断从可估函数 \\(\\symbf{K'\\beta}\\) 开始。因为所有线性模型都是根据连接函数定义的 \\(\\symbf{\\eta}=g\\left(\\symbf{\\mu}\\right)=\\symbf{X}\\symbf{\\beta}\\)（\\(+\\symbf Z\\symbf{b}\\) 如果存在随机效应），\\(\\symbf{K'\\beta}\\) 得到关于连接函数的结果。对于高斯模型，LMs 和 LMMs，连接函数实际上是不可见的。这是因为这些模型使用恒等连接。模型参数的线性组合直接估计期望值、期望值之差以及期望值之间的对比。LM 的推断很简单。事实上，高斯情形的线性模型理论完全可以在没有连接函数概念的情况下进行。对于非高斯线性模型，GLMs 和 GLMMs，情况并非如此。这里，\\(\\symbf{K'\\beta}\\) 的估计得到了 \\(\\symbf\\eta\\) 元素的线性组合，即 \\(g(\\symbf\\mu)\\) 的线性组合——通常是 \\(\\symbf\\mu\\) 的非线性函数，而不是 \\(\\symbf\\mu\\) 本身。例如，对于二项数据，\\(\\symbf{K'\\beta}\\) 通常是 logits 或 probits 的函数；对于泊松数据，它是 logs 的函数。然而，在大多数情况下，研究人员和决策者希望看到以感兴趣结果的概率表示的二项结果和以计数表示的泊松结果。这是模型尺度/数据尺度，或连接尺度/平均尺度的问题。为了说明模型尺度/数据尺度的问题，我们考虑了五个例子。前两个例子来自两处理完全随机设计，一个使用高斯数据，另一个使用二项数据。接下来的两个例子说明了一个更复杂的数据结构：四处理和多地点，一个是高斯数据，另一个是二项数据。我们在二项数据中看到的问题是非高斯数据的典型问题，因此这些例子的教训适用于其他非高斯数据。第五个例子回顾了四种处理场景。在第三和第四个例子中，没有对四处理设计施加任何结构；而在第五个例子中，我们假定四处理具有 \\(2×2\\) 析因结构。这使我们能够考虑一些有用的策略为析因处理设计建模。在此过程中，本节应提供额外的工具来处理可估函数以及理解模型尺度和数据尺度。","code":""},{"path":"chap3.html","id":"sec3-3-1","chapter":"第 3 章 搭建舞台","heading":"3.3.1 一个模型尺度示例——高斯数据，两处理独立样本","text":"示例 3.1  (原书示例 3.3.1) 我们从一个熟悉的入门统计示例开始：比较两处理的独立样本，更正式地称为两处理完全随机设计。我们在前几章中开发了该模型。回顾一下，我们将其描述为线性预测器：\\(\\eta_i=\\eta+\\tau_i\\)（模型 3.3.1）分布：\\(y_{ij}\\thicksim N\\left(\\mu_i,\\sigma^2\\right)\\)连接：恒等 \\(\\eta_i=\\mu_i\\)此示例的数据在 SAS Data Program Library8 中显示为 Data Set 3.1. 请注意，Data Set 3.1 显示了假定为高斯的响应变量 \\(Y\\) 和由变量 \\(N\\) 和 \\(F\\) 定义的另一个响应，将在本示例的二项版本中使用。在 SAS 中可以使用任何线性模型软件程序（GLM, GLIMMIX 或 MIXED）获得的参数估计值，为：严格来说，我们应该将这些称为“解”而不是估计，除了 \\(\\hat\\sigma^2\\)，因为这是一个过度参数化模型，必须使用广义逆——在这种情况下，必须使用将最后一个类别的效应设置为零的 SAS 扫描算子。参见第 4 章以了解过度参数化模型和广义逆的介绍。在本示例中，考虑处理均值和处理均值差的可估函数。它们是“处理 0”（“对照”处理）的 \\(\\eta+\\tau_0\\)，“处理 1”（“试验”处理）的 \\(\\eta+\\tau_1\\)，以及均值差的 \\(\\tau_0-\\tau_1\\)。根据这些解，我们可以很容易地看出，“处理 0”的估计为 \\(12.188-1.664=10 524\\)，“处理 1”的估计为 \\(12.188\\)，均值差为 \\(-1.664\\)。在 SAS 中，我们可以使用 LSMEANS 或 ESTIMATE 语句来获取这些值。GLIMMIX 程序是LSMEANS 语句足以计算处理均值和均值差。E 选项为你提供了 SAS 用于计算 LSMEANS 的可估函数的系数列表，因此你可以确保它正在执行你认为它正在执行的操作。ESTIMATE 语句更明确地指定了所需的项：INTERCEPT 的系数指的是 \\(\\eta\\)，TRT 的系数指的是 \\(\\tau_i\\)，其中第一个和第二个系数对应于它们在上述参数估计列表中列出的顺序。请注意，你可以以负系数开头，如在 'reverse diff' 中，这样可以让你在不考虑符号的情况下计算差异的大小。你还可以计算由可估函数 \\(\\eta+\\frac12\\big(\\tau_0+\\tau_1\\big)\\) 定义的总均值这些语句的输出如下所示：这里需要强调的重要一点是，所有这些估计都是模型参数本身的线性组合，它们都估计了观测变量期望值的线性组合—— \\(\\symbf{K'\\hat\\beta}\\)直接产生 \\(\\hat\\mu_i\\) 的线性组合。","code":"proc glimmix data=CRD_2Trt_Ex;\n class Trt;\n model Y=Trt / solution;\n lsmeans Trt / Diff e;\n estimate 'LSM Trt 0' intercept 1 Trt 1 0;\n estimate 'LSM Trt 1' intercept 1 Trt 0 1;\n estimate 'overall mean' intercept 1 Trt 0.5 0.5;\n estimate 'Overall Mean' intercept 2 Trt 1 1 / divisor=2;\n estimate 'Trt diff' Trt 1 -1;\n estimate 'reverse diff' Trt -1 1;"},{"path":"chap3.html","id":"sec3-3-2","chapter":"第 3 章 搭建舞台","heading":"3.3.2 模型尺度估计","text":"示例 3.2  (原书示例 3.3.2；具有二项响应的两处理完全随机设计) 我们现在考虑由变量 \\(N\\) 和 \\(F\\) 定义的 Data Set 3.1 中的二项响应，其中 \\(N\\) 表示在第 \\(\\) 个单元上观察的独立伯努利试验数（例如临床试验中的受试者数或参加测试的学生数），\\(F\\) 表示“成功”数（如症状改善的受试者或通过考试的学生）。线性预测器与我们刚刚考虑的模型 3.3.1 相同，但分布和连接不同：分布：\\(y_{ij}\\thicksim \\text{Binomial}\\left(N_{ij},\\pi_i\\right)\\)（模型 3.3.2）连接：\\(\\eta_{ij}=\\text{logit}\\big(\\pi_i\\big)=\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)\\)可在 SAS 中使用 PROC GENMOD 或 GLIMMIX 获得的效应的解为与高斯示例类似，我们可以通过 \\(−1.2682-0=−1.2682\\) 来估计处理效应之差 \\(\\tau_0-\\tau_1\\)，“处理 0”和“处理 1”线性预测器分别为 \\(\\hat{\\eta}+\\hat{\\tau}_0=-0.886-1.2682=-2.1542\\) 和 \\(\\hat{\\eta}+\\hat{\\tau}_1=-0.886-0=-0.886\\)，我们还可估计“总体总和” (“overall total”)：\\[\\hat{\\eta}+\\frac12\\Big(\\hat{\\tau}_0+\\hat{\\tau}_1\\Big)=-0.886+\\frac12\\Big(-1.2682+0\\Big)=-1.5201\\]问题是，这些实际上在估计什么？在“处理 0”和“处理 1”的情况下，我们从模型的线性预测器中可看出，根据定义，它们估计 \\(\\log\\biggl[\\pi_0\\bigl/\\bigl(1-\\pi_0\\bigr)\\biggr]\\) 和 \\(\\log\\biggl[\\pi_1\\bigl/\\bigl(1-\\pi_1\\bigr)\\biggr]\\)，处理 0 和 1 的对数几率。我们可以通过应用逆连接获得 \\(\\pi_0\\) 和 \\(\\pi_1\\) 的估计，即处理 0 和 1 有利结果的概率 \\(\\hat{\\pi}_0=1\\Big/\\Big[1+e^{-(\\eta+\\hat{\\tau}_0)}\\Big]=1\\Big/\\Big(1+e^{2.1542}\\Big)=0.104\\) 和 \\(\\hat{\\pi}_1=1/\\left(1+e^{0.886}\\right)=0.292\\)。在处理 0 和处理 1 之间存在差异的情况下，我们可以看到 \\(\\tau_0-\\tau_1\\) 估计 \\(\\log\\begin{bmatrix}\\pi_0/(1-\\pi_0)\\end{bmatrix}-\\log\\begin{bmatrix}\\pi_1/(1-\\pi_1)\\end{bmatrix}=\\log\\left(\\begin{bmatrix}\\pi_0/(1-\\pi_0)\\end{bmatrix}/\\begin{bmatrix}\\pi_1/(1-\\pi_1)\\end{bmatrix}\\right)\\)，对数几率比 (log odds ratio). 然而，如果我们将逆连接应用于几率比，我们得到 \\(1/\\left[1+e^{-(\\hat{\\tau}_{0}-\\hat{\\tau}_{1})}\\right]=1/\\left(1+e^{1.2682}\\right)=0.220\\)。尚还不清楚这是什么，但显然不是均值差 \\(\\hat{\\pi}_0-\\hat{\\pi}_1\\)。在大多数情况下，将逆连接应用于定义差异的可估函数会得到无意义的结果。","code":""},{"path":"chap3.html","id":"sec3-3-3","chapter":"第 3 章 搭建舞台","heading":"3.3.3 数据尺度","text":"示例 3.2 清楚地表明，对于非高斯数据，估计和推断发生在两个不同的尺度上。线性预测器 \\(\\symbf{X\\beta}\\) 和可估函数 \\(\\symbf{K'\\beta}\\) 用连接函数表示——示例 3.2 中的 logit. 这些称为模型尺度（或连接尺度）估计。在某些模型中，这些估计具有可识别的解释。logit 模型就是这样一种情况——给定处理的线性预测器估计对数几率，差异估计对数几率比，这都是分类数据分析中常见的工具。然而，logit 连接与其说是规则，不如说是例外。通常，如果不进行一些转换，则很难解释广义线性模型中的模型尺度估计。这就是数据尺度的用武之地。在最基本的形式中，数据尺度涉及将逆连接应用于可估函数，就像我们将每种处理的对数几率转换为概率一样。然而，正如我们在差异中看到的那样，逆连接不能不加区分地应用。一般地，我们使用逆连接将模型尺度的位置估计（连接尺度上的“均值”）转换为数据尺度的估计，但我们不使用逆连接来估计差异。连接函数通常是非线性的；差异在非线性函数中无法保留。将逆连接应用于差异通常会产生无意义的结果。要估计数据尺度上的差异——例如概率之差，而不是对数几率之差——我们必须变得更聪明。在 logit 模型中，考虑解决差异问题的两种方法。首先，在将逆连接应用于每种处理的线性预测器后，我们可以简单地取 \\(\\hat\\pi_0\\) 和 \\(\\hat\\pi_1\\) 之差。正式而言，我们通过 \\(\\left[1/\\left(1+e^{-\\left(\\eta+\\hat{\\tau}_0\\right)}\\right)\\right]-\\left[1/\\left(1+e^{-\\left(\\eta+\\hat{\\tau}_1\\right)}\\right)\\right]\\) 来估计 \\(\\pi_0\\) 和 \\(\\pi_1\\) 之差，而非 \\(1/\\left(1+e^{-\\left(\\hat{\\tau}_0-\\hat{\\tau}_1\\right)}\\right)\\)。这是方法一。或者，我们知道 \\(\\tau_0-\\tau_1\\) 估计对数几率比，则 \\(e^{\\left(\\tau_0-\\tau_1\\right)}\\) 是几率比的估计，这是分类数据分析中熟悉且常用的量。这是方法二。任何一种方法都可以辩护。你使用哪一个取决于特定研究的要求，在某种程度上也取决于进行数据分析的学科文化。表 3.2 总结了模型尺度/数据尺度的区别。\n在 SAS PROC GLIMMIX 中，可以使用 ILINK, EXP 和 ODDSRATIO 选项实现数据尺度。下面的 GLIMMIX 程序对此进行了说明，修改了完全随机设计高斯数据（示例 3.1）的程序以用于二项数据。ILINK 选项出现在 LSMEANS 语句和 ESTIMATE 语句中，其中计算的是均值而不是差值。在 LSMEANS 语句中，GLIMMIX 仅计算最小二乘均值的 ILINK，而不计算它们的差。对于“总体均值”，使用 ILINK 选项值得怀疑；此处显示它出于演示目的。均值的逆连接不等于逆连接的均值——免责声明。ODDSRATIO 选项出现在 MODEL 和 LSMEANS 语句中，以及计算差异的 ESTIMATE 语句中。或者，你可以使用 EXP 选项——顾名思义，它计算 \\(e^{\\text{estimate}}\\)。这是与 ODDSRATIO 相同的计算：结果和输出列表都相同。以下显示了选定的 GLIMMIX 输出。MODEL 语句的 ODDSRATIO 输出。注意几率比的 95% 置信区间；置信区间是默认输出。MODEL 语句的 ODDSRATIO 输出。注意几率比的 95% 置信区间；置信区间是默认输出。least square means 中的 “Estimate” 是模型尺度 \\(\\hat{\\eta}+\\hat{\\tau}_i\\)。“MEAN” 为逆连接估计 \\(\\hat{\\pi}_i=1/\\left[1+e^{-(\\hat{\\eta}+\\hat{\\tau}_i)}\\right]\\)least square means 中的 “Estimate” 是模型尺度 \\(\\hat{\\eta}+\\hat{\\tau}_i\\)。“MEAN” 为逆连接估计 \\(\\hat{\\pi}_i=1/\\left[1+e^{-(\\hat{\\eta}+\\hat{\\tau}_i)}\\right]\\)差异的 “Estimate” 是模型尺度 \\(\\hat\\tau_0-\\hat\\tau_1\\)。“OddsRatio” 是 \\(e^{\\hat\\tau_0-\\hat\\tau_1}\\)。我们可以添加 CL 选项来获得置信区间。默认不给出 CI.在 “Estimates” 列表中，无论你在 Estimate 语句中使用 ODDSRATIO 还是 EXP 选项，几率比都显示为 “Exponentiated Estimate”. 请注意，应用于“总体平均”连接函数 \\(\\hat{\\eta}+\\frac12\\big(\\hat{\\tau}_0+\\hat{\\tau}_1\\big)\\) 的逆连接值 \\(0.1795\\) 显然与每个处理 \\(\\hat\\tau_i\\) 的平均 \\((0.1039+0.2919)/2=0.1979\\) 不同。示例 3.3  (原书示例 3.3.3) 在本示例中，有四种处理，记为 0, 1, 2 和 3. 这项研究在八个地点进行，在每个地点观察所有四种处理。从实验设计的角度来看，这是一个以地点为区组的随机完全区组设计。然而，具有如此结构的数据集可能来自非正式设计的实验研究。数据在 SAS Data Program Library 中显示为 Data Set 3.2. 此数据集有多个响应变量。在本例中，我们考虑标记为 \\(Y\\) 的高斯响应。这些数据的候选模型如下：线性预测器：\\(\\eta_{ij}=\\eta+\\tau_{}+L_{j}\\)，其中 \\(\\tau_i(=0,1,2,3)\\) 表示第 \\(\\) 个处理效应；\\(L_j(j=1,2,\\ldots,8)\\) 表示第 \\(j\\) 个地点效应。分布 \\(y_{ij}\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)连接：恒等 \\(\\eta_{ij}=\\mu_{ij}\\)实验设计的学生会意识到，这是以“适合 GLMM”的形式呈现的随机区组设计的标准线性模型。在本例中，地点效应是固定的。在 3.4 节中我们会重新讨论这个决定。实际上，我们需要询问这些地点是如何选择的，以及它们应该代表什么。这些问题引出了我们在下一节将要处理的推断空间问题。与示例 3.1 和示例 3.2 一样，我们从处理均值和处理差异开始。处理均值立即提出了一个问题。在前面的例子中，我们将其定义为 \\(\\eta+\\tau_i\\)，然而在本例中它是不可估的。为什么？我们知道，我们通过 \\(\\bar{y}_{\\cdot}=1/8\\sum_{j=1}^8y_{ij}\\) 估计处理均值，以模型项表示，\\(\\bar y_{\\cdot}\\) 估计 \\(1/8\\sum_{j=1}^8\\left(\\eta+\\tau_i+L_j\\right)=\\eta+\\tau_i+1/8\\sum_jL_j\\)。对于最后一项 \\(1/8\\sum_jL_j\\)，我们可以使用简写符号 \\(\\bar L_\\cdot\\)。该项在处理均值的可估函数中必须出现。对于处理均值差，我们通过取最小二乘均值之差来定义其可估函数，即\\[diff=\\eta+\\tau_i+\\bar{L}_\\cdot-\\left(\\eta+\\tau_{'}+\\bar{L}_\\cdot\\right)=\\tau_i-\\tau_{'}\\]根据目标，我们还可以定义几种均值的平均。例如，考虑处理 0, 2 和 3 的平均。首先使用 \\(\\bar{\\eta}_{\\cdot}=1/8\\sum_j\\eta_{ij}\\) 来表示第 \\(\\) 种处理的最小二乘均值。取这些的平均，并用模型项表示为：\\[avg=\\frac{\\bar{\\eta}_{0\\cdot}+\\bar{\\eta}_{2\\cdot}+\\bar{\\eta}_{3\\cdot}}3=\\frac{\\left(\\eta+\\tau_0+\\bar{L}_\\cdot\\right)+\\left(\\eta+\\tau_2+\\bar{L}_\\cdot\\right)+\\left(\\eta+\\tau_3+\\bar{L}_\\cdot\\right)}3=\\eta+\\frac{\\tau_0+\\tau_2+\\tau_3}3+\\bar{L}_\\cdot\\]如果目标是将处理 1 与处理 0, 2 和 3 的均值进行比较，我们使用可估函数\\[diff\\left(\\text{trt 1 vs. trts 0,2,3}\\right)=\\bar{\\eta}_{1\\cdot}-\\left(\\frac{\\bar{\\eta}_{0\\cdot}+\\bar{\\eta}_{2\\cdot}+\\bar{\\eta}_{3\\cdot}}3\\right)=\\tau_1-\\left(\\frac{\\tau_0+\\tau_2+\\tau_3}3\\right)\\]处理均值的总体相等性用可估函数可表示为\\[\\bar{\\eta}_{0\\cdot}=\\bar{\\eta}_{1\\cdot}=\\bar{\\eta}_{2\\cdot}=\\bar{\\eta}_{3\\cdot}\\Rightarrow\\tau_0=\\tau_1=\\tau_2=\\tau_3\\]正如我们在本章前面看到的，可以构造几个 \\(\\symbf K\\) 矩阵，当 \\(\\symbf{K'\\beta}=\\symbf 0\\) 时，意味着 \\(\\tau_0=\\tau_1=\\tau_2=\\tau_3\\)。举三个例子：\\[\\begin{aligned}\\symbf{K'}&=\\begin{bmatrix}0&1&0&0&-1&0&0&0&0&0&0&0&0\\\\0&0&1&0&-1&0&0&0&0&0&0&0&0\\\\0&0&0&1&-1&0&0&0&0&0&0&0&0\\end{bmatrix}\\\\\n\\symbf{K'}&=\\begin{bmatrix}0&1&-1&0&0&0&0&0&0&0&0&0&0\\\\0&0&1&-1&0&0&0&0&0&0&0&0&0\\\\0&0&0&1&-1&0&0&0&0&0&0&0&0\\end{bmatrix}\\\\\n\\symbf{K'}&=\\begin{bmatrix}0&-1&3&-1&-1&0&0&0&0&0&0&0&0\\\\0&2&0&-1&-1&0&0&0&0&0&0&0&0\\\\0&0&0&1&-1&0&0&0&0&0&0&0&0\\end{bmatrix}\\end{aligned}\\]注意，对应于截距 \\(\\eta\\) 的第一列的系数总是为零，最后的对应与 \\(L_1,L_2,\\ldots,L_8\\) 的八列也是如此。只有 \\(\\tau_i\\) 可能具有非零系数。请注意，第三个 \\(\\symbf K\\) 矩阵仅仅是先前定义的“处理 1 与处理 0, 2 和 3 的均值”对比的一组正交对比的补全。\\(\\symbf K\\) 矩阵可以由正交对比组成但这不是必要条件，正如前两个 \\(\\symbf K\\) 矩阵所示。两个要求是：\\(\\symbf K'\\) 的每行对应一个自由度行必须相互独立，但不必相互正交。相互独立意味着没有一行的比较可以由其他两行的结果决定。例如，在第二个矩阵中，第 1 行和第 2 行定义了 \\(\\tau_0-\\tau_1\\) 和 \\(\\tau_1-\\tau_2\\)。如果它们都等于零，那么 \\(\\tau_0=\\tau_1=\\tau_2\\) 必为真，但它不涉及 \\(\\tau_3\\)。必须添加一个额外的涉及 \\(\\tau_3\\) 的比较，且不与 \\(\\tau_0=\\tau_1=\\tau_2\\) 冲突。为什么要关注使用 \\(\\symbf K\\) 矩阵来定义通常与 ANOVA 总体处理效应检验相关的比较呢？首先，这是在仅包含固定效应的高斯线性模型软件（例如 SAS PROC GLM）中计算 ANOVA 平方和的一个常用策略。更重要的是，在其他线性模型—— GLM, LMM 和 GLMM 中，平方和失去了其通常的含义（事实上，在这些模型中的大多数情况下并无实际意义），因此我们需要寻找平方和的替代方法，以便构建检验处理效应总体相等性的检验。这里展示的可估函数方法是一个重要且广泛应用的策略。实现本例的 GLIMMIX 语句如下该程序包含我们之前看到的常见的 CLASS, MODEL 和 LSMEANS 语句。ESTIMATE 语句明确定义了可估函数。前两个，“LSM Trt 0” 和 “LSM Trt 0 alt” 定义了处理 0 的均值。第一个是正确的可估函数；第二个显示了当我们试图将 \\(\\bar L_\\cdot\\) 设置为零时会发生什么。语句 “Trt 0 v Trt 1” 不言自明。语句 “avg Trt 0+1” 和 “avg Trt 0_2_3” 分别定义了处理 0 和 1 的均值以及处理 0, 2 和 3 的均值的可估函数。回想我们之前是如何构建 “avg Trt 0_2_3” 的，方法是从 \\(\\left(\\bar{\\eta}_{0\\cdot}+\\bar{\\eta}_{2\\cdot}+\\bar{\\eta}_{3\\cdot}\\right)/3\\) 开始，然后用线性预测器中的效应来表示。当你不确定在 ESTIMATE 语句中分配什么系数时，请使用此策略。下一个 ESTIMATE 语句显示如何在一个语句中计算多个估计。如果我们想使用 adjust= 选项调整多重性（控制实验的 (experiment-wise) 而不是比较的 (comparison-wise) 类错误），我们会这样做。最后是三个对比语句。这些实现了我们前面讨论的 \\(\\tau_0=\\tau_1=\\tau_2=\\tau_3\\) 的三个 \\(\\symbf K\\) 矩阵。选定的输出显示如下。为了技术上的正确，上面的 “parameter estimates” 实际上是使用 SAS 扫描算子来计算广义逆的解，该算子将最后一个效应设置为零。“Coefficients Trt Least Squares Means” 显示软件如何使用参数的解来计算处理均值。我们通过在最小二乘均值中使用 E 选项获得此列表。此列表验证了 \\(\\bar L_\\cdot\\) 用于最小二乘均值可估函数。请注意，“LSM Trt 0 alt” 对比标记为 “Non-est” —— 以 SAS 语言表示的不可估。回想一下，这是我们试图定义为 \\(\\eta+\\tau_i\\) 的估计，其中 \\(\\bar L_\\cdot=0\\)。GLIMMIX 估计表达式 \\(\\symbf{K}'=(\\symbf{X'X})^-(\\symbf{X'X})\\mathbf{K}'\\)。如果等式不成立，我们在列表中得到 “Non-est”.从具有多个可估函数和多重性调整的 ESTIMATE 语句中获得的估计出现在单独的表中。出现两列 \\(p\\) 值。第一个 “Pr > |t|” 是基于 21 个自由度的 \\(t\\) 分布的未调整 \\(p\\) 值。第二个 “Adj P” 使用 ADJUST= 选项中指定的多重性调整进行调整。请注意，三个 \\(\\symbf{K}\\) 矩阵定义的对比与 “Type III tests Fixed Effects” 列表中的 \\(F\\) 值产生了相同的结果。示例 3.4  (原书示例 3.3.4；二项响应的多地点四处理) 假设我们想对与前一个例子相同的四处理、多地点随机区组研究进行建模，但采用二项响应。相对于高斯情形，我们对模型所做的更改类似于两处理完全随机设计。分布和连接发生变化，线性预测器不变。因此模型为：线性预测器 \\(\\eta_{ij}=\\eta+\\tau_i+L_j\\)，其中 \\(\\tau_i:=0,1,2,3\\) 表示第 \\(\\) 种处理效应，\\(L_j:j=1,2,\\ldots,8\\) 表示第 \\(j\\) 个地点效应。分布：\\(y_{ij}\\thicksim \\text{Binomial}\\left(N_{ij},\\pi_{ij}\\right)\\)。在本例中，简单起见，对于每个处理-地点组合，\\(N_{ij} = 60\\)。连接：logit，\\(\\eta_{ij}=\\log\\biggl[\\pi_{ij}\\biggr/\\biggl(1-\\pi_{ij}\\biggr)\\biggr]\\)这里可以使用与示例 3.3 中为高斯响应定义的可估函数相同的函数。无需进一步讨论，只是提醒一下，所有可估函数都会得到模型尺度的估计，我们需要决定我们想要的数据尺度转换。以下是 GLIMMIX 程序中包含的高斯数据和适当的数据尺度转换的可估函数列表。注意，对于一些可估函数，没有适当的转换直接应用于 \\(\\symbf K' \\hat{\\symbf\\beta}\\)：最小二乘均值：逆连接。最小二乘均值对之差：几率比。ESTIMATE “LSM trt 0”：逆连接。ESTIMATE “LSM trt 0 alt”：无——它不可估，所以放弃。ESTIMATE “Trt 0 v Trt 1”：指数（即 \\(e^{\\symbf K' \\hat{\\symbf\\beta}}\\)）（或几率比）。多重 ESTIMATE 语句：有两个语句。第一个定义了处理差异，指数（或几率比转换）是合适的。第二个是两个或以上处理的均值，不适合直接转换，可根据最小二乘均值的逆连接变换对指定的估计 \\(\\hat\\pi_i\\) 进行平均。Contrast：无需转换为数据尺度。这些只计算 \\(F\\) 统计量。实现此模型的 GLIMMIX 程序是该程序使用 Data Set 3.2 中表示为 S_1 的二项响应变量。注意，我们可以使用 ADJUST= 选项来调整非高斯广义线性模型的多重性，就像我们对高斯线性模型所做的那样。除了在需要和适当的情况下添加 ILINK, ODDSRATIO 或 EXP 选项外，其他语句与高斯数据的程序一样。选定输出：广义逆的“末效应置零” (last-effect-set--zero) 约定适用于广义线性模型和高斯模型。这些是模型尺度的解：应用于它们的任何 \\(\\symbf K' \\hat{\\symbf\\beta}\\) 将是模型尺度的估计。最后，没有出现 \\(\\hat\\sigma^2\\)。二项分布的方差不是一个独立的参数，而是 \\(\\pi_{ij}(1-\\pi_{ij})\\)。这些是 MODEL 语句中 ODDSRATIO 选项产生的几率比估计。出现了两组估计，分别对应于模型中的 LOC 和 TRT 效应。所有这些都是第一列所示的水平与最后一个因子水平的几率比——这是获得 \\(\\symbf\\beta\\) 解的置零约定的结果。对地点的几率比可能不感兴趣或根本没有兴趣，但对处理的几率比显然是感兴趣的。标记为 “Estimate” 的列及其标准误给出了模型尺度估计 \\(\\hat{\\eta}+\\hat{\\tau}_i+\\hat{\\bar{L}}_\\cdot\\) 。标记为 “Mean” 和 “Standard Error Mean” 的列分别应用逆连接和 Delta 规则来获得 \\(\\hat\\pi_i\\) 及其标准误。标记为 “Estimate” 的列给出了 \\(\\hat\\tau_i-\\hat\\tau_{'}\\) 的模型尺度结果。应用 \\(e^{\\hat\\tau_i-\\hat\\tau_{'}}\\) 得到最后一列给出的几率比。所有未经多重性调整的估计都显示在一个表中。只有在请求估计可估函数时，逆连接结果才会出现，几率比 “Exponentiated estimate” 也仅在请求时才出现。所有其他估计仅以模型尺度的形式出现。多重性调整表中出现两个 \\(p\\) 值：比较的 (comparison-wise) “Pr > |t|” 和实验的 (experiment-wise) “Adj p”. 这些 \\(p\\) 值同样适用于模型尺度估计和数据尺度几率比 (“Exponentiated estimate”).与高斯数据一样，所有三个 \\(\\symbf K\\) 矩阵的对比产生与 “Type III Tests Fixed Effects” 相同的 \\(F\\) 值。示例 3.5  (原书示例 3.3.5；作为 2 × 2 析因的四处理多地点的回顾) 假设多地点 Data Set 3.2 的处理设计比简单的 0, 1, 2 和 3 处理更为复杂。具体地，假设处理具有以下析因结构ABij 表示第 \\(ij\\) 个 \\(× B\\) 处理组合，其中 \\(=0,1,j=0,1\\)。为了适应这种结构，我们将线性预测器中的 \\(\\eta+\\tau_i\\) 替换为 \\(\\eta_{ij}\\)，又可进一步分解为析因分量 \\(\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\)。因此，全线性预测器为\\[\\eta_{ijk}=\\eta_{ij}+L_k=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+L_k\\]将线性预测器的 \\(\\eta_{ijk}=\\eta_{ij}+L_k\\) 形式视为广义线性模型的单元格均值模型版本。事实上，对于高斯数据或任何使用恒等连接的 GLM，\\(\\eta_{ijk}=\\eta_{ij}+L_k=\\mu_{ijk}=\\mu_{ij}+L_k\\) 为单元格均值模型。回想一下，表 3.2 显示了析因处理设计的标准分析中感兴趣的可估函数。表 3.2 仅关注在 \\(\\eta_{ij}\\) 上定义的可估函数。对于多地点数据，在固定地点效应的情况下，我们也有地点效应。它们完全像示例 3.3 和示例 3.4 中那样进入可估函数。\n例如，根据表 3.2，\\(A_0\\) 的主效应“均值”的可估函数为 \\(\\overline{\\eta}_{0\\cdot}=\\eta+\\alpha_0+1/2\\sum_{j=0}^1\\beta_j+1/2\\sum_{j=0}^1\\left(\\alpha\\beta\\right)_{0j}\\)，\\(\\eta_{ij}\\) 部分，加上 \\(\\bar L_\\cdot\\) 保证可估性要求。因此，完全可估函数为 \\(\\eta+\\alpha_0+1/2\\sum_{j=0}^1\\beta_j+1/2\\sum_{j=0}^1\\left(\\alpha\\beta\\right)_{0j}+1/8\\sum_{k=1}^8L_k\\)。使用线性预测器的“单元格均值”形式，该可估函数为 \\(1/2\\sum_{j=0}^{1}\\eta_{0j}+1/8\\sum_{k=1}^{8}L_{k}=\\bar{\\eta}_{0\\cdot}+\\bar{L}_{\\cdot}\\)。请注意，该可估函数仅仅是示例 3.3 和示例 3.4 中的 \\(\\eta+1/2\\sum_{=0}^1\\tau_i+\\bar{L}_\\cdot\\)，只是用 \\(\\alpha_i+\\beta_j+\\left(\\alpha\\beta\\right)_{ij}\\) 替换了 \\(\\tau_i\\)。该分析的其他可估函数也遵循相同的思路。与示例 3.3 和示例 3.4 一样，\\(\\bar L_\\cdot\\) 出现在均值及其平均的可估函数中，但它通常在差异的可估函数中抵消。如前所述，值得强调的一点是，解释析因处理结构有一个逻辑顺序。你应该始终首先评估交互。借用 Nelder 的一句话，在存在不可忽略的交互作用的情况下，主效应是“无趣的”可估函数。请注意短语“不可忽略” (“non-negligible”) 而不是“不显著” (“non-significant”). 单词的选择很重要。它们不是同义词。统计上显著但几乎没有实际后果的交互作用确实可能出现。这些并不妨碍评估主效应。然而，一般来说，如果你评估主效应，则不应评估简单效应，反之亦然——至少不要在同一个报告中同时评估两种效应：在考虑给定数据集中给定响应变量的分析时，主效应分析和简单效应分析应视为互斥的。使用析因处理设计模型对 Data Set 3.2 中的高斯响应进行分析的 GLIMMIX 语句如下与示例 3.3 相同数据的高斯分析相比，B 替换了 CLASS 语句的 TRT，以及 |B（B *B 的 SAS 简写）替换了 MODEL 语句中的 TRT。出现两个 LSMEANS 语句。一个是主效应 和 B 的均值，另一个是处理组合的均值，表示为 *B。我们可以使用一个语句。事实上，我们可以写语句 LSMEANS |B / DIFF SLICEDIFF=(AB);。我们将这两格语句分开的原因是，LSMEANS *B / DIFF 不加区分地列出了所有可能的均值对，这在析因处理结构中通常不感兴趣。在标准析因分析中，*B 处理组合的比较意味着，当对它们感兴趣时，只关注简单效应——保持 B 的水平不变比较 的水平，反之亦然——而不是所有可能的配对。SLICEDIFF 将 *B 组合之间的差异仅限于简单效应，并相应地生成列表。LSMESTIMATE 允许我们根据最小二乘均值可估函数来定义估计，而不是根据模型效应明确地定义估计。例如，定义 \\(\\bar{\\eta}_{ij\\cdot}=1/8\\sum\\eta_{ijk}\\)，第 \\(\\) 个 AB 组合在 8 个地点上的平均可估函数，当用最小二乘法定义时，给定 B0 的简单效应的可估函数为 \\(\\bar{\\eta}_{00\\cdot}-\\bar{\\eta}_{10\\cdot}\\)。LSMESTIMATE 允许你以这种方式指定可估函数。如果我们改为使用 ESTIMATE 语句，则必须写出模型中所有效应的系数。在所有效应上明确定义的可估函数为\\[\\eta+\\alpha_0+\\bar{\\beta}_\\cdot+\\left(\\overline{\\alpha\\beta}\\right)_{0\\cdot}+\\bar{L}_\\cdot-\\left(\\eta+\\alpha_1+\\bar{\\beta}_\\cdot+\\left(\\overline{\\alpha\\beta}\\right)_{1\\cdot}+\\bar{L}_\\cdot\\right)\\\\=\\alpha_0+\\left(\\overline{\\alpha\\beta}\\right)_{0\\cdot}-\\left(\\alpha_1+\\left(\\overline{\\alpha\\beta}\\right)_{1\\cdot}\\right)=\\alpha_0-\\alpha_1+\\left(\\overline{\\alpha\\beta}\\right)_{0\\cdot}-\\left(\\overline{\\alpha\\beta}\\right)_{1\\cdot}\\]SLICEDIFF 和 LSMESTIMATE 仅在 GLIMMIX 程序中可用。对于 SAS PROC GLM, MIXED 或 GENMOD，可以通过 PROC PLM 访问这些选项，尽管不太方便。最后，E 选项出现在整个程序中。SAS 的内部逻辑处理地点效应以实现可估性。E 选项使我们能够看到 GLIMMIX 在“黑盒内”如何处理地点效应。选定输出：这些是 ESTIMATE 语句的结果。请注意，对于 A0 和 B0 的边际均值，我们需要给出处理相关效应 \\(\\eta,\\alpha_i,\\beta_j\\) 和 \\((\\alpha\\beta)_{ij}\\) 的所有系数，但程序的内部逻辑对地点效应进行平均——即添加 \\(\\bar L_\\cdot\\) —— 这是可估性必需的。为进行比较，LSMESTIMATE 给出了 A0 边际均值的如下结果出于篇幅考虑，此处未显示其他 LSMESTIMATE 系数。请注意，即使 LSMESTIMATE 语句只提到了模型效应 ，并将系数 1 和 0 分别分配给 A0 和 A1，但 GLIMMIX 从该语句中“理解”的系数（就模型效应而言）与明确写出所有系数的 ESTIMATE 语句相同。估计也相同。例如，对于 A0 的边际均值，我们得到这与之前展示的 ESTIMATE 列表相同。SLICEDIFF 输出如下：其他输出与在前面的示例中看到的类似。这里不展示并不是因篇幅所限，而是留作练习。最后需要注意，\\(×B\\) 交互作用检验的 \\(p\\) 值为 0.6661. 此外，在上面的 SLICEDIFF 列表中，简单效应的差异似乎很小。在实践中，我们需要知道响应变量是什么，并寻求主题专家对于何种程度的差异被认为是重要或可忽略不计的判断。但根据我们对问题的了解以及缺乏统计学上令人信服的交互作用证据，我们的分析应侧重于这些数据的主效应。3.3 节的关键思想：模型或连接尺度；数据或逆连接尺度；逆连接；Delta 规则；位置度量（又称均值）；差异度量；数据/模型尺度因素如何影响均值估计与位置估计","code":"proc glimmix data=CRD_2Trt_Ex;\n class Trt;\n model F/N=Trt / solution oddsratio;\n lsmeans Trt / Diff ilink oddsratio e;\n estimate 'LSM Trt 0' intercept 1 Trt 1 0/ilink;\n estimate 'LSM Trt 1' intercept 1 Trt 0 1/ilink;\n estimate 'overall mean' intercept 1 Trt 0.5 0.5 /ilink;\n estimate 'Overall Mean' intercept 2 Trt 1 1 / divisor=2 ilink;\n estimate 'Trt diff' Trt 1 -1 / oddsratio;\n estimate 'reverse diff' Trt -1 1 / exp;proc glimmix data=MultiLoc_4Trt;\n class loc trt;\n model Y=loc trt;\n lsmeans trt / diff e;\n estimate 'LSM Trt 0' intercept 8 Trt 8 0 0 0 Loc 1 1 1 1 1 1 1 1 / \n   divisor=8;\n estimate 'LSM Trt 0 alt' intercept 1 Trt 1 0 0 Loc 0 0 0 0 \n   0 0 0 0;\n estimate 'Trt 0 v Trt 1 diff' Trt 1 -1 0 0;\n estimate 'avg Trt 0+1' Intercept 8 Trt 4 4 0 0 Loc 1 1 1 1 1 \n   1 1 1 / divisor=8;\n estimate 'avg Trt 0_2_3' Intercept 24 Trt 8 0 8 8 Loc 3 3 3 3 \n   3 3 3 3 / divisor=24;\n estimate 'Trt 0 v Trt 1 diff' Trt 1 -1 0 0,\n          'Trt 2 v Trt 3 diff' Trt 0 0 1 -1,\n          'Trt 1 v Trt 2 diff' Trt 0 1 -1 0,\n          'Trt 1 v Trt 3 diff' Trt 0 1 0 -1,\n          'avg Trt 0+1 v avg Trt 2+3' Trt 1 1 -1 -1,\n          'trt 1 v avg Trt 0+1+2' Trt -1 3 -1 -1 / divisor= \n   1,1,1,1,2,3 adjust=sidak;\n contrast 'Type 3 Trt SS' Trt 1 -1 0 0,\n                          Trt 1 0 -1 0,\n                          Trt 1 0 0 -1;\n contrast 'Type 3 Trt Test' Trt 1 -1 0 0,\n                            Trt 0 1 -1 0,\n                            Trt 0 0 1 -1;\n contrast 'Type 3 Trt H_0' Trt -1 3 -1 -1,\n                           Trt 2 0 -1 -1,proc glimmix data=MultiLoc_4Trt;\n class loc trt;\n model S_1/N_Bin=loc trt / solution oddsratio;\n lsmeans trt / diff oddsratio e;\n estimate 'LSM Trt 0' intercept 8 Trt 8 0 0 0 Loc 1 1 1 1 1 1 1 1 / \n   divisor=8 ilink;\n *estimate 'LSM Trt 0 alt' intercept 1 Trt 1 0 0 Loc 0 0 0 0 \n   0 0 0 0;\n estimate 'Trt 0 v Trt 1 diff' Trt 1 -1 0 0 / exp;\n estimate 'avg Trt 0+1' Intercept 8 Trt 4 4 0 0 Loc 1 1 1 1 1 \n   1 1 1 / divisor=8;\n estimate 'avg Trt 0_1_2' Intercept 24 Trt 8 0 8 8 Loc 3 3 3 3 \n   3 3 3 3 / divisor=24;\n estimate 'Trt 0 v Trt 1 diff' Trt 1 -1 0 0,\n          'Trt 2 v Trt 3 diff' Trt 0 0 1 -1,\n          'Trt 1 v Trt 2 diff' Trt 0 1 -1 0,\n          'Trt 1 v Trt 3 diff' Trt 0 1 0 -1 / exp adjust=sidak;\n estimate 'avg Trt 0+1 v avg Trt 2+3' Trt 1 1 -1 -1,\n          'trt 1 v avg Trt 0+1+2' Trt -1 3 -1 -1 / divisor=2,3;\n contrast 'Type 3 Trt SS' Trt 1 -1 0 0,\n                          Trt 1 0 -1 0,\n                          Trt 1 0 0 -1;\n contrast 'Type 3 Trt Test' Trt 1 -1 0 0,\n                            Trt 0 1 -1 0,\n                            Trt 0 0 1 -1;\n contrast 'Type 3 Trt H_0' Trt -1 3 -1 -1,\n                           Trt 2 0 -1 -1,\n                           Trt 0 0 1 -1;proc glimmix data=MultiLoc_4Trt;\n class loc a b;\n model y=a|b loc;\n lsmeans a b / diff e;\n lsmeans a*b / slicediff=(a b);\n lsmestimate a 'a0 marg mean' 1 0 / e;\n lsmestimate b 'b0 marg mean' 1 0 / e;\n lsmestimate a*b 'a|b0 simple effect' 1 0 -1 0,\n                 'b|a0 simple effect' 1 -1 0 0 / e;\n estimate 'a0 marginal mean' intercept 2 a 2 0 b 1 1 a*b 1 1 0 0,\n          'b0 marginal mean' intercept 2 a 1 1 b 2 0 a*b 1 0 1 0,\n          'a|b0 simple effect' a 1 -1 a*b 1 0 -1 0,\n          'b|a0 simple effect' b 1 -1 a*b 1 -1 0 0 / e \n          divisor=2,2,1,1;"},{"path":"搭建舞台.html","id":"搭建舞台","chapter":"►搭建舞台","heading":"►搭建舞台","text":"","code":""},{"path":"搭建舞台.html","id":"sec3-4","chapter":"►搭建舞台","heading":"3.4 问题 2：推断空间","text":"在 3.2 节中，我们介绍了广义推断和狭义推断这两个术语。这种区别只存在于混合模型，即具有线性预测器 \\(\\symbf{X\\beta}+\\symbf{Zb}\\) 的模型。对于这些模型，我们可以仅基于可估函数（\\(\\symbf{K'\\beta}\\)）或基于可预测函数（\\(\\symbf{K'\\beta}+\\symbf{M'b}\\)）进行推断。注意，可估函数可视为 \\(\\symbf M=\\symbf 0\\) 的可预测函数。在可估函数中，估计的所有信息都来自固定效应，而不确定性则来自所有随机效应。换句话说，可估函数的不确定性来自所有随机效应的综合贡献。在可预测函数中，\\(\\symbf{M'b}\\) 的非零分量消除了这些效应的不确定性。因此，它们将推断范围从由随机效应所代表的整个总体缩小到仅仅包含在可预测函数中的那些效应。只针对可估函数 \\(\\symbf{K'\\beta}\\) 进行的推断称为广义空间推断 (broad space inference). 而针对可预测函数 \\(\\symbf{K'\\beta}+\\symbf{M'b}\\) 进行的推断称为狭义空间推断 (narrow space inference).为了说明广义推断和狭义推断之间的区别，我们在本节中考虑两个例子。第一个回顾了 3.3 节最后三个示例中使用的四处理多地点数据。第二个是多批次回归，其中批次效应是随机的。我们首先回顾一下基本思想。","code":""},{"path":"搭建舞台.html","id":"sec3-4-1","chapter":"►搭建舞台","heading":"3.4.1 广义推断","text":"在混合模型中，随机效应的水平代表了对其进行抽样的总体。这些可以是从一个州或地区抽样的诊所，从一个地区抽样的学校，代表更大劳动力总体的单个工人，代表农业地区的农场，等等。理想情况下，抽样是随机进行的，使得总体中的所有成员都有平等且独立的机会被选中。随机效应的水平应该是可交换的，这意味着，例如，在我们的八地点示例中，我们也可以使用任意八个随机选择的地点——我们在这项研究中碰巧观察到的这八个地点并没有什么特别之处。在实践中，随机效应水平是通过不太理想的抽样方式纳入研究的。然而，如果这些水平——诊所、学校、工人、农场等——能够合理地代表总体，那么随机模型效应背后的主要思想——它们的定义性质——仍然成立。这些性质包括：随机效应水平表示目标总体。随机效应水平具有概率分布。在随机效应的两个定义性质中隐含着广义推断。非正式地说，广义推断指的是适用于随机模型效应所代表的整个总体的点估计、区间估计和假设检验。正式地说，广义推断是指仅由可估函数 \\(\\symbf{K'\\beta}\\) 定义的基于混合模型的估计或假设检验。如果 \\(\\symbf M\\) 中存在非零系数，则估计或检验由可预测函数 \\(\\symbf{K'\\beta}+\\symbf{M'b}\\) 定义，此时我们不再有广义推断。广义推断通常会让数据分析师感到困惑，因为乍一看，所有具有固定效应模型的推断都必须基于 \\(\\symbf{K'\\beta}\\)，因此所有具有固定效应模型的推断必为广义推断。果真如此吗？现实情况更为微妙。一路伴随我们的多地点研究提供了一个完美的例子。“地点效应是固定的还是随机的？”这个问题在统计学中有着悠久的争议历史。这个两难困境的核心是“在将地点效应定义为固定或随机之后，我们如何合理地将推断广泛地应用于实际中？”示例 3.6  (原书示例 3.4.1；四处理多地点、高斯数据、随机地点效应) 示例 3.3 引入 Data Set 3.2，其中模型假设固定地点效应。我们这样做时有一个警告，即将地点效应视为固定的是值得怀疑的，应该重新考虑，这是本例所要做的。我们知道，如果地点代表一个总体，并且观测地点构成该总体的代表性样本，那么选用固定地点效应的理由就不充分。此外，如果地点是通过随机抽样选择的，并且可以包含任何 8 个地点，那么根据定义，地点效应是随机的。在本例中，假设地点确实是通过随机抽样选择的。所得模型为线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+L_j\\)，其中 \\(\\tau_i:=0,1,2,3\\) 表示第 \\(\\) 个处理效应，\\(L_j:j=1,2,\\ldots,8\\) 表示第 \\(j\\) 个地点效应。分布：\n\\(y_{ij}\\mid L_j\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)\n\\(L_j\\text{ iid }N\\left(0,\\sigma_L^2\\right)\\)\n\\(y_{ij}\\mid L_j\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)\\(L_j\\text{ iid }N\\left(0,\\sigma_L^2\\right)\\)连接：恒等 \\(\\eta_{ij}=\\mu_{ij}\\)请注意这一变化对处理均值可估函数的影响。在固定地点效应模型中 \\(\\eta+\\tau_i\\) 是不可估的。相反，需要由 \\(E(\\bar{y}_{\\cdot})=\\eta+\\tau_i+\\bar{L}_\\cdot\\) 定义的函数来满足可估性要求。在随机地点效应模型中，\\(E(\\bar{y}_{\\cdot})=\\eta+\\tau_i\\)，因为 \\(L_j\\) 现在是期望等于零的随机变量。因此，\\(\\eta+\\tau_i\\) 是随机地点效应模型中处理均值的可估函数。从而引出两个问题：在随机地点效应模型中，处理均值的最小二乘估计与在固定地点效应情况下的估计有何不同？由固定地点效应模型最小二乘均值与由其相应系数定义的可预测函数相比会是什么情况？换言之，固定地点效应模型下的最小二乘均值表示为 \\(\\eta+\\tau_i+\\bar L_\\cdot\\)，而它在随机地点效应模型下是一个可预测函数。它们相比如何？随机地点效应模型的 GLIMMIX 语句为：请注意 ESTIMATE 语句。与 \\(\\symbf K\\) 矩阵相关的系数，用于 \\(\\eta\\) 和 \\(\\tau_i\\)，出现在垂直条 (|) 之前；与 \\(\\symbf M\\) 矩阵相关的系数，用于 \\(L_j\\)，出现在垂直条之后。固定效应和随机效应的解为将这些解与示例 3.3 中的固定地点效应模型进行比较，请注意，在模型的固定部分，广义逆的末效应零约定成立，但地点效应并非如此。在第 5 章中，我们将开发随机效应的最佳线性无偏预测的正式理论。对于本例的地点效应，估计如下。使用地点 1 进行说明。将地点 1 效应的未调整估计定义为地点 1 均值与总均值之差 \\(\\bar{y}_{\\cdot}-\\bar{y}_{\\cdot\\cdot}=26.075-25.8875=0.1875\\)。调整的随机效应估计，也称为最佳线性无偏预测 (best linear unbiased predictor, BLUP).\\[\\hat{L}_1=E\\Big(L_j\\mid\\symbf{y}\\Big)=E\\Big(L_j\\Big)+Cov\\Big(L_j,\\bar{y}_{\\cdot j}\\Big)\\Big[Var\\Big(\\bar{y}_{\\cdot j}\\Big)\\Big]^{-1}\\Big(\\bar{y}_{\\cdot j}-\\bar{y}_{\\cdot\\cdot}\\Big)\\]注意到 \\(E(L_j)=0\\)，使用第 5 章中介绍的方法，可写出地点 1 的 BLUP 为\\[\\hat{L}_j=0+\\hat{\\sigma}_L^2\\left(\\frac{\\sigma^2+4\\sigma_L^2}4\\right)^{-1}\\left(\\bar{y}_{\\cdot j}-\\bar{y}_{\\cdot\\cdot}\\right)\\]方差分量估计为将这些代入，\\(\\hat{L}_j=0+1.7352\\biggl(\\frac{2.877+4\\times1.7352}{4}\\biggr)^{-1}\\bigl(26.075-25.8875\\bigr)=0.1326\\)BLUP 也称为收缩估计 (shrinkage estimators)，因为它们使用了有关分布的信息（在这里指地点效应）来减少极值——本质上就是将估计“收缩”至零。收缩的程度取决于方差分量。如果 \\(\\sigma^2_L\\) 较小，收缩程度就较大，因为地点效应的分布在零附近非常紧密。如果 \\(\\sigma^2_L\\) 较大，收缩程度就较小，因为地点效应的分布相对分散。现在回到处理最小二乘均值，它们的估计和标准误是将这些与固定地点效应固定的最小二乘均值进行比较，估计是相同的，但标准误不同：此处为 0.7593，而固定地点效应模型为 0.5997. 为什么会出现这种差异？对于这里考虑的两个模型，标准误为 \\(Var(\\bar y_{\\cdot})\\) 的平方根估计。对于这两个模型，\\[Var\\left(\\bar{y}_{\\cdot}\\right)=Var\\left\\{1/8\\sum_j\\left(\\eta+\\tau_i+L_j+w_{ij}\\right)\\right\\}\\]其中 \\(w_{ij}\\) 表示地点内观察单元间 (units observation within locations) 的随机变异（超出处理效应）。在固定地点效应模型中，只有 \\(w_{ij}\\) 是随机变量。因此\\[Var\\left(\\bar{y}_{\\cdot}\\right)=Var\\left\\{1/8\\sum_jw_{ij}\\right\\}=\\left(1/8\\right)^2\\sum_jVar\\left(w_{ij}\\right)=\\sigma^2/8\\]在随机地点效应模型中，\\(L_j\\) 也是一个随机变量，因此\\[Var\\left(\\bar{y}_{\\cdot}\\right)=Var\\left\\{1/8\\sum_j\\left(L_j+w_{ij}\\right)\\right\\}=\\left(1/8\\right)^2\\left[\\sum_jVar\\left(L_j\\right)+\\sum_jVar\\left(w_{ij}\\right)\\right]=\\left(\\sigma^2+\\sigma_L^2\\right)/8\\]现考虑 ESTIMATE 语句的结果。请注意，处理 0 的 BLUP 的标准误使用与固定地点效应最小二乘均值相同的系数，其标准误与我们使用固定地点效应模型获得的最小二乘均值的标准误完全相同。这给了我们一个解读的窗口。设想，我们希望根据我们在该数据集中观察到的八个地点来估计处理 0 的均值，并用它来预测在其他地点（而不是研究中的八个地点）应用处理 0 会发生什么。显然，对预期响应的最佳估计是 24.95——所有估计都一致给出该数字。然而，如果我们想要估计的置信上下限，我们需要决定是使用 0.5997 还是 0.7593 作为置信区间的标准误。答案取决于我们在试图预测处理 0 将如何确定时所预期的不确定性来源。不确定性是否仅仅来自于每个地点内部观察单元之间的变异，还是说除了观察单元变异之外的地点间变异也对此有所贡献呢？随机地点效应最小二乘均值的标准误包括两个不确定性来源。随机地点效应 BLUP 仅包括地点内观察单元之间的方差。实际上，BLUP 将适用于研究中可能包括的整个地点总体的陈述的推断范围限制为仅处理 0 BLUP 的可预测函数中具有非零系数的地点9。随机地点效应 BLUP 和固定地点效应最小二乘均值共享相同的系数，这一事实提供了对将地点效应视为固定的隐含结果的深入了解。实际上，将地点效应定义为固定效应缩小了推断范围：处理均值的置信区间仅适用于研究中观察到的地点。它们不适用于——也无法解释为适用于——本研究的观测地点所代表的地点总体。随机地点效应模型的最小二乘均值是一个广义推断空间估计。最佳线性无偏预测（以及固定地点效应模型的最小二乘均值）是一狭义推断空间估计。前者广泛适用于整个总体，后者仅适用于观测的 8 个地点。本示例说明了推断空间问题的一个重要方面。下一个示例说明了狭义推断空间的一个重要应用——特定个体推断。","code":"proc glimmix data=MultiLoc_4Trt;\n class loc trt;\n model Y=trt /solution;\n Random Loc / solution;\n lsmeans trt / diff e;\n estimate 'BLUP Trt 0' intercept 8 Trt 8 0 0 0 | Loc 1 1 1 1 1 \n  1 1 1 / divisor=8;\n estimate 'LSM Trt 0' intercept 1 Trt 1 0 0 | Loc 0 0 0 0 0 0 0 0;"},{"path":"搭建舞台.html","id":"sec3-4-2","chapter":"►搭建舞台","heading":"3.4.2 狭义推断","text":"上一节关注广义推断。理解什么是广义推断，什么不是广义推断，是处理当代线性模型的核心。本节则关注狭义推断——通常称为特定个体推断。狭义推断在许多领域都有其根源。在动物遗传学中，交配试验是为了获得公畜和母畜之间方差的估计。这些估计用于确定具有选择育种潜力的性状——对疾病的易感性，增加产奶量等。从这个角度来看，公畜和母畜的效应显然是随机的。然而，在同一试验中，必须鉴别出具有特殊育种或商业潜力的公畜。在对固定效应和随机效应的经典理解中，这似乎是试图两者兼顾。一个效应要么被纳入模型来估计方差（随机效应），要么被纳入模型来估计或比较地点的度量（固定效应）。而动物育种者希望根据同一效应估计方差和地点的度量！这就是推动 Henderson 开发最佳线性无偏预测 (BLUP) 的问题，这是混合模型文献中的开创性工作 (Henderson, 1950, 1963, 1975, 1984).在医学中，临床试验通常涉及患者的随机样本。这些试验的一个共同目标是确定患者对药物剂量的响应。对剂量响应的广义推断（也称为总体平均推断）是目标之一，但跟踪单个患者的响应通常同样重要。单个患者反应——也称为特定个体 (subject-specific) 响应——是一种狭义推断形式。它是由固定效应（剂量）和随机效应（患者）的线性组合定义的。还有其他的例子，在多地点研究中，特定地点的处理效应可用于识别处理效应中意外的局部异常。近年来，教师效能评估的“增值” (“value-added”) 模型受到广泛关注，这些模型本质上将教师效应定义为随机的，并使用特定教师的可预测函数来估计。示例 3.7 说明了狭义推断的一个常见示例。示例 3.7  (原书示例 3.4.2；随机系数回归模型) 从易腐产品的生产过程中随机抽取了四个批次。产品在建议的条件下储存，并在指定的储存时间测量产品质量特性，下文记为 \\(Y\\)。假设过去此类研究的经验允许我们假定 \\(Y\\) 随时间线性恶化，并具有近似高斯分布。数据在 SAS Data Program Library 显示为 Data Set 3.3.在第 2 章中，我们为这类研究开发了一个合理的模型——随机系数回归模型 (random coefficient regression model). 对于本研究，模型为线性预测器：\\(\\eta_{ij}=\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_j\\)，其中，\\(\\beta_0\\) 和 \\(\\beta_1\\) 表示固定的截距和斜率系数，\\(b_{0i}\\) 和 \\(b_{1i}\\) 分别表示第 \\(\\) 个批次对截距和斜率的随机效应，\\(X_j\\) 表示第 \\(j\\) 个时间。分布：有两个，一个用于 \\(y_{ij}\\)，在第 \\(j\\) 个时间对第 \\(\\) 批的观测，另一个用于 \\(b_{0i}\\) 和 \\(b_{1i}\\) 的联合分布：\n\\(y_{ij}\\mid b_{0i,}b_{1i}\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)\n\\(\\begin{bmatrix}b_{0i}\\\\b_{1i}\\end{bmatrix}\\thicksim N\\left(\\begin{bmatrix}0\\\\0\\end{bmatrix},\\begin{bmatrix}\\sigma_0^2&\\sigma_{01}\\\\\\sigma_{01}&\\sigma_1^2\\end{bmatrix}\\right)\\)\n\\(y_{ij}\\mid b_{0i,}b_{1i}\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)\\(\\begin{bmatrix}b_{0i}\\\\b_{1i}\\end{bmatrix}\\thicksim N\\left(\\begin{bmatrix}0\\\\0\\end{bmatrix},\\begin{bmatrix}\\sigma_0^2&\\sigma_{01}\\\\\\sigma_{01}&\\sigma_1^2\\end{bmatrix}\\right)\\)连接函数：恒等 \\(\\eta_{ij}=\\mu_{ij}\\)关于推断空间，有两组估计需要关注。第一组仅涉及固定效应：截距 \\(\\beta_0\\)、斜率 \\(\\beta_1\\) 和时间 T 时的预期响应 \\(\\beta_0+\\beta_1T\\)。截距可解释为产品最开始存储时（时间 0）产品质量的度量。斜率度量了产品的恶化率。在随机系数语言中，这些称为总体平均 (population-averaged, PA) 估计。请注意，它们完全取决于固定效应，都是 \\(\\symbf K'\\symbf\\beta\\) 的形式。因此，PA 估计是广义推断的一种形式。另一组涉及可预测函数 \\(\\beta_0+b_{0i},\\beta_1+b_{1i}\\) 和 \\({\\beta}_{0}+b_{0i}+\\left({\\beta}_{1}+b_{1i}\\right)T\\)。这些称为个体特定 (subject-specific, SS) “估计”——第 \\(\\) 批的截距、斜率和时间 T 时的预期响应。SS “估计”实际上是预测而非估计。它们都是 \\(\\symbf K'\\symbf\\beta+\\symbf M'\\symbf b\\) 的形式。SS 估计是 BLUPs，因此是狭义推断的一种形式。用于估计 Data Set 3.3 随机系数模型参数的 GLIMMIX 语句如下这些语句在第 2 章附录 B 中进行了讨论。请注意，变量 X 表示存储时间。GCORR 选项允许我们检查估计的 \\(\\symbf G\\) 矩阵：随机截距和斜率效应的方差-协方差阵。继续行文前，查看该矩阵有助于我们的理解。SAS 日志中出现警告：“NOTE: Estimated G matrix positive definite.” 查看第一个表中所示的协方差参数估计，没有什么突出的，但查看 \\(\\symbf G\\) 相关矩阵会发现我们得到了无意义的估计。这表明我们对随机截距系数和斜率系数的协方差进行了过度建模。我们可以推测，随机截距和斜率之间不存在有意义的相关性，或者我们用于估计协方差的批次数量太少。不管是何种情况，我们都需要通过从 \\(Var\\begin{bmatrix}b_{0i}\\\\b_{1i}\\end{bmatrix}\\) 中删除 \\(\\sigma_{01}\\) 来简化模型。获得 PA 和 SS 估计的修订的 RANDOM 语句和 ESTIMATE 语句如下所示。在这个说明中，我们使用时间 \\(T=10\\) 来展示如何在给定时间获得预期响应。在实践中，我们可以将 \\(T\\) 设为研究感兴趣的任何值从 RANDOM 语句中删除 TYPE=UN 选项会从模型中删除 \\(\\sigma_{01}\\)。带 'PA' 的 ESTIMATE 语句仅针对固定效应进行定义。带 'SS' 的语法使用 RANDOM 语句的结构来帮助定义可预测函数。考虑第一个语句 'SS intercept batch 1'。垂直条之前的 INTERCEPT 1 识别固定效应 \\(\\beta_0\\)；垂直条之后的 INTERCEPT 1 识别随机效应 \\(b_{0i}\\)；SUBJECT 1 0 0 0指回其中 SUBJECT 被定义为 BATCH 的 RANDOM 语句。根据这些指定，我们有 \\(\\beta_0+b_{01}\\)，即对于批次 1 截距的特定批次 (SS) BLUP。在 'SS intercept batch 2' 中，除 SUBJECT 0 1 0 0 之外，其他都是相同的，因此在第二个批次上定义了可预测函数，即 \\(\\beta_0+b_{02}\\)。有关特定批次斜率预测的语句出现在单个 ESTMATE 语句中。请注意用于识别与该语句每一行相关的个体的语法。这些预测定义为一个组，并进行了多重性调整，因为检验每个 SS 斜率为 0 的原假设可能是感兴趣的。如果是这样，我们需要控制实验的 类错误。最后两个语句展示了如何定义涉及 SS 预测的差异和均值。在第一个语句中，垂直条之前没有任何内容，因此此可预测函数不涉及固定效应，即 \\(\\symbf k =\\symbf 0\\)。INTERPT 1 X 10 出现在垂直条之后。这定义了 \\(b_{0i}+b_{1i}×10\\)。SUBJECT 1–1 0 0 表示将第 1 批中的所有系数乘以 1，将第 2 批中的全部系数乘以 –1. 结果为 \\(\\mathbf{m'b}=b_{01}-b_{02}+\\left(b_{11}-b_{12}\\right)10\\)，即时间 \\(T=10\\) 时第 1 批和第 2 批 SS 预测之差。固定效应未出现在本语句中，因为固定效应分量 \\(\\beta_0+\\beta_1×10\\) 在完整的差异 SS 语句中抵消了：\\(\\beta_0+b_{01}+\\left(\\beta_1+b_{11}\\right)10-\\left[\\beta_0+b_{02}+\\left(\\beta_1+b_{12}\\right)10\\right]\\)。批次 1 和 2 的 SS 预测均值为 \\(1/2\\{[\\beta_0+b_{01}+(\\beta_1+b_{11})10]+[\\beta_0+b_{02}+(\\beta_1+b_{12})10]\\}=\\beta_0+\\beta_1\\times10+1/2\\bigg[b_{01}+b_{02}+(b_{11}+b_{12})10]\\)。ESTIMATE 语句实现的正如其字面含义：\\(\\symbf k'\\symbf\\beta+\\symbf m'\\symbf b=\\left\\{2\\beta_0+\\beta_1\\times20+\\left[b_{01}+b_{02}+\\left(b_{11}+b_{12}\\right)10\\right]\\right\\}/2\\)。列表输出为给出的估计/预测是直接应用上述可估和可预测函数的结果。与之前的例子不同，这里的标准误并非根据直观的公式来计算。在第 6 章中，我们将开发计算这些估计和预测的标准误所需的理论。请注意，特定批次斜率的标准误尤其小，这不是一个错误，而恰恰反映了该特定的可预测函数消除了大量的不确定性10。3.4 节的关键思想：广义推断；狭义推断；最佳线性无偏预测 (BLUP)；总体平均推断；特定个体推断","code":"proc glimmix data=multi_batch_1;\n class batch;\n model y = X / solution;\n random intercept X / solution subject=batch type=un gcorr;proc glimmix data=multi_batch_1;\n class batch;\n model y = X / solution;\n random intercept X / solution subject=batch;\n estimate 'PA intercept' intercept 1;\n estimate 'PA slope' X 1;\n estimate 'PA y-hat at X=10' intercept 1 x 10;\n estimate 'SS intercept for batch 1' intercept 1 | intercept 1 / \n  subject 1 0 0 0;\n estimate 'SS intercept for batch 2' intercept 1 | intercept 1 / \n  subject 0 1 0 0;\n estimate 'SS slope for batch 1' x 1 | x 1,\n 'SS slope for batch 2' x 1 | x 1,\n 'SS slope for batch 3' x 1 | x 1,\n 'SS slope for batch 3' x 1 | x 1 / subject 1 0 0 0, \n 0 1 0 0, 0 0 1 0, 0 0 0 1 adjust=bonferroni;\n estimate 'SS y-hat @ X=10, batch 1 ' intercept 1 x 10 | \n  intercept 1 x 10\n  / subject 1 0 0 0;\n estimate 'SS y-hat @ X=10, batch 2 ' intercept 1 x 10 | \n  intercept 1 x 10\n  / subject 0 1 0 0;\n estimate 'SS diff @ x=10, batch 1 v 2' | intercept 1 x 10 / \n  subject 1 -1 0 0;\n estimate 'SS y-hat @ x=10, avg batch 1+2' intercept 2 x 20 |\n  intercept 1 x 10\n  / subject 1 1 0 0 divisor=2;"},{"path":"搭建舞台.html","id":"sec3-5","chapter":"►搭建舞台","heading":"3.5 问题 3：条件和边际模型","text":"在 3.3 节中，我们考虑了非高斯模型中出现的模型/数据尺度问题。在 3.4 节中，我们考虑了在模型中添加随机效应时出现的推断空间问题。当我们有非高斯响应和随机模型效应时会发生什么？除模型/数据尺度和推断空间问题之外，我们还有第三个问题：我们现在可以定义两种不同类型的模型——条件模型和边际模型。对于非高斯数据，它们是不等价的；他们估计的对象不一样。更糟糕的是，即使使用被认为不受 GLMM 问题影响的方法论，条件-边际问题也隐含于多区组和集群非高斯数据中。用拳击手 Joe Louis 的话来说就是“你跑得了但躲不了” (“can run can’t hide”)。为了说明这一点，我们继续以本章中一直使用的四处理多地点研究为例。示例 3.8  (原书示例 3.5.1；具有随机地点效应的二项多地点) 如示例 3.6 一样假定随机地点效应。但假设我们对每个地点的每种处理都有 \\(N=100\\) 个二项响应观测，而不是高斯数据。也就是说，对于处理 \\(＝1,2,3,4\\) 以及地点 \\(j＝1,2,\\ldots,8\\)，\\(y_{ij}\\mid L_j\\sim \\text{Binomial}\\left(100,\\pi_{ij}\\right)\\)。","code":""},{"path":"搭建舞台.html","id":"sec3-5-1","chapter":"►搭建舞台","heading":"3.5.1 正态近似","text":"乍一看，这些数据似乎是正态近似的候选者。即使概率 \\(\\pi_{ij}\\) 接近 0（如 0.05）或接近 1（如 0.95），当 \\(N=100\\) 时，正态能很好地近似于二项，这一点可以通过模拟轻松证明——这是统计入门课程中的常见活动。对于此类数据，一种方法——可以说是最常见的方法——是将高斯模型拟合到样本比例 \\(p_{ij}=y_{ij}/n_{ij}\\)。这里，所有 \\(n_{ij}=100\\)。正式地，模型为线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+l_j\\)分布：\n数据：\\(p_{ij}\\mid L_j\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)\n地点效应：\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)\n数据：\\(p_{ij}\\mid L_j\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)地点效应：\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)连接：恒等 \\(\\eta_{ij}=\\mu_{ij}\\)在这种方法中，估计值 \\(\\hat\\mu_{ij}\\) 通常解释为 \\(\\hat\\pi_{ij}\\) 的估计。关注点可能在于处理最小二乘均值，该均值将分别解释为 \\(\\hat\\pi_{0},\\hat\\pi_{1},\\hat\\pi_{2},\\hat\\pi_{3}\\)，即每种处理产生有利结果的概率。该模型本质上与前两节讨论的此类设计的高斯模型完全相同。我们已经看到，无论是固定地点效应版本还是随机地点效应版本的这些模型，在对处理差异进行广义推断时得出的结果是一致的。事实上，两者都等价于 ANOVA 的结果。这就是为什么那些认为 GLMM 让他们感到不安的数据分析师相信，当他们对样本比例进行 ANOVA 时，或者在方差稳定变换（例如反正弦平方根 \\(\\sin^{-1}(\\sqrt{p_{ij}})\\) ）上进行 ANOVA 时，他们可以避免 GLMM 问题。然而，这正是“你跑得了但躲不了”的体现。让我们看看这是为什么。对于高斯多区组模型，使用 3.3 节和 3.4 节所示的相同方法，可得到如下的相关结果：这些结果存在明显的问题。值得注意的是，如果处理均值确实可以解释为相应的 \\(\\pi_i\\) 的估计，那么它们的方差估计，以及它们的标准误，不应该表现出对二项方差 \\(\\pi_i(1−\\pi_i)\\) 的某种依赖性吗？我们暂时搁置这些问题，但我们稍后会回来讨论它们。","code":""},{"path":"搭建舞台.html","id":"sec3-5-2","chapter":"►搭建舞台","heading":"3.5.2 二项 GLMM","text":"正态近似与真正的 GLMM 相比如何？对于这些数据，广义线性混合模型可描述为线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+l_j\\)分布：\n数据：\\(p_{ij}\\mid L_j\\sim \\text{Bnomial}\\left(N_{ij},\\pi_{ij}\\right)\\)\n地点效应：\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)\n数据：\\(p_{ij}\\mid L_j\\sim \\text{Bnomial}\\left(N_{ij},\\pi_{ij}\\right)\\)地点效应：\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)连接：logit，\\(\\eta_{ij}=\\log[\\pi_{ij}/(1-\\pi_{ij})]\\)请注意，除了数据的分布（现在明确为二项式）和连接函数外，该模型与正态近似完全相同。此模型的 GLIMMIX 程序为选定结果：我们看到这些结果与正态近似之间存在一些差异。首先，只有一个方差分量估计：\\(\\hat\\sigma_L^2= 0.946\\)。这可以解释为不同地点间对数几率的变异。不存在残差方差，因为在处理地点组合内，观测具有二项分布。其方差由 \\(\\pi_{ij}(1-\\pi_{ij})\\) 决定。一旦我们有了 \\(\\pi_{ij}\\) 的估计，那么根据定义，我们就有了方差的估计。不需要方差分量的进一步估计。\\(F\\) 值为 57.99，而正态近似相应值为 40.01. 这些不同并不奇怪。然而，我们可能会问 GLMM 具有更大的 \\(F\\) 值这一事实是否有什么原因（答案是肯定的——我们稍后会探讨这一点）。四种处理的概率估计具有不同的标准误，这反映了二项的均值-方差关系。正如预期的那样，随着概率估计接近 0.5，标准误会增加。提问：如果我们的概率估计大于 0.5，当 \\(\\hat\\pi_i\\) 接近 1 时，它们的标准误会如何变化？评论：读者会注意到，上表中的数据尺度标准误不能直接通过对 \\(\\hat\\pi_i\\) 应用方差函数来获得。部分原因是方差函数适用于 \\(\\hat\\pi_i\\)，而 \\(\\hat\\pi_i\\) 是在地点上进行平均的，并且是在应用逆连接之前进行平均的。在确定标准误时还有其他微妙之处。我们将在第 6 章中考虑标准误计算的“大局观”。最后，将 GLMM 估计的概率与正态近似的概率进行比较，我们得到有两点很突出。首先，概率是不同的——这并不奇怪；正态近似就是一个近似。其次，GLMM 估计的概率都小于正态近似的相应概率。这有影响吗？如果有，怎么办？进一步探讨这个问题，如果我们取每个处理的样本比例的算术平均，它们等于正态近似的估计。这是否意味着 GLMM 的估计是错误的？如果不是，即如果事实上 GLMM 提供了更好的 \\(\\hat\\pi_i\\) 估计值，那么我们需要一个令人信服的解释来消除这种认知失调。我们习惯于算术平均是最佳线性无偏估计，至少对于均衡数据是如此。","code":"proc glimmix data=MultiLoc_4Trt;\n class loc trt;\n model S_1/N_Bin=trt;\n random intercept / subject=loc;\n lsmeans trt / ilink;"},{"path":"搭建舞台.html","id":"sec3-5-3","chapter":"►搭建舞台","heading":"3.5.3 条件和边际分布","text":"为了理解正态近似和 GLMM 结果之间的差异，我们需要回到建模第一原则，并将其仔细应用于本例。回想第 1 章的第一句话：统计模型是对产生观测数据的合理过程的数学和概率描述。示例 3.8 中的正态近似和 GLMM 都假定了如下过程。对于第 \\(j\\) 个地点的第 \\(\\) 种处理，观察到 \\(y_{ij}\\) 次成功。成功次数具有二项分布：正式说 \\(y_{ij}\\mid L_j\\thicksim\\text{Binomial}\\left(n_{ij},\\pi_{ij}\\right)\\)。在我们的例子中，所有 \\(n_{ij}=100\\)，但是关于 \\(y_{ij}\\mid L_j\\) 的分布假定将适用于任何 \\(n_{ij}\\ge 1\\)。请注意，概率陈述是指在给定地点的条件下观察到的成功次数。条件这个词很重要。这意味着 \\(\\pi_{ij}\\) 本身就像一个随机变量。具体地\\[\\pi_{ij}=1/\\left[1+e^{-\\left(\\eta+\\tau_i+L_j\\right)}\\right]\\]这意味着 \\(\\pi_{ij}\\) 因随机地点效应 \\(L_j\\) 而异，我们假定其为 \\(\\operatorname{iid}N\\left(0,\\sigma_L^2\\right)\\)。通过将 \\(\\pi_{ij}\\) 的分布重写为 \\(\\operatorname{logit}\\left(\\pi_{ij}\\right)\\mid L_j\\thicksim N\\left(\\eta+\\tau_i,\\sigma_L^2\\right)\\)，有助于形象地理解 \\(\\pi_{ij}\\) 在 \\(L_j\\) 条件下的分布。一般来说，混合模型描述了一个具有两个随机元素的过程：\\(\\symbf y\\mid \\symbf b\\) 的分布，以随机模型效应为条件的观测；以及 \\(\\symbf b\\)，随机模型效应。然而，我们无法彼此孤立地直接观察到这两个随机过程中的任何一个。我们观察到的唯一随机变量是 \\(\\symbf y\\)，即响应变量。对于非高斯数据，条件分布 \\(\\symbf y\\mid \\symbf b\\) 和观测数据 \\(\\symbf y\\) 的分布不相同。最重要的是，它们的期望值并不相同。这就是关于如何解释模型或 ANOVA 结果的困惑开始的地方。观测数据的分布是边际分布。我们通过应用众所周知的概率论来获得它。令 \\(f\\left(\\symbf{y}\\mid\\symbf{b}\\right)\\) 表示以随机效应为条件的数据的 p.d.f.，\\(f\\left(\\symbf{b}\\right)\\) 表示随机效应的 p.d.f. 那么数据的边际分布为\\[f\\left(\\symbf{y}\\right)=\\int_{\\symbf{b}}f\\left(\\symbf{y}\\mid\\symbf{b}\\right)f\\left(\\symbf{b}\\right)d\\,\\symbf{b}\\]边际 p.d.f. \\(f\\left(\\symbf{b}\\right)\\) 描述了我们实际观察数据时的概率行为。我们必须从边际分布中推断出有关该模型的一切。在高斯情况下，假定产生数据的概率过程—— \\(f\\left(\\symbf{y}\\mid\\symbf{b}\\right)\\) 和 \\(f\\left(\\symbf{b}\\right)\\) ——与我们观察到的数据的概率分布—— \\(f\\left(\\symbf{y}\\right)\\) ——之间的关系是直接的：\\(\\symbf{y}\\mid\\symbf{b}\\sim N\\left(\\symbf{X}\\symbf{\\beta}+\\symbf{Z}\\symbf{b},\\symbf{R}\\right),\\quad\\symbf{b}\\sim N\\left(\\symbf{0},\\symbf{G}\\right)\\) 以及 \\(\\symbf{y}\\sim N\\left(\\symbf{X}\\symbf{\\beta},\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{R}\\right)\\)。重要的是，固定效应向量 \\(\\symbf\\beta\\) 的含义没有歧义，因此如何解释可估函数 \\(\\symbf K'\\symbf\\beta\\) 也没有歧义。然而在非高斯情况下，情况并非如此。我们的二项示例说明了非高斯数据的这些分布之间的相互作用。给定地点效应的数据条件分布为\\[f\\left(y_{ij}\\mid L_j\\right)=\\binom{100}{y_{ij}}\\Big(1/\\Big(1+e^{-n_{ij}}\\Big)\\Big)^{y_{ij}}\\Big[1-\\Big(1/\\Big(1+e^{-n_{ij}}\\Big)\\Big)\\Big]^{100-y_{ij}}\\]其中 \\(\\eta_{ij}=\\eta+\\tau_i+L_j\\)。随机模型效应的分布可从假定 \\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\) 开始或从其推论 \\(\\eta_{ij}\\sim N\\left(\\eta_i,\\sigma_L^2\\right)\\) 开始，其中 \\(\\eta_i=\\eta+\\tau_i\\)。后一种形式使边际 p.d.f. 更容易开发。具体而言，边际 p.d.f. 为\\[f\\left(y_{ij}\\right)=\\int_{-\\infty}^\\infty\\binom{100}{y_{ij}}\\left[1/\\left(1+e^{-\\eta_{ij}}\\right)\\right]^{y_{ij}}\\left[1-\\left(1/\\left(1+e^{-\\eta_{ij}}\\right)\\right)\\right]^{100-y_{ij}}\\left(1/\\sqrt{2\\pi\\sigma_L^2}\\right)e^{-\\left[\\left(\\eta_{ij}-\\eta_t\\right)^2/2\\sigma_L^2\\right]}d\\eta_{ij}\\]这是由非高斯混合模型定义的过程产生的数据的边际 p.d.f. 的典型形式。除了一些例外，GLMM 产生的边际 p.d.f. 难以进行直接的分析评估。然而，它们可以通过模拟进行可视化，也可以通过数值近似进行评估。我们将在第 6 章中讨论 GLMM 的近似方法。在本节中，只要将这种分布可视化，并确定 \\(y_{ij}\\) 的两个中心趋势指标——期望值和中位数——就足够了。这将阐明我们在示例 3.8 中关于正态近似和 GLMM 估计的困境。从更大的角度来看，我们需要了解边际分布的特征与我们为 GLMM 描述的过程有何关系，以便充分了解我们的建模选项是什么以及这些模型实际在估计什么（以及在许多情况下同样重要的是，他们没有估计什么）。","code":""},{"path":"搭建舞台.html","id":"sec3-5-4","chapter":"►搭建舞台","heading":"3.5.4 可视化边际 p.d.f.","text":"在我们的多地点二项示例中，处理 0 和 1 的 \\(\\pi_i\\) 估计在 0.1 附近，而在 GLMM 中，地点方差的估计约为 \\(\\hat\\sigma_L^2=1\\)。注意，与正态近似的方差分量估计不同，它是用定义边际分布的积分中使用的尺度来表示的。图 3.1 显示了本示例所涉及过程的随机数模拟生成的 1,000,000 个观测值的直方图。随机数是使用以下步骤生成的令 \\(\\eta=\\text{logit}\\begin{pmatrix}\\text{prob=}0.1\\end{pmatrix}=\\log\\begin{bmatrix}0.1/(1-0.1)\\end{bmatrix}=-2.19722\\)。为第 \\(\\) 个模拟观测生成标准正态随机变量 \\(z_i\\)。从而 \\(\\sqrt{\\sigma_L^2}z_i\\thicksim N\\left(0,\\sigma_L^2\\right)\\)。对于该模拟，\\(\\sigma^2_L=1\\)。到目前为止，我们已经模拟了产生随机模型效应的过程。也就是说，\\(\\sqrt{\\sigma_L^2}z_i\\) 模拟了产生地点效应的过程。因此，对于第 \\(\\) 个模拟观测，二项概率为 \\(p_i=1\\Big/\\Big[1+e^{-(\\eta+z_i)}\\Big]\\)。这模拟了地点效应对二项概率的效应。生成 \\(\\text{Binomial }(100,p_i)\\) 随机变量 \\(y_i\\)。这模拟了根据地点效应生成数据的过程。从直方图中可以明显看出，边际分布是强右偏的。根据 Box Whisper 图，我们可以看到这种分布的中位数是 0.1. 这是有道理的：只有当模拟的地点效应为 0（地点效应分布的中心）时，生成观测值的概率才恰好为 0.1. 由于地点效应的分布到生成概率的转换是非线性的，任何不在其分布确切中心的位地点效应都将不对称地转换为二项概率的分布，得到高度偏斜的边际分布。边际分布将始终是偏斜的，除非条件分布具有恰好为 0.5 的概率（决定 \\(\\eta\\) 的概率为 0.5） 的二项 p.d.f. 如果它小于 0.5，则边际分布将右偏；如果它大于 0.5，则边际分布将左偏。偏斜度随着概率接近 0 或 1 而增加。偏斜度也随着方差（本例为 \\(\\sigma^2_L\\)）的增加而增加。边际分布的均值是多少？我们可以使用高斯-埃尔米特求积 (Gauss-Hermite quadrature) 来近似。样本比例 \\(y_i/100\\) 的条件分布是 \\(\\text{Binomial }(100,p_i)\\)。二项比例 \\(p_i=1/\\left(1+e^{-\\eta_i}\\right)\\) 以及 \\(\\eta_i\\thicksim N\\left(\\eta,\\sigma_L^2\\right)\\) 其中 \\(\\eta=\\text{logit}(0.1)\\) 以及 \\(\\sigma^2_L=1\\)。样本比例的期望值可表示为\\[E_{p_i}\\left[E\\left(\\left[y_i/100\\right]|p_i\\right)\\right]=E\\left(p_i\\right)=\\int{-\\infty}^\\infty\\left[1/\\left(1+e^{-\\eta_i}\\right)\\right]\\frac1{\\Tiny\\sqrt{2\\pi\\sigma_L^2}}e^{-\\left[\\left(\\eta_i-\\eta\\right)^2/2\\sigma_L^2\\right]}d\\eta_i\\]令 \\(x_i^2=\\left(\\eta_i-\\eta\\right)^2/2\\sigma_L^2\\) 并代入上式得到\\[\\int_{-\\infty}^{\\infty}\\left(1\\Big/\\left[1+e^{-\\left(\\eta+\\sqrt{2\\sigma_Lx_i}\\right)}\\right]\\sqrt{\\pi}\\right)e^{-x_i^2}dx_i\\]使用高斯-埃尔米特求积，这可以近似为\\[\\sum_{k=1}^qw_k\\left\\{1\\Big/\\left(\\left[1+e^{-\\left(\\eta+\\sqrt{2\\sigma_Lx_k}\\right)}\\right]\\sqrt{\\pi}\\right)\\right\\}\\]其中 \\(w_k\\) 表示求积权重，\\(x_k\\) 表示求积节点，\\(q\\) 表示求积点的数量。使用 9 个求积点，样本比例的近似期望值为 0.1339. 图 3.1 所示模拟数据的样本均值为 0.1338.","code":""},{"path":"搭建舞台.html","id":"sec3-5-5","chapter":"►搭建舞台","heading":"3.5.5 正态近似和 GLMM 估计了什么？","text":"现在，我们可以通过边际分布看到基于正态近似的 ANOVA 与 GLMM 估计的差异以及为什么有这种差异。正态近似使用边际分布的样本比例作为响应变量。我们知道，处理均值的 ANOVA 估计是每种处理响应变量期望值的无偏估计。也就是说，ANOVA 提供了每个处理的边际分布均值的无偏估计。另一方面，GLMM 通过估计线性预测器的参数，从而确定模型尺度估计 \\(\\hat{\\eta}+\\hat{\\tau}_i\\)，最后确定数据尺度估计 \\(\\hat{{\\pi}}_i=1\\Big/\\Big(1+e^{-(\\hat{\\eta}+\\hat{\\tau}_i)}\\Big)\\)。注意，这些是广义推断空间估计，因为它们基于可估函数 \\(\\symbf k'\\symbf\\beta\\)。此外，由于在可估函数中 \\(\\symbf M=\\symbf 0\\) ——对于该模型，所有 \\(L_j=0\\) ——这些估计发生在假定产生这些数据的二项概率分布的中心。换句话说，\\(\\hat\\pi_i\\) 估计第 \\(\\) 种处理的边际分布的中位数。这回答了“为什么正态近似和 GLMM 会产生不同的结果？”同时，它也以可理解的方式提出了一个问题：“应该使用哪种估计？”思考该问题的第一种方法是回顾统计学入门课中关于位置度量的讨论。如图 3.1 所示的偏斜分布通常用于说明中位数是首选的位置度量，而均值被认为是不明智的选择。思考该问题的第二种方法是回到统计模型的定义。统计模型应该描述一个“产生数据的合理过程”。在这个例子中，条件模型显然为数据提供了一个连贯且可信的概率机制。而正态近似则不然。在条件 GLMM 中，\\(\\pi_i\\)（应用第 \\(\\) 种处理时感兴趣结果发生的概率）的含义是清晰且明确的。但对于正态近似，这一含义是模糊的。思考该问题的第三种方法是考虑研究人员如何构思这类数据的。每种处理都有发生感兴趣结果的概率，这就是研究者想要确定的“\\(\\pi_i\\)”。研究人员还意识到，地点会影响处理概率。研究人员所理解的“第 \\(\\) 种处理的概率”，我们可以认为是“金发女郎”概率——在某个“恰到好处”的地点（既不高于又不低于平均），结果发生的概率。正式地，这是地点分布中心的概率，即 GLMM 的可估函数 \\(\\eta+\\tau_i\\)。无论研究人员使用正态近似进行 ANOVA 还是使用 GLMM，他们都认为自己在估计这个概率。这就是 GLMM 相关问题的“你跑得了但躲不了”方面。ANOVA 用户不会回避这个问题；他们只是忽视了存在该问题的事实。思考该问题的最后一种方法是更仔细地询问“研究人员在试图估计什么？”目标是估计总体中一个典型成员感兴趣结果的概率吗？对于本例，是在一个典型的地点吗？若如此，“金发女郎”的做法似乎是恰当的。如果你从总体中随机抽取一个地点，它的预期值既不会高于也不会低于平均值。另一方面，如果你真的想估计整个总体的均值，并且也理解使用均值来刻画偏斜分布的含义，那么由正态近似得到的估计实际上可能更为合适。例如，如果你要估计疾病发病率，GLMM 估计值将为你提供典型社区中的预期发病率；通过正态近似得出的估计将为你提供整个人群的疾病总发病率。“估计”一词在上句话中加粗了，因为正如我们所知，在统计推断中，这并不完全是关于估计的。如果边际均值确实是研究的焦点，那么仍然存在准确计算标准误并由此得到准确的检验统计量的问题。如前所述，正态近似不仅估计边际分布的均值，而且还假定方差同质性——我们知道，对于二项分布数据而言，这一假定并不成立，除非在处理对概率没有影响的情况下（然而这通常是无趣的）。为全面解决这些问题，我们现在引入本节最终的重点——条件与边际混合模型。","code":""},{"path":"搭建舞台.html","id":"sec3-5-6","chapter":"►搭建舞台","heading":"3.5.6 高斯条件和边际模型","text":"线性混合模型的标准指定包括以随机模型效应为条件的数据 \\(\\symbf y\\mid\\symbf b\\) 的分布。随机模型效应 \\(\\symbf b\\) 的分布。连接函数：\\({\\symbf\\eta}=g\\left[E(\\symbf{y}\\mid\\symbf{b})\\right]\\)。线性预测器：\\({\\symbf\\eta}=\\symbf{X\\beta}+\\symbf{Zb}\\)。另一种方法是，描述与混合模型定义的过程相同的模型可根据边际分布和线性预测器 \\(\\symbf{X\\beta}\\) 来指定，而无需明确指出随机效应。这是通过在边际分布的方差中嵌入与随机效应相关的方差来完成的。对于高斯数据，混合模型定义了 \\(\\symbf{y}\\mid\\symbf{b}\\thicksim N\\left(\\symbf{X}\\symbf{\\beta}+\\symbf{Z}\\symbf{b},\\symbf{R}\\right)\\) 以及 \\(\\symbf{b}\\sim N(\\symbf{0},\\symbf{G})\\)。\\(\\symbf y\\) 的边际分布为 \\(N\\left(\\symbf{X\\beta},\\symbf{ZGZ}^{\\prime}+\\symbf{R}\\right)\\)。由 \\(\\symbf{\\eta}=\\symbf{X\\beta}\\) 和方差 \\(\\symbf{V}=Var\\left(\\symbf{y}\\right)=\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{R}\\) 定义的仅固定效应模型对固定效应的推断与混合模型基于可估函数的推断相同。对于非高斯数据，这种等价性不成立。根据 \\(\\symbf y\\mid\\symbf b\\) 和 \\(\\symbf b\\) 的分布以及线性预测器 \\(\\symbf{\\eta}=\\symbf{X\\beta}+\\symbf{Zb}\\) 定义的模型，会得出我们在上一节中考虑的二项多地点示例（示例 3.8）中 GLMM 所沿用的固定效应推断。一个仅基于 \\(\\symbf{\\eta}=\\symbf{X\\beta}\\) 定义的广义线性模型会得出以边际分布均值为目标的推断。正如我们在上一节看到的，这两种方法会得到具有不同含义的不同估计。根据 \\(\\symbf y\\mid\\symbf b\\) 和 \\(\\symbf b\\) 的分布以及线性预测器 \\(\\symbf{\\eta}=\\symbf{X\\beta}+\\symbf{Zb}\\) 定义的模型称为条件模型 (conditional models)；根据 \\(\\symbf y\\) 的边际分布和线性预测器 \\(\\symbf{\\eta}=\\symbf{X\\beta}\\) 定义的模型称为边际模型 (marginal models).为了解它们在高斯情况下的工作原理，我们回到假定随机地点效应的多地点数据（示例 3.6）。回想，数据的条件分布为 \\(y_{ij}\\mid L_j\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)，线性预测器为 \\(\\eta_{ij}=\\eta+\\tau_i+L_j\\) 以及 \\(L_j\\operatorname{iid}N\\left(0,\\sigma_L^2\\right)\\)。这就是条件模型。我们可以用模型方程形式表示为 \\(y_{ij}=\\eta+\\tau_i+L_j+w_{ij}\\)，其中 \\(w_{ij}\\) 表示地点内变异，并假定为 \\(\\operatorname{iid}N\\big(0,\\sigma_W^2\\big)\\)。注意 \\(\\sigma^2=\\sigma^2_W\\)。模型方程可重写为 \\(y_{ij}=\\eta+\\tau_i+e_{ij}\\)，其中 \\(e_{ij}\\) 定义为 \\(L_{j}+w_{ij}\\)。注意，我们正在做的是从线性预测器中移除所有随机模型效应——此处得到 \\(\\eta+\\tau_i\\) ——并将所有随机变异嵌入 \\(y_{ij}\\) 的方差中。地点效应变异仍存在于模型中，只是不显式存在于线性预测器中。模型的方差结构现在为：\\(Var\\left(y_{ij}\\right)=Var\\left(e_{ij}\\right)=Var\\left(L_j+w_{ij}\\right)=\\sigma_L^2+\\sigma_W^2\\)\\(Cov\\left(y_{ij},y_{'j}\\right)=Cov\\left(e_{ij},e_{'j}\\right)=Cov\\left(L_j+w_{ij},L_j+w_{'j}\\right)=\\sigma_L^2\\)\\(Cov\\left(y_{ij},y_{^{\\prime}j^{\\prime}}\\right)=Cov\\left(e_{ij},e_{^{\\prime}j^{\\prime}}\\right)=0\\)因此，对于第 \\(j\\) 个地点：\\[Var\\begin{bmatrix}e_{1j}\\\\e_{2j}\\\\e_{3j}\\\\e_{4j}\\end{bmatrix}=\\begin{bmatrix}\\sigma_L^2+\\sigma_W^2&\\sigma_L^2&\\sigma_L^2&\\sigma_L^2\\\\&\\sigma_L^2+\\sigma_W^2&\\sigma_L^2&\\sigma_L^2\\\\&&\\sigma_L^2+\\sigma_W^2&\\sigma_L^2\\\\&&&\\sigma_L^2+\\sigma_W^2\\end{bmatrix}=\\sigma_E^2\\begin{bmatrix}1&\\rho&\\rho&\\rho\\\\&1&\\rho&\\rho\\\\&&1&\\rho\\\\&&&1\\end{bmatrix}\\]其中 \\(\\sigma_E^2=\\sigma_L^2+\\sigma_W^2\\) 以及 \\(\\rho=\\sigma_L^2/\\left(\\sigma_L^2+\\sigma_W^2\\right)\\)。这称为复合对称 (compound symmetry) 协方差模型，许多文本称 \\(\\rho=\\sigma_L^2/\\left(\\sigma_L^2+\\sigma_W^2\\right)\\) 为类内相关性 (intra class correlation)。注意，当 \\(\\rho\\ge 0\\) 时它等价于混合模型。唯一的区别在于，\\(\\hat L_j\\) 以及涉及它们的可预测函数无法从复合对称模型中获得。对于多地点数据，复合对称模型是边际模型。地点效应不出现在模型中，但随机地点效应的影响蕴含在方差结构中。以下 GLIMMIX 语句实现了边际多地点模型。请注意，出现的唯一 RANDOM 语句使用了 _RESIDUAL_。GLIMMIX 将其理解为定义一个响应 \\(Y\\)，其分布为 \\(\\symbf{y}\\sim N(\\symbf{X\\beta},\\symbf{V})\\)，因为正态是默认分布，\\(\\symbf{X\\beta}\\) 由 MODEL 语句的右侧定义，\\(\\symbf V\\) 的结构由 RANDOM _RESIDUAL_ 语句定义。在 RANDOM _RESIDUAL_语句中，SUBJECT=LOC 设置分块对角阵；TYPE=CS将每个分块 (LOC) 定义为具有上述的复合对称结构。选定输出：除方差分量估计的标签外，就固定效应的推断而言——包括处理均值、差异及任何可能感兴趣的对比——这里的结果与之前在示例 3.6 的条件随机地点效应高斯模型展示的结果完全相同。","code":"proc glimmix data=MultiLoc_4Trt;\n class loc trt;\n model Y= trt / solution;\n random _residual_ / subject=loc type=cs;\n lsmeans trt / diff;"},{"path":"搭建舞台.html","id":"sec3-5-7","chapter":"►搭建舞台","heading":"3.5.7 非高斯边际模型与条件模型","text":"现在考虑二项情形。在本节开头，我们考虑了条件模型——二项 GLMM：线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+l_j\\)分布：\n\\(y_{ij}\\mid L_j\\sim \\text{Binomial}\\left(N_{ij},\\pi_{ij}\\right)\\)\n\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)\n\\(y_{ij}\\mid L_j\\sim \\text{Binomial}\\left(N_{ij},\\pi_{ij}\\right)\\)\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)连接：logit，\\(\\eta_{ij}=\\text{logit} (\\pi_{ij})\\)那么边际模型呢？首先，我们从线性预测器中移除随机效应。这就留下了 \\(\\eta_{ij}=\\eta+\\tau_{}\\)。“分布”为 \\(\\mathrm{''Binomial''}(n_{ij},\\tilde\\pi_{ij})\\)。请注意，分布实际上不是二项的。它是边际分布，是关于随机地点效应积分的结果。考虑地点效应的二项条件分布是一个组件，但正如我们之前看到的，该模型中的分布不是二项的。尽管如此，广义线性模型约定将模型标记为 \\(\\mathrm{''Binomial''}\\)，因此我们遵守此约定。为避免与条件模型混淆，这里的关键符号说明为：我们将概率参数称为 \\(\\tilde \\pi_{ij}\\) 而非 \\(\\pi_{ij}\\)。\\(\\tilde \\pi_{ij}\\) 表示边际期望；而 \\(\\pi_{ij}\\) 表示条件模型中二项分布的概率。最后，我们需要考虑协方差结构中的地点效应。在非高斯边际 GLMM 中，这是使用工作协方差阵 (working covariance matrix) 来完成的。工作协方差阵模拟了高斯协方差模型的结构，但它不是真正的协方差。此处第 \\(j\\) 个地点的工作协方差阵为\\[\\symbf{P}_W\\begin{bmatrix}y_{1j}\\\\y_{2j}\\\\y_{3j}\\\\y_{4j}\\end{bmatrix}={\\phi}\\begin{bmatrix}1&\\rho&\\rho&\\rho\\\\&1&\\rho&\\rho\\\\&&1&\\rho\\\\&&&1\\end{bmatrix}\\]其中，\\(\\symbf{P}_W\\) 表示工作协方差，\\(\\phi\\) 是尺度参数，\\(\\rho\\) 表示工作相关性。尺度参数和工作相关性的解释与高斯情况下的 \\(\\sigma^2_E\\) 和 \\(\\rho\\) 的解释在精神上是相同的，但实际的解释又不尽相同。工作协方差模型主要是一种获得边际估计的工具，允许考虑地点方差对标准误和检验统计量的影响。拟似然 (Quasi-likelihood). 当我们引入工作协方差结构时，所得结构的基本形式源自二项 p.d.f.，但其中嵌入了工作协方差。其结果是一个数学形式，它不再是一个概率分布，实际上也不对应于任何可行的概率机制。在第 5 章我们将引入拟似然。Wedderburn (1974) 提出的拟似然，指的是这样一类数学形式，它们虽然与已知概率分布的似然函数共享进行广义线性模型估计所必需的特征，但其期望和协方差结构意味着它们不是真正的似然。所有非高斯边际 GLMM 都依赖于拟似然理论。二项边际模型的 GLIMMIX 语句如下：请注意，除了响应变量以及 LSMEANS 语句中 ILINK 选项的使用外，该程序与高斯边际模型的程序相同。对于非高斯数据，RANDOM _RESIDUAL_ 语句定义了工作协方差结构。选定结果：在当前的讨论中，最值得注意的结果是 “Trt Least Squares Means” 表中的数据尺度估计和标准误。请注意，这些估计与正态近似的估计相同：这些是边际均值的估计值。它们是 \\(\\tilde\\pi_{ij}\\) ——不是二项分布的概率，而是边际分布期望——的无偏估计。作为对边际 GLMM 的最后评论，请记住，我们对统计模型的定义标准之一为，理想情况下，它应该描述产生观测数据的合理机制。按照这个标准，边际 GLMM 失败了。一旦我们根据工作协方差定义了模型，我们就不再有真正的概率分布。相反，我们有一个拟似然，我们将在第 5 章中正式定义它。没有已知的概率机制可以产生边际模型所描述的数据。在我们这里考虑的模型中，条件 GLMM 是唯一一个真正描述可能过程的模型。任何合理的替代方案仍然会有一个两步过程——根据某个分布改变二项概率，然后根据以该地点为条件的二项概率生成二项观测。不管你喜欢与否，该生成机制描述了一个条件模型。边际 GLMM 并没有描述一个过程；它只是允许在边际均值被视为目标时对其进行估计。","code":"proc glimmix data=MultiLoc_4Trt;\n class loc trt;\n model S_1/N_Bin = trt / solution;\n random _residual_ / subject=loc type=cs;\n lsmeans trt / diff ilink;"},{"path":"搭建舞台.html","id":"条件模型的最后一个方面残差在高斯-lmm-与单参数非高斯-glmm-中的作用","chapter":"►搭建舞台","heading":"条件模型的最后一个方面：“残差”在高斯 LMM 与单参数非高斯 GLMM 中的作用","text":"在第 1 章中，我们建立了“概率分布形式”作为表示线性模型的首选方式。该讨论使我们认识到，在传统“模型方程”形式的模型中所包含的“误差”或“残差”项，在以“概率分布”形式表述的线性模型的线性预测器中是不应该出现的。对于高斯数据，\\(\\symbf y\\mid\\symbf b\\) 条件分布的均值和方差是独立的参数，必须单独估计。线性预测器估计均值，而估计 \\(Var(\\symbf y\\mid\\symbf b)\\) 所需的信息必须来自其他地方。我们在第 2 章中看到，从线性预测器中排除“残差”相当于从估计 \\(Var(\\symbf y\\mid\\symbf b)\\) 的框架 ANOVA 的残差分量中去除信息。更进一步，我们提出以下问题。对于具有单参数非高斯分布（如二项分布或泊松分布）的数据的模型，既然我们不需要框架 ANOVA 中的“最后一项”来估计方差，那么该项是否可能有其他作用呢？在某些情况下，答案是“是的”。这里我们利用多地点数据来说明这一点。回想第 2 章用于帮助构建线性预测器的“Fisher会怎么做？”框架 ANOVA 练习。表 3.3 总结了多地点数据的这一练习。\n组合框架 ANOVA 的三个版本如表 3.3 所示。三者在地点和处理主效应的表现方式上一致。所得的任何线性预测器必须至少包含 \\(\\eta+\\tau_i+L_j\\) ——我们在本章中使用多地点数据的所有示例中使用的线性预示器。框架 ANOVA 对最后具有 21 个自由度的项的理解不同。具有高斯数据的模型必须使用 “Combined v.3.”。数据分布对于仅固定效应模型为 \\(y_{ij}\\sim NI\\left(\\mu_{ij},\\sigma^2\\right)\\)，对于随机地点效应模型为 \\(y_{ij}\\mid L_j\\sim NI\\left(\\mu_{ij},\\sigma^2\\right)\\)。无论哪种方式，都必须保留“残差”来估计 \\(\\sigma^2\\)。如果每个地点-处理组合没有多个观测，具有高斯数据的模型就无法使用 “Combined v.3.” ANOVA. 如果为 \\(\\sigma^2\\) 保留了大于 0 的自由度，就无法在高斯模型中估计地点 × 处理交互作用。对于具有二项数据的模型，这是不正确的。在本章迄今为止的所有示例中，我们使用了与高斯模型相同的线性预测器。然而，原则上没有理由不将地点 × 处理交互作用纳入线性预测因子。如果我们这样做会发生什么？我们可以将线性预测因子改为 \\(\\eta_{ij}=\\eta+\\tau_i+L_j+\\left(\\tau L\\right)_{ij}\\)，其中 \\(\\left(\\tau L\\right)_{ij}\\) 表示第 \\(ij\\) 个地点 × 处理效应，假定为 \\(\\mathrm{iid~}N\\left(0,\\sigma_{TL}^2\\right)\\)，使用 GLIMMIX 程序计算如下方差分量估计为 \\(\\hat{\\sigma}_L^2=0.948\\) 以及 \\(\\hat{\\sigma}_{TL}^2=0.003\\)。相比之下，无交互作用模型 \\(\\hat{\\sigma}_L^2=0.946\\)。其他估计和检验统计量也发生了可忽略的变化。对于该响应变量，没有证据表明存在不可忽略的地点 × 处理交互作用。不管该项是否保留在模型中，结果都基本相同。然而，情况并非总是如此。我们现在考虑相同的数据，但具有不同的响应变量，在 Data Set 3.2 中表示为 S_2。将比较四种分析：无交互作用的二项条件 GLMM. 线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+L_j\\)。随机地点效应。Logit 连接。这是最开始介绍条件 GLMM 的模型。正态近似。响应变量：样本比例。线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i+L_j\\)。这是最开始介绍边际分布的模型。具有交互作用的二项条件 GLMM。线性预测因子：\\(\\eta_{ij}=\\eta+\\tau_i+L_j+\\left(\\tau L\\right)_{ij}\\)。Logit 连接。二项边际 GLMM. 线性预测器：\\(\\eta_{ij}=\\eta+\\tau_i\\)。Logit 连接。复合对称工作协方差。表 3.4 概述了相关结果。\n表 3.4 中最引人注目的结果是，在关于原假设 \\(H_0\\)：所有 \\(\\tau_i=0\\) 的 \\(F\\) 检验中，不包含交互作用的条件 GLMM 与其他三个模型之间存在巨大差异。为什么会存在这种差异呢？在无交互作用的 GLMM 中，线性预测器是 \\(\\eta_{ij}=\\eta+\\tau_i+L_j\\)，并假设给定地点内观测间的所有变异为 \\(\\pi_{ij}(1-\\pi_{ij})\\)（二项的均值-方差关系的结果）。在每个其他模型中，都规定了每个地点内处理间的相对差异会受到随机过程的干扰。这在正态近似中体现为残差方差 \\(\\sigma^2\\)；在边际 GLMM 中体现为尺度参数 \\(\\phi\\)。在有交互作用的条件 GLMM 中，根据定义，地点 × 处理交互作用是不同地点的处理之间对数几率比的随机变异。无交互作用的条件 GLMM 做出了一个潜在的不切实际的假设，即处理之间的几率比在不同地点保持绝对不变，同时假定每个地点所有处理的平均对数几率在因地点随机变异。若未考虑到各地点间几率比的随机变异，会导致 \\(F\\) 值膨胀，扭曲处理概率的估计，并低估其标准误。这是广义线性混合模型表现出过度分散症状的一种预示。当观测方差大于模型所能解释的方差时，过度分散就会发生。与前面的例子一样，正态近似和边际 GLMM 得到了相同的处理期望估计，即前面讨论的边际 \\(\\tilde\\pi_i\\)。正态近似的标准误没有考虑到二项数据定义中已知的方差异质性。而边际 GLMM 考虑了。这两种分析的 \\(F\\) 值基本一致。具有交互作用的条件 GLMM 产生了条件模型中定义的 \\(\\pi_i\\) 的精确估计。回想一下，这些是估计的“金发女郎”概率——我们在典型地点的预期。标准误反映了二项分布的均值-方差关系的全部影响。用于检验处理效应的 \\(F\\) 值反映了随机地点 × 处理效应的影响，因此它不像无交互作用的条件 GLMM 那样膨胀。同时，\\(F\\) 值明显大于正态近似和边际 GLMM 的 \\(F\\) 值。这不是该数据集的假象。这是由边际分布的偏斜引起的。回想一下，相对于产生数据的实际过程的概率，即 \\(\\pi_i\\)，边际“概率”向 0.5 移动。这导致边际 \\(\\tilde\\pi_i\\) 之间的差异相对于 \\(\\pi_i\\) 之间的差异被减弱。这反过来又降低了处理效应检验的功效：假定 类错误得到了适当的控制，就像这里具有交互作用的条件 GLMM 所控制的那样，边际 GLMM 的功效总是低于相应的条件 GLMM.3.5 节的关键主题：给定随机效应的数据的条件分布；观测的边际分布；我们实际看到的数据分布是什么；当 ANOVA 应用于非正态数据时会发生什么？；边际分布的两个位置度量以及哪些模型估计了什么？；复合对称；条件模型；边际模型；工作相关性；工作协方差；残差在高斯数据中的作用；非高斯数据中“残差”的另一种可能的理解。","code":"proc glimmix data=MultiLoc_4Trt;\n class loc trt;\n model s_1/N_Bin= trt / solution;\n random intercept trt / subject=loc;\n /* or alternatively random loc loc*trt; */\n lsmeans trt / ilink;"},{"path":"搭建舞台.html","id":"sec3-6","chapter":"►搭建舞台","heading":"3.6 总结","text":"在许多方面，本章中的材料是这本教科书中最重要的。这些想法很复杂，包括统计建模中一些被广泛误解和误用的概念。具体地，在建模领域，条件模型和边际模型关于推断的区别才刚刚开始被认识和理解。理解这一区别及其对模型编写方式和推断过程的影响，是有效使用现代统计模型的关键先决条件。用四个总体思想来概括本章的精髓：统计模型的推断是通过可估和可预测函数进行的。它们构成了线性模型假设检验和区间估计的基础。为了进行统计推断，必须能够用可估或可预测函数来表达研究目标。统计模型的推断是通过可估和可预测函数进行的。它们构成了线性模型假设检验和区间估计的基础。为了进行统计推断，必须能够用可估或可预测函数来表达研究目标。对于具有非恒等连接函数的广义线性模型，所有估计和推断都发生在模型尺度上——也就是说，对于所有 GLM 为参数向量 \\(\\symbf\\beta\\)，对于 GLMMs 还需加上随机向量 \\(\\symbf b\\)。然而，出于报告目的，通常需要将估计从模型尺度转换为数据尺度。对于具有非恒等连接函数的广义线性模型，所有估计和推断都发生在模型尺度上——也就是说，对于所有 GLM 为参数向量 \\(\\symbf\\beta\\)，对于 GLMMs 还需加上随机向量 \\(\\symbf b\\)。然而，出于报告目的，通常需要将估计从模型尺度转换为数据尺度。对于混合模型，推断可以广义地适用于由随机模型效应表示的整个总体，或狭义地适用于观测随机效应的子集。可估函数定义了广义推断；可预测函数定义了狭义推断。我们将这些称为推断空间问题。对于混合模型，推断可以广义地适用于由随机模型效应表示的整个总体，或狭义地适用于观测随机效应的子集。可估函数定义了广义推断；可预测函数定义了狭义推断。我们将这些称为推断空间问题。广义推断有两种——条件推断和边际推断。对于高斯数据它们是等价的，而对于非高斯数据则不然。条件推断的目标是给定随机模型效应的观测假定分布的期望值。边际推断的目标是观测边际分布的期望值。除高斯数据外，观测边际分布与响应变量的假定分布不同。例如，对于二项数据，条件广义推断估计二项概率；边际推断则不然。边际广义推断也称为总体平均推断（有点欺骗性）。一般来说，GLMM 的边际分布是高度偏斜的。对于边际广义推断（总体平均推断），可估函数取决于作为位置度量的边际分布均值。对于条件广义推断，可估函数取决于条件分布在 \\(\\symbf M'\\symbf b=\\symbf 0\\) 处的均值，可转译为作为位置度量的边际分布中位数。选择边际还是条件广义推断，本质上是在使用均值还是中位数作为高度偏斜分布的优选位置度量之间的选择。广义推断有两种——条件推断和边际推断。对于高斯数据它们是等价的，而对于非高斯数据则不然。条件推断的目标是给定随机模型效应的观测假定分布的期望值。边际推断的目标是观测边际分布的期望值。除高斯数据外，观测边际分布与响应变量的假定分布不同。例如，对于二项数据，条件广义推断估计二项概率；边际推断则不然。边际广义推断也称为总体平均推断（有点欺骗性）。一般来说，GLMM 的边际分布是高度偏斜的。对于边际广义推断（总体平均推断），可估函数取决于作为位置度量的边际分布均值。对于条件广义推断，可估函数取决于条件分布在 \\(\\symbf M'\\symbf b=\\symbf 0\\) 处的均值，可转译为作为位置度量的边际分布中位数。选择边际还是条件广义推断，本质上是在使用均值还是中位数作为高度偏斜分布的优选位置度量之间的选择。只有当我们有随机模型效应和非正态数据时，条件-边际问题才会出现。它不出现在高斯数据中，也不出现在仅具有固定效应的非高斯数据中。因此，在 GLMMs 出现之前，建模界的任何人都不会遇到这个问题。鉴于 GLMM 理论相对较新，并且可用的 GLMM 软件在过去十几年中才出现，因此条件-边际问题没有出现在传统的线性模型文献中，直到现在才开始受到重视，这并不奇怪。然而，每当非高斯数据和混合模型结构共存时，例如二项或计数数据的裂区实验，条件-边际问题都存在，无法避免。从这个意义上说，除非理解本章中的问题，否则学习本教材其余部分的材料在某种程度上是没有意义的。","code":""},{"path":"搭建舞台.html","id":"exe3","chapter":"►搭建舞台","heading":"练习","text":"website data set/Exercises Chapter 311 中有四个数据集：4rt_blocked_1：是一个有四种处理的随机完全区组。4rt_blocked_2：是一个有四种处理的均衡不完全区组 (balanced incomplete block, BIB).4rt_blocked_3：一个不完全区组设计——在 BIB 意义上不均衡，但扩增了，因此所有处理都重复了四次。4rt_blocked_4：一个具有四种处理的不完全区组设计。在线性模型课程中，你会称之为“不连通”的设计——但你可能会错过它的真正含义。所有“4rt_”数据集都有 2 个响应变量—— \\(Y\\) 可以假定为正态（高斯）；Count 应假定为泊松。regression_y_count：在多个 “reps”（可以是地点、批次、区组等）中应用的定量水平——可以假定响应变量 \\(Y\\) 为正态（高斯）；假定（除非另有指示）响应变量 COUNT 具有泊松分布。regression_binomial：在多个 “reps”（可以是地点、批次、区组等）中应用的定量水平——响应变量是二项的（每个重复 × \\(X\\) 的水平中 \\(N\\) 次观测“成功”了 \\(Y\\) 次）。使用专注于 PROC GLIMMIX 的 SAS 线性模型软件，将其视为对推断问题的探索——模型与数据尺度、狭义与广义、条件与边际。以下是建议你应该回答的问题。此外，你应该超越自我，提出自己的“当我这样做时会发生什么？”的探索。对于 4rt_blocked_1 数据集—— \\(Y\\) 变量：\n使用固定区组写出模型，然后使用随机区组写出模型。\n为固定区组和随机区组模型编写所需的 SAS 语句。\n验证 random block 和 random intercept / subject=block 是否得到相同的结果（并确保你理解原因）\n将 e 选项添加到 LSMEANS 语句中。比较和对比固定区组模型和随机区组模型的结果。这些告诉你什么？为什么它们不同？\n比较并对比 (compare contrast) 由 LSMEANS 语句计算的处理均值和差异的标准误。说明相似之处和不同之处。\n对于 4rt_blocked_1 数据集—— \\(Y\\) 变量：使用固定区组写出模型，然后使用随机区组写出模型。为固定区组和随机区组模型编写所需的 SAS 语句。验证 random block 和 random intercept / subject=block 是否得到相同的结果（并确保你理解原因）将 e 选项添加到 LSMEANS 语句中。比较和对比固定区组模型和随机区组模型的结果。这些告诉你什么？为什么它们不同？比较并对比 (compare contrast) 由 LSMEANS 语句计算的处理均值和差异的标准误。说明相似之处和不同之处。对于 4rt_blocked_2 数据集—— \\(Y\\) 变量：\n重复 1. 中的 -e.\n使用 bylevel 和 e 选项运行第二个 LSMEANS 语句。这些与默认的 LSMEANS 相比如何？LSMEANS BYLEVEL 在估计什么？\n写一个 ESTIMATE 语句来计算 trt1 和 trt2 LSMEANS BYLEVEL 估计之差。这些在估计什么？\n通常，你会使用默认的 LSMEANS 结果还是 LSMEANS BYLEVEL 的结果？请简单解释！\n将区组随机模型重写为具有复合对称协方差结构的编辑模型。验证随机区组效应模型和复合对称模型是否产生相同的结果。\n对于 4rt_blocked_2 数据集—— \\(Y\\) 变量：重复 1. 中的 -e.使用 bylevel 和 e 选项运行第二个 LSMEANS 语句。这些与默认的 LSMEANS 相比如何？LSMEANS BYLEVEL 在估计什么？写一个 ESTIMATE 语句来计算 trt1 和 trt2 LSMEANS BYLEVEL 估计之差。这些在估计什么？通常，你会使用默认的 LSMEANS 结果还是 LSMEANS BYLEVEL 的结果？请简单解释！将区组随机模型重写为具有复合对称协方差结构的编辑模型。验证随机区组效应模型和复合对称模型是否产生相同的结果。对 4rt_blocked_2 数据集重复 2.对 4rt_blocked_2 数据集重复 2.对 4rt_blocked_4 重复 2. 特别注意固定区组模型和随机区组模型结果的比较。\n我们可以提出一个令人信服的论点，称这种设计为“不连通”并未抓住重点。什么是不连通设计？为什么称此设计为不连通？为什么它未抓住重点？【提示：Fisher 会如何看待这个设计？继 Fisher 之后，该设计的另一个名字（通常在实验课程的设计中讨论）是什么？】这是一个充分展示了固定区组思维的局限性的例子——它把你逼到了一个无法逃脱的角落。对 4rt_blocked_4 重复 2. 特别注意固定区组模型和随机区组模型结果的比较。我们可以提出一个令人信服的论点，称这种设计为“不连通”并未抓住重点。什么是不连通设计？为什么称此设计为不连通？为什么它未抓住重点？【提示：Fisher 会如何看待这个设计？继 Fisher 之后，该设计的另一个名字（通常在实验课程的设计中讨论）是什么？】这是一个充分展示了固定区组思维的局限性的例子——它把你逼到了一个无法逃脱的角落。对于数据集 regression_y_count（此练习使用响应变量 \\(Y\\)）：\n写出假定固定 “reps” 的模型，然后写出随机 “reps” 的模型。\n写出这两种模型的 SAS 语句，这里假定固定效应相互独立。\n比较并对比两种分析的结果。\n在 c. 中纳入对“总体平均”回归方程的估计。请注意，随机“rep”模型很容易获得这一点；固定的“rep”模型需要识字的ESTIMATE语句。\n重写随机 “rep” 模型，假定随机截距和随机斜率项在每个 “rep” 内相关。将这些结果与 b. 中的结果进行比较和对比。\n对于数据集 regression_y_count（此练习使用响应变量 \\(Y\\)）：写出假定固定 “reps” 的模型，然后写出随机 “reps” 的模型。写出这两种模型的 SAS 语句，这里假定固定效应相互独立。比较并对比两种分析的结果。在 c. 中纳入对“总体平均”回归方程的估计。请注意，随机“rep”模型很容易获得这一点；固定的“rep”模型需要识字的ESTIMATE语句。重写随机 “rep” 模型，假定随机截距和随机斜率项在每个 “rep” 内相关。将这些结果与 b. 中的结果进行比较和对比。使用数据集 regression_binomial 重复 5.使用数据集 regression_binomial 重复 5.对数据集 regression_y_count 中的 Count 响应变量重复 1. - 4.对数据集 regression_y_count 中的 Count 响应变量重复 1. - 4.（续）对 Count 响应变量重复 1.，但假定计数近似正态。（续）对 Count 响应变量重复 1.，但假定计数近似正态。（续）对 Count 响应变量重复 1.，但使用对数变换并假定 log_count 近似正态。（续）对 Count 响应变量重复 1.，但使用对数变换并假定 log_count 近似正态。比较并对比 7. - 9. 的结果，特别注意边际推断与条件推断问题。比较并对比 7. - 9. 的结果，特别注意边际推断与条件推断问题。","code":""},{"path":"chap4.html","id":"chap4","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"第 4 章 GLMM 之前的估计和推断基础知识","text":"","code":""},{"path":"chap4.html","id":"sec4-1","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.1 介绍","text":"本章开始本教材的第二篇：线性模型估计与推断的理论与方法。在第一篇——前三章——我们探讨了线性模型的基本结构、研究设计和模型构建之间的相互作用以及主要的推断问题，包括随机模型效应和非高斯响应变量引入的问题。到目前为止，我们一直故意对黑匣子里发生的事情含糊其辞。我们如何估计参数并构建推断统计？什么理论证明了这些方法的合理性？本篇分为四章。在本章中，我们回顾了经典线性模型估计和推断理论的亮点，为本篇后续章节提供了必要的背景。在第 5 章中，我们讨论了广义线性模型、线性混合模型和广义线性混合模型的估计。接下来的两章介绍了 GLMM 的推断理论和方法。第 6 章介绍了模型效应的推断，第 7 章介绍了协方差分量的推断。经典的，或“一般”线性模型，在本书中以当代缩写称为 LM 的模型，由以下特征定义：\\(E\\left(\\symbf{y}\\right)=\\symbf{\\mu}\\)\\(Var\\left(\\symbf{y}\\right)=\\boldsymbol\\Sigma\\sigma^2\\)，其中 \\(\\boldsymbol\\Sigma\\) 未知线性预测器：\\(\\symbf{X\\beta}\\)连接：恒等，即 \\(\\symbf\\eta=\\symbf\\mu\\)，因此 \\(\\symbf{X\\beta}\\) 旨在估计 \\(\\symbf\\mu\\)注：SAS PROC GLM 的名字来源于缩写词 “General Linear Model”. 1976 年，当 PROC GLM 首次出现时，我们现在所说的 LM 被认为是“一般的”——按照 1976 年的标准。它称为“一般的”，因为它允许我们指定 \\(\\symbf{X\\beta}\\) 的方式具有完全的灵活性。按照 2023 年的标准，将这种模型称为“一般的”是过时的，因为我们现在的模型包括随机效应（混合模型, mixed models），并将线性预测因子拟合到 \\(\\symbf\\mu\\) 的非线性函数（广义线性模型, generalized linear models）（勿将“一般”与“广义”混淆——它们不一样）。按照 2023 年的标准，真正的一般线性模型是广义线性混合模型 (generalized linear mixed model, GLMM). 在我们介绍 GLMM 的估计和推断之前，建立 LM（经典的“一般”线性模型）的基本结果和基本定理是有帮助的。这就是本章的目的。","code":""},{"path":"chap4.html","id":"sec4-2","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.2 普通最小二乘","text":"经典的 LM 估计使用最小二乘，最小二乘有两种形式：普通最小二乘 (ordinary least squares, OLS) 和广义最小二乘 (generalized least squares, GLS)，有时称为加权最小二乘 (weighted least squares). OLS 最小化了 \\(\\symbf y\\) 的观测值与模型下 \\(\\symbf y\\) 的预测值之差的平方，后者表示为 \\(\\hat{\\symbf y}=\\symbf X\\hat{\\symbf\\beta}\\)。以矩阵表示，通过令导数 \\(\\partial{\\left[\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)^{\\prime}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)\\right]}/\\partial\\symbf{\\beta}\\) 等于零求出解 \\(\\hat{\\symbf\\beta}\\)，OLS 最小化了 \\(\\left(\\symbf{y}-\\symbf{X}\\hat{\\symbf{\\beta}}\\right)^{\\prime}\\left(\\symbf{y}-\\symbf{X}\\hat{\\symbf{\\beta}}\\right)\\)。该导数为\\[\\partial{\\left[\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)^{\\prime}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)\\right]}/\\partial\\symbf{\\beta}=\\left(\\symbf{y}^{\\prime}\\symbf{y}-\\symbf{y}^{\\prime}\\symbf{X}\\symbf{\\beta}-\\symbf{\\beta}^{\\prime}\\symbf{X}^{\\prime}\\symbf{y}+\\symbf{\\beta}^{\\prime}\\symbf{X}^{\\prime}\\symbf{X}\\symbf{\\beta}\\right)/\\partial\\symbf{\\beta}=-2\\symbf{X}^{\\prime}\\symbf{y}+2\\symbf{X}^{\\prime}\\symbf{X}\\symbf{\\beta}=0\\]由此得到 OLS 估计方程 (estimating equation)：\\(\\symbf{X}^{\\prime}\\symbf{X}\\symbf{\\beta}=\\symbf{X}^{\\prime}\\symbf{y}\\)。若 \\(\\symbf X\\) 满秩，我们可利用 \\(\\symbf X'\\symbf X\\) 得到估计 \\(\\hat{\\symbf{\\beta}}=\\left(\\symbf{X}'\\symbf{X}\\right)^{-1}\\symbf{X}'\\symbf{y}\\)。另一方面，若 \\(\\symbf X\\) 不满秩，这是所有 ANOVA 型效应模型的情况，那么我们需要一种修改的方法。在计算机时代之前，\\(\\symbf X\\) 被修改为使用参数约束使其满秩。例如，具有线性预测器 \\(\\mu+\\tau_i\\) 的单向 ANOVA 模型通常使用西格玛约束 (sigma-constraint)，这意味着处理效应和为零，正式写为 \\(\\sum_i\\tau_i=0\\)。约束在简单的 LM 中工作得很好。然而，即使 LM 用于缺失数据的区组设计，约束的实现也不必要地复杂。对于混合模型，Nelder 的描述 “way madness lies” 最好地形容了约束方法的不适用性。我们需要一个更好的方法。","code":""},{"path":"chap4.html","id":"sec4-3","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.3 广义逆和可估函数","text":"当 \\(\\symbf X\\) 不满秩时，计算机时代使用广义逆 (generalized inverse) 方法，通常称为 “g-inverse”. 广义逆定义如下：被称为 Penrose 条件的额外要求通常与广义逆一起用于线性模型估计和推断。Penrose 条件为\\({\\symbf{}^-}\\symbf{}{\\symbf{}^-}={\\symbf{}^-}\\)\\(\\left(\\symbf{}^-\\symbf{}\\right)'=\\symbf{}^-\\symbf{}\\)\\(\\left(\\symbf{}\\symbf{}^-\\right)'=\\symbf{}\\symbf{}^-\\)附录 ??12 包含与广义逆相关的其他结果，这些结果对于本章和接下来的三章所示的估计和推断的推导非常重要。当 \\(\\symbf X\\symbf X'\\) 存在真逆时，解 \\(\\hat{\\symbf{\\beta}}=\\left(\\symbf{X}'\\symbf{X}\\right)^{-1}\\symbf{X}'\\symbf{y}\\) 是唯一的，但当需要广义逆时，解 \\(\\hat{\\symbf{\\beta}}=\\left(\\symbf{X}'\\symbf{X}\\right)^{-1}\\symbf{X}'\\symbf{y}\\) 不是唯一的。遵循 Searle (1971)，我们将 \\(\\symbf\\beta\\) 的解向量表示为 \\(\\tilde{\\symbf{\\beta}}=\\left(\\symbf{X}'\\symbf{X}\\right)^{-1}\\symbf{X}'\\symbf{y}\\)。当 \\(\\symbf X\\) 不满秩时，解不是唯一的，所以 \\(\\tilde{\\symbf{\\beta}}\\) 没有内在的解释。这就是如下定义的可估函数 (estimable function) 的用武之地。换言之，为了使函数可估，我们必须能够将其写成观测期望值的线性组合。可估函数的常见例子是处理均值，例如单向处理设计中的 \\(\\eta+\\tau_i\\)，以及处理差异，例如 \\(\\tau_1-\\tau_2\\)。\\(\\symbf K\\) 矩阵中也可以有多个元素。例如，在具有四种处理的单向处理设计中，\\[\\begin{equation}\n\\symbf{K'\\beta}=\\begin{bmatrix}0&1&0&0&-1\\\\0&0&1&0&-1\\\\0&0&0&1&-1\\end{bmatrix}\\begin{bmatrix}\\eta\\\\\\tau_1\\\\\\tau_2\\\\\\tau_3\\\\\\tau_4\\end{bmatrix}\n\\tag{4.1}\n\\end{equation}\\]用于解决所有处理均值同时相等的问题。请注意，在 LM 中，\\(E(\\symbf y) =\\symbf {X\\beta}\\)，这意味着 \\(\\symbf{K'\\beta} = \\symbf{T'X\\beta}\\)，因此 \\(\\symbf{K'} = \\symbf{T'X}\\)。可估函数的首要重要之处在于，若 \\(\\symbf{K}'\\tilde{\\symbf{\\beta}}\\) 可估，它就是可解释的，即使 \\(\\tilde{\\symbf{\\beta}}\\) 不可解释。原因如下：证明：\\(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}=\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^{-}{\\symbf{X}^{\\prime}\\symbf{y}}=\\symbf{T}^{\\prime}\\symbf{X}(\\symbf{X}'\\symbf{X})^{-}{\\symbf{X}^{\\prime}\\symbf{y}}\\)。附录 ?? 证明了 \\(\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\) 对 \\((\\symbf {X^{\\prime}X})^-\\) 的选择是不变的，从而得证。\n","code":""},{"path":"chap4.html","id":"sec4-4","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.4 最佳线性无偏估计和 OLS","text":"现在我们有了 OLS 估计方程和可估函数，我们将注意力转向 OLS 的性质。具体来说，OLS 有多好？判断估计有两个标准：估计是有偏的吗？估计的方差与其他估计相比如何？在线性模型理论中，如果一个估计是：观测的线性组合无偏方差小于任何其他基于观测的线性组合定义的估计那么它称为最佳线性无偏估计 (Best Linear Unbiased Estimator)，通常使用缩写 BLUE. 与其关注参数向量 \\(\\symbf\\beta\\) 的解，不如关注可估函数的 OLS 解。这是因为 \\(\\symbf K'\\tilde{\\symbf\\beta}\\) 是可解释的，而 \\(\\tilde{\\symbf\\beta}\\) 不是。定理：\\(Var\\left(\\symbf{y}\\right)=\\symbf{}\\sigma^2\\)，也就是说，根据上面 LM 的定义，若 \\(\\boldsymbol\\Sigma = \\symbf \\)，那么 OLS 解 \\(\\symbf{K'}\\tilde{\\symbf{\\beta}}=\\symbf{K}'(\\symbf{X}'\\symbf{X}){\\symbf{X}'}\\symbf{y}\\) 是 \\(\\symbf K'\\symbf\\beta\\) 的 BLUE.证明，第一部分：我们首先证明 \\(\\symbf{K'}\\tilde{\\symbf{\\beta}}\\) 是无偏的。\\(E\\left(\\symbf{K}'\\tilde{\\symbf{\\beta}}\\right)=\\symbf{K}'(\\symbf{X}'\\symbf{X})^-{\\symbf{X}'}E(\\symbf{y})=\\symbf{K'}(\\symbf{X}'\\symbf{X})^{-}\\symbf{X'}\\symbf{X}\\symbf{\\beta}=\\symbf{K}'\\symbf{\\beta}\\)。因此 OLS 估计是无偏的。证明，第二部分：令 \\(\\symbf C\\symbf y'\\) 是 \\(\\symbf K'\\symbf\\beta\\) 的任何其他无偏估计。因此 \\(E\\left(\\symbf{C'\\symbf y}\\right)=\\symbf{K'\\beta}=\\symbf{C^{\\prime}}E(\\symbf{y})=\\symbf{C^{\\prime}}\\symbf{X}\\symbf{\\beta}\\)，因此 \\(\\symbf K'＝\\symbf C'\\symbf X\\)。此外，OLS 估计的方差为\\[\\begin{aligned}\nVar\\left(\\symbf{K^{\\prime}\\tilde{\\beta}}\\right)& =\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^{-}\\symbf{X}^{\\prime}Var(\\symbf{y})\\symbf{X}(\\symbf{X}'\\symbf{X})^{-}\\symbf{K}=\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^{-}\\symbf{X}^{\\prime}(\\symbf{}\\sigma^{2})\\symbf{X}(\\symbf{X}'\\symbf{X})^{-}\\symbf{K}  \\\\\n&=\\symbf{K^{\\prime}}(\\symbf{X}'\\symbf{X})^-{\\symbf{K}}\\sigma^2.\n\\end{aligned}\\]现在\\[\\begin{aligned}&Var\\left(\\symbf{C^{\\prime}y-K^{\\prime}(X'X)^-{X^{\\prime}y}}\\right)\\\\=&\\,Var\\left(\\symbf{C'y}\\right)+Var\\left(\\symbf{K'}\\left(\\symbf{X'X}\\right)^{-}\\symbf{X'y}\\right)-2Cov\\left(\\symbf{C'y},\\symbf{K'}\\left(\\symbf{X'X}\\right)^{-}\\symbf{X'y}\\right)\\end{aligned}\\]注意到 \\(Cov\\left(\\symbf{C}^{\\prime}\\symbf{y},\\symbf{K}^{\\prime}\\left(\\symbf{X}'\\symbf{X}\\right)^-\\symbf{X}^{\\prime}\\symbf{y}\\right)=\\symbf{C}^{\\prime}Var\\left(\\symbf{y}\\right)\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^-\\symbf{K}=\\symbf{C}^{\\prime}\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^-\\symbf{K}\\sigma^{2}\\)回想 \\(\\symbf{K}^{\\prime}=\\symbf{C}^{\\prime}\\symbf{X}\\) 从而 \\(Cov\\Big(\\symbf{C^{\\prime}y},\\symbf{K^{\\prime}}(\\symbf{X^{\\prime}X})^-{\\symbf{X^{\\prime}y}}\\Big)=\\symbf{K^{\\prime}}(\\symbf{X'X})^-{\\symbf{K}}\\sigma^2\\)那么\\[\\begin{aligned}\nVar\\left(\\symbf{C^{\\prime}y-K^{\\prime}(X'X)^-{X^{\\prime}y}}\\right)& =\\symbf{C}^{\\prime}\\symbf{C}\\sigma^2+\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^-\\symbf{K}\\sigma^2-2\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^-\\symbf{K}\\sigma^2  \\\\\n&=\\symbf{C}^{\\prime}\\symbf{C}\\sigma^2-\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^-\\symbf{{K}}\\sigma^2.\n\\end{aligned}\\]因为 \\(Var\\left(\\symbf{C^{\\prime}y-K^{\\prime}(X'X)^-}{\\symbf{X^{\\prime}y}}\\right)\\geq0\\)，所以 \\(Var\\left(\\symbf{C^{\\prime}y}\\right)=\\symbf{C^{\\prime}C}\\sigma^2\\) 必大于等于 OLS 估计的方差 \\(\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^{-}\\symbf{K}\\sigma^2\\)。从而得证。","code":""},{"path":"chap4.html","id":"sec4-5","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.5 广义最小二乘","text":"到目前为止，本章的结果适用于 OLS 以及 \\(Var\\left(\\symbf{y}\\right)=\\symbf{}\\sigma^2\\) 的情况。我们现在问，当 \\(Var\\left(\\symbf{y}\\right)=\\boldsymbol\\Sigma\\sigma^2\\) 且 \\(\\boldsymbol\\Sigma\\ne\\symbf \\) 时会发生什么。两个常见的例子是复合对称和一阶自回归【first-order autoregressive, AR(1)】协方差模型。我们首先问如果使用 OLS 会发生什么。首先，所得估计是无偏的，因为 OLS 估计不依赖于协方差模型。其次，可估函数 OLS 解的方差为\\[\\begin{aligned}Var\\left(\\symbf{K'\\tilde{\\beta}}\\right)&=Var\\left(\\symbf{K'}(\\symbf{X}'\\symbf{X})^-\\symbf{X'}\\symbf{y}\\right)=\\symbf{K'}(\\symbf{X}'\\symbf{X})^-\\symbf{X'}Var\\left(\\symbf{y}\\right)\\symbf{X}(\\symbf{X}'\\symbf{X})^-\\symbf{K}\\\\&=\\symbf{K'}(\\symbf{X}'\\symbf{X})^-{\\symbf{X'}}\\boldsymbol\\Sigma\\symbf{X}(\\symbf{X'X})^-{\\symbf{K}}\\sigma^2\\end{aligned}\\]换句话说，我们有一个无偏估计，但这是我们能做到的最好的吗？答案是否定的。通过利用来自协方差阵 \\(\\boldsymbol\\Sigma\\) 的信息的估计，我们可以做得更好。这称为广义最小二乘 (generalized least squares, GLS).为了开发 GLS 估计并理解它何时为 BLUE，我们需要矩阵代数的结果。具体地，对于矩阵 \\(\\boldsymbol\\Sigma\\)，存在一个矩阵 \\(\\symbf \\)，使得 \\(\\symbf '\\symbf =\\boldsymbol \\Sigma^-\\)。从而 \\(\\symbf{}'\\symbf{}\\boldsymbol\\Sigma\\symbf{}'=\\boldsymbol\\Sigma^{-1}\\boldsymbol\\Sigma\\symbf{}'=\\symbf{}'\\) 那么有 \\(\\symbf{}\\boldsymbol\\Sigma\\symbf{}^{\\prime}=\\symbf{}\\)。因此，如果我们有一个观测向量 \\(\\symbf y\\)，使得 \\(E\\left(\\symbf{y}\\right)=\\symbf{\\mu}\\) 以及 \\(Var(\\symbf{y})=\\boldsymbol\\Sigma\\sigma^2\\)。现在考虑向量 \\(\\symbf w =\\symbf \\symbf y\\)。\\(\\symbf w\\) 的期望和方差分别为 \\(E(\\symbf w )=\\symbf \\symbf\\mu\\) 以及 \\(Var(\\symbf w)=\\symbf \\boldsymbol\\Sigma\\symbf '\\sigma^2=\\symbf \\sigma^2\\)。现在假设我们想要拟合线性模型 \\(\\symbf{X\\beta}\\) 来估计 \\(\\symbf\\mu\\)。等价地，我们可以使用线性预测器 \\(\\symbf {AX\\beta}\\) 来估计 \\(\\symbf {\\mu}\\)。令 \\(\\symbf X_w=\\symbf {AX}\\)。\\(\\symbf w\\) 的方差告诉我们 OLS 估计 \\(\\tilde{\\symbf{\\beta}}=\\left(\\symbf{X}_w^{\\prime}\\symbf{X}_w\\right)^-{\\symbf{X}}_w^{\\prime}\\symbf{w}\\) 是 BLUE. 用实际响应变量 \\(\\symbf y\\) 重写估计方程，我们得到 \\({(\\symbf{X}^{\\prime}\\symbf{}^{\\prime}\\symbf{}\\symbf{X})^-}{\\symbf{X}^{\\prime}\\symbf{}^{\\prime}\\symbf{}\\symbf{y}}=\\left(\\symbf{X}^{\\prime}\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^-{\\symbf{X}^{\\prime}}\\boldsymbol\\Sigma^{-1}\\symbf{y}\\)。以线性模型术语，\\(\\tilde{\\symbf{\\beta}}=\\left(\\symbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X}\\right)^-{\\symbf{X}'}\\boldsymbol\\Sigma^{-1}\\symbf{y}\\) 称为 GLS 估计方程。这个推导表明，当 \\(Var\\left(\\symbf{y}\\right)=\\boldsymbol\\Sigma\\sigma^2\\) 时，GLS 解是 BLUE，或者更准确地说，可估函数 \\(\\symbf{K'\\beta}\\) 的 GLS 估计是 BLUE.","code":""},{"path":"chap4.html","id":"sec4-6","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.6 OLS 和 GLS BLUE 的推断","text":"一旦我们有了感兴趣的参数向量和可估函数的解，我们该如何处理它们？这使我们从估计转向推断。正如本教科书中所理解的，推断是指假设检验和置信区间（或者，后文中的预测和置信区间）。让我们从最简单的、可用于统计推断的可解释项开始，即可估函数 \\(\\symbf {k'\\beta}\\)，其中 \\(\\symbf k\\) 是一个向量。常见的例子包括处理均值、处理差异、对比、来自回归模型的预测值等。","code":""},{"path":"chap4.html","id":"sec4-6-1","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.6.1 方差已知时的检验统计量","text":"让我们从最简单的估计方法—— OLS ——开始。估计及其方差分别为 \\(\\symbf{k'(X'X)^-X'y}\\) 和 \\(\\symbf{k'(X'X)^-k}\\sigma^2\\)。对于原假设 \\(H_0\\colon\\symbf {k'\\beta}=0\\)（或其单侧版本），检验统计量是估计与标准误之比（或比值的平方），此时为\\[\\symbf{k^{\\prime}(X'X)^-X^{\\prime}y}\\Big/\\sqrt{\\symbf{k^{\\prime}(X^{\\prime}X)^-k}\\sigma^2}\\]如果 \\(\\sigma^2\\) 已知，则将其称为 \\(Z\\) 统计量。该统计量的平方，以矩阵形式表达为\\[\\begin{equation}\n\\symbf{y}^{\\prime}\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^-{\\symbf{k}}\\left[\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^-{\\symbf{k}}\\right]^{-1}\\symbf{k}^{\\prime}\\left(\\symbf{X}'\\symbf{X}\\right)^-{\\symbf{X}}^{\\prime}\\symbf{y}/\\sigma^2\n\\tag{4.2}\n\\end{equation}\\]这是我们对一种称为二次型的矩阵的介绍。如果 \\(\\symbf y\\) 是一个随机向量，那么可写为 \\(\\symbf{yA'y}\\) 的矩阵就称为二次型 (quadratic form). 这里我们发现\\[\\begin{equation}\n\\symbf{}=\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{k}{\\left[\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{k}\\right]}^{-1}\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\big/\\sigma^2\n\\tag{4.3}\n\\end{equation}\\]若检验统计量为具有已知方差的二次型，则称其为 Wald 统计量。如果 \\(\\symbf k\\) 是一个向量，那么 \\(Z\\) 和 Wald 统计量同样有用。如果 \\(\\symbf K\\) 是一个矩阵，例如，式 (4.1) 所示的用于同时相等性的可估函数，则必须使用 Wald 统计量，在式 (4.2) 和 (4.3) 中用 \\(\\symbf K\\) 替换 \\(\\symbf k\\)。","code":""},{"path":"chap4.html","id":"sec4-6-2","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.6.2 方差未知时的检验统计量","text":"\\(Z\\) 和 Wald 统计量都是用已知的 \\(\\sigma^2\\) 定义的。在大多数实际数据分析中，\\(\\sigma^2\\) 是未知的，必须进行估计。对于 LM 的 OLS 估计，我们可以使用观测向量 \\(\\symbf y\\) 和预测观测向量 \\(\\hat{\\symbf{y}}=\\symbf{X}\\hat{\\symbf{\\beta}}=\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\symbf{y}\\) 之差的平方和来估计 \\(\\sigma^2\\)。这称为残差平方和 (sums squares residual)，通常缩写为 SSR，写为\n\\(SSR=\\left(\\symbf{y}-\\hat{\\symbf{y}}\\right)^{\\prime}\\left(\\symbf{y}-\\hat{\\symbf{y}}\\right)=\\symbf{y}^{\\prime}\\symbf{y}-\\symbf{y}^{\\prime}\\hat{\\symbf{y}}-\\hat{\\symbf{y}}^{\\prime}\\symbf{y}+\\hat{\\symbf{y}}^{\\prime}\\hat{\\symbf{y}}=\\symbf{y}^{\\prime}\\symbf{y}-\\symbf{y}^{\\prime}\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\symbf{y}\\)。项 \\(\\symbf{y'y}\\) 在线性模型理论中称为未校正总平方和 (uncorrected total sum squares)，\n\\(\\symbf{y}^{\\prime}\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\symbf{y}\\) 称为模型平方和 (sum squares model). 因此，\\(SSR=SS(\\text{total})-SS(\\text{model})\\)。\\(SSR\\) 也可写为\n\\(\\symbf{y'}\\left[\\symbf{-X}(\\symbf{X'X})^{-}{\\symbf{X'}}\\right]\\symbf{y}\\)，表明它是一个二次型，其中 \\(\\symbf{}=\\symbf{}-\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\)。为完成估计 \\(\\sigma^2\\) 的任务，我们必须确定 SSR 的期望值。为此，我们首先需要获得二次型期望值的一般结果。首先，\\(E\\left(\\symbf{y'Ay}\\right)=E\\left[\\operatorname{trace}\\left(\\symbf{y'Ay}\\right)\\right]=E\\left[\\operatorname{trace}\\left(\\symbf{Ayy'}\\right)\\right]=\\operatorname{trace}\\left[\\symbf{}E\\left(\\symbf{yy'}\\right)\\right]\\)其次，\\(Var\\left(\\symbf{y}\\right)=\\symbf{V}=E\\left[\\left(\\symbf{y}-\\symbf{\\mu}\\right)\\left(\\symbf{y}-\\symbf{\\mu}\\right)'\\right]=E\\left(\\symbf{y}\\symbf{y}'\\right)-\\symbf{\\mu}\\symbf{\\mu}'\\) 从而 \\(E\\left(\\symbf{y}\\symbf{y}'\\right)=\\symbf{V}+\\symbf{\\mu}\\symbf{\\mu}'\\)最后，\\(\\operatorname{trace}\\left[\\symbf{}E\\left(\\symbf{y}\\symbf{y'}\\right)\\right]=\\operatorname{trace}\\left[\\symbf{}\\left(\\symbf{V}+\\symbf{\\mu}\\symbf{\\mu'}\\right)\\right]=\\operatorname{trace}\\left(\\symbf{}\\symbf{V}\\right)+\\symbf{\\mu'}\\symbf{\\mu}=E\\left(\\symbf{y'}\\symbf{y}\\right)\\)使用该结果，以及 \\(\\symbf{V}=Var\\left(\\symbf{y}\\right)=\\symbf{}\\sigma^2\\) 和 \\(\\symbf \\mu=\\symbf{X\\beta}\\)，那么\\[\\begin{aligned}E(SSR)&=E\\left[\\symbf y^{\\prime}(\\symbf{}-\\symbf{X}(\\symbf{X}'\\symbf{X}\\right)^-{\\symbf{X}}^{\\prime})\\symbf{y}]\\\\&=\\operatorname{trace}\\left[\\left(\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right)\\symbf{V}\\right]\\\\&\\quad+\\symbf{\\mu}^{\\prime}\\left(\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right)\\symbf{\\mu}\\\\&=\\operatorname{trace}\\left[\\left(\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right)\\symbf{}\\sigma^{2}\\right]+\\symbf{\\beta}^{\\prime}\\symbf{X}^{\\prime}\\\\&=\\left(\\symbf{}-\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\right)\\symbf{X}\\symbf{\\beta}\\\\&=\\sigma^2\\left[N-\\operatorname{rank}(\\symbf{X})\\right]\\end{aligned}\\]其中 \\(N\\) 表示观测总数。因此，\\(\\sigma^2\\) 的估计为 \\(\\hat{\\sigma}^2=SSR/\\left[N-\\operatorname{rank}(\\symbf{X})\\right]\\)。回到 Wald 统计量，如果我们用方差估计替换 \\(\\sigma^2\\)，我们现在有 \\(\\symbf{y}^{\\prime}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^-{\\symbf{K}}\\left[\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-{\\symbf{K}}\\right]^{-1}\\symbf{K}^{\\prime}(\\symbf{X}\\symbf{X})^-{\\symbf{X}}^{\\prime}\\symbf{y}\\Big/\\hat{{\\sigma}}^2\\)。将其与经典 ANOVA 相结合，Wald 统计量的分子称为假设平方和 (sum squares hypothesis)，缩写为 SSH. 我们将 \\(\\hat{\\sigma}^2=SSR\\Big/\\left[N-\\operatorname{rank}(\\mathbf{X})\\right]\\) 视为残差均方 (mean square residual)，即 MSR. 因此，具有方差估计的基于 Wald 的统计量为 \\(SSH/MSR\\)。由 \\(\\symbf K\\) 指定的假设均方为 \\(MSH＝SSH/\\operatorname{rank}(\\symbf K)\\)。因此，Wald 统计量除以 \\(\\operatorname{rank}(\\symbf K)\\) 得到了经典的 \\(F\\) 统计量，即 \\(MSH/MSR\\)。","code":""},{"path":"chap4.html","id":"sec4-6-3","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.6.3 检验统计量的分布","text":"此时，有理由询问这些统计量是如何分布的。为了对它们所检验的假设得出结论，并且为了利用它们构建置信区间，我们需要这样做。关于检验统计量的分布，我们可以使用两种方法。首先，我们可以进行随机化检验 (randomization test)，这就是 Fisher 最初所做的，其优点是我们不需要对观测的分布做出假定。或者，我们可以对 \\(\\symbf y\\) 的分布做出假定，这是现代线性模型理论中几乎普遍采用的方法。我们现在采用后者，具体来说是遵循基于高斯分布的经典线性模型理论。","code":""},{"path":"chap4.html","id":"sec4-7","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"4.7 基于高斯的二次型分布理论","text":"到目前为止的所有结果都不需要对观测的分布做出假定。在没有任何此类假定的情况下，Fisher 使用对 \\(MSH/MSR\\) 定义的随机化检验来导出 \\(F\\) 分布。因此，\\(MSH/MSR\\) 称为 \\(F\\) 比。注意，\\(F\\) 比等于基于 Wald 的统计量（用 MSR 替换 \\(\\sigma^2\\)）除以 \\(\\operatorname{rank}(\\symbf X)\\)。尽管可以出于假设检验的目的来计算 \\(F\\) 比，但假设检验本身只能告诉我们可估函数是否与指定的目标量不同；它没有告诉我们有多大差异，也没有告诉我们从实际角度来看这种差异是否重要。为此，我们需要一种基于分布的方法来构建置信区间。更重要的是，我们需要一种可以推广到不满足 OLS 甚至 GLS 估计所需假定的数据的方法。二次型的经典分布理论始于假定 \\(\\symbf{y}\\thicksim N(\\symbf{\\mu},\\symbf{V})\\)。在本节中，我们进一步假定 OLS 条件为 \\(\\symbf{V}=\\symbf{}\\sigma^2\\)。以下结果建立了与 LM 相关的检验统计量的分布性质。对于 \\(\\symbf{y}\\thicksim N(\\symbf{\\mu},\\symbf{V})\\)，二次型 \\(\\symbf{y^{\\prime}Ay}\\sim\\chi_{\\operatorname{rank}(\\symbf ),\\symbf{\\mu^{\\prime}\\mu}}^2\\) 当且仅当 \\(\\symbf{AV}\\) 是幂等的（即 \\(\\symbf{AVAV}=\\symbf{AV}\\)）。\\(\\symbf{\\mu^{\\prime}\\mu}\\) 称为非中心参数 (non-centrality parameter).对于 \\(\\symbf{y}\\thicksim N(\\symbf{\\mu},\\symbf{V})\\)，二次型 \\(\\symbf{y^{\\prime}Ay}\\sim\\chi_{\\operatorname{rank}(\\symbf ),\\symbf{\\mu^{\\prime}\\mu}}^2\\) 当且仅当 \\(\\symbf{AV}\\) 是幂等的（即 \\(\\symbf{AVAV}=\\symbf{AV}\\)）。\\(\\symbf{\\mu^{\\prime}\\mu}\\) 称为非中心参数 (non-centrality parameter).若 \\(\\symbf{\\mu^{\\prime}\\mu}=0\\)，则二次型具有中心 \\(\\chi^2\\) 分布，否则具有非中心 \\(\\chi^2\\) 分布。符号 \\(\\chi^2_{{\\operatorname{rank}(\\symbf K)}}\\)（即只给出自由度，而没有给出非中心参数时）指的是中心 \\(\\chi^2\\) 分布。若 \\(\\symbf{\\mu^{\\prime}\\mu}=0\\)，则二次型具有中心 \\(\\chi^2\\) 分布，否则具有非中心 \\(\\chi^2\\) 分布。符号 \\(\\chi^2_{{\\operatorname{rank}(\\symbf K)}}\\)（即只给出自由度，而没有给出非中心参数时）指的是中心 \\(\\chi^2\\) 分布。当 \\(\\symbf{AVB}=0\\) 时，二次型 \\(\\symbf{yA'y}\\) 和 \\(\\symbf{yB'y}\\) 是独立的。当 \\(\\symbf{AVB}=0\\) 时，二次型 \\(\\symbf{yA'y}\\) 和 \\(\\symbf{yB'y}\\) 是独立的。当 \\(\\symbf{AVB}=0\\) 时，二次型 \\(\\symbf{yA'y}\\) 和 \\(\\symbf{B'y}\\) 是独立的。当 \\(\\symbf{AVB}=0\\) 时，二次型 \\(\\symbf{yA'y}\\) 和 \\(\\symbf{B'y}\\) 是独立的。若 \\(Z\\thicksim N(0,1)\\) 以及 \\(\\chi^2\\thicksim \\chi^2_\\nu\\)，则 \\(t=Z\\big/\\sqrt{X/\\nu}\\) 具有 \\(\\nu\\) 个自由度的 \\(t\\) 分布。若 \\(Z\\thicksim N(0,1)\\) 以及 \\(\\chi^2\\thicksim \\chi^2_\\nu\\)，则 \\(t=Z\\big/\\sqrt{X/\\nu}\\) 具有 \\(\\nu\\) 个自由度的 \\(t\\) 分布。若 \\(X_1\\sim\\chi_{\\nu_1,\\varphi_1}^2,\\; X_2\\sim\\chi_{\\nu_2}^2\\)（即 \\(X_2\\) 为中心 \\(\\chi^2\\) 分布）并且 \\(X_1,X_2\\) 独立，那么 \\(F=(X_1/\\nu_1)\\big/(X_2/\\nu_2)\\) 为具有 \\((\\nu_1,\\nu_2)\\) 个自由度且非中心参数为 \\(\\varphi_1\\) 的非中心 \\(F\\) 分布。若 \\(X_1\\sim\\chi_{\\nu_1,\\varphi_1}^2,\\; X_2\\sim\\chi_{\\nu_2}^2\\)（即 \\(X_2\\) 为中心 \\(\\chi^2\\) 分布）并且 \\(X_1,X_2\\) 独立，那么 \\(F=(X_1/\\nu_1)\\big/(X_2/\\nu_2)\\) 为具有 \\((\\nu_1,\\nu_2)\\) 个自由度且非中心参数为 \\(\\varphi_1\\) 的非中心 \\(F\\) 分布。这里不加证明地给出了这些结果。证明可以在附录 ?? 或经典线性模型文本中找到，例如 Searle (1971).例如，考虑用于估计 \\(\\sigma^2\\) 的残差平方和 SSR. 假定 \\(\\symbf V=\\symbf \\sigma^2\\) 则 \\(SSR/\\sigma^2\\sim \\chi^2_{N-\\operatorname{rank}(\\symbf X)}\\)。要证明这一点，利用二次型 \\(SSR/\\sigma^2=\\symbf{y}\\left\\{\\left[\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right]\\Big/\\sigma^2\\right\\}\\symbf{y}^{\\prime}\\)，从而 \\(\\left[\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right]\\Big/\\sigma^2\\) 以及 \\(\\symbf{AV}=\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\)。现在，\\[\\begin{aligned}\\symbf{AVAV}&=\\left[\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right]\\left[\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right]\\\\&=\\symbf{-X}(\\symbf{X}^{\\prime}\\symbf{X})^{\\prime}\\symbf{X}^{\\prime}-\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{\\prime}\\symbf{X}^{\\prime}+\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\\\&=\\symbf{}-\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\\\&=\\symbf{}\\symbf{V}\\end{aligned}\\]也就是说，\\(\\symbf{AV}\\) 是幂等的，从而证明了 \\(SSR/\\sigma^2\\sim\\chi^2_{\\operatorname{rank}(\\symbf )}\\)。回想 \\({\\operatorname{rank}(\\symbf )}=N-{\\operatorname{rank}(\\symbf X)}\\) 完成证明。继续本例，考虑用于检验 \\(H_0\\colon\\symbf {k'\\beta}=0\\) 的 \\(t\\) 统计量\\[t=\\symbf{k}^{\\prime}\\tilde{\\symbf{\\beta}}/\\sqrt{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\hat{\\symbf{\\sigma}}^{2}}=\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\symbf{y}/\\sqrt{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\left[\\left(\\symbf{}-\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\right)/\\left(N-\\operatorname{rank}\\left(\\symbf{X}\\right)\\right)\\right]}\\]可写为\\[\\frac{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{\\prime}\\symbf{X}^{\\prime}\\symbf{y}\\Big/\\sqrt{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\sigma^{2}}}{\\sqrt{\\left[\\left(1/\\sigma^2\\right)\\left(\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\right)\\Big/\\left(N-\\operatorname{rank}\\left(\\symbf{X}\\right)\\right)\\right]}}\\]假定 \\(\\symbf y\\sim N(\\symbf{X\\beta},\\symbf \\sigma^2)\\)，\\(\\symbf{k'\\beta}\\) 分布于 \\(N\\left(\\symbf{k}^{\\prime}\\symbf{\\beta},\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\sigma^2\\right)\\)，这在 \\(H_0\\) 下简化为 \\(N\\left(0,\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\sigma^2\\right)\\)。那么 \\(t\\) 统计量的分母 \\[\\sqrt{\\left[\\left(1/\\sigma^2\\right)\\left(\\symbf{}-\\symbf{X}\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^-\\symbf X'\\right)\\Big/\\left(N-\\operatorname{rank}\\left(\\symbf{X}\\right)\\right)\\right]}\\] 为 \\(\\chi^2_\\nu/\\nu\\)，其中 \\(\\nu=N-\\operatorname{rank}(\\symbf X)\\)。我们可以通过证明 \\(\\symbf{AVB}=\\symbf 0\\) 来证明分子和分母是独立的，其中 \\(\\symbf{}=\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\) 以及 \\(\\symbf{B}=\\symbf{}-\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}/\\sigma^{2}\\)。因此，\\[\\begin{aligned}\\symbf{}\\symbf{V}\\symbf{B}&=\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}(\\symbf{}\\sigma^2)(\\symbf{}-\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime})/\\sigma^2\\\\&=\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}-\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\\\&=\\symbf 0\\end{aligned}\\]因此 \\(t\\) 统计量 \\(\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\symbf{y}\\Big/\\sqrt{SSR/\\left[N-\\operatorname{rank}\\left(\\symbf{X}\\right)\\right]}\\) 为【标准正态随机变量】与【\\(\\chi^2\\) 随机变量除以其自由度 的平方根】之比，其中两随机变量独立。因此，该比具有 \\(N-\\operatorname{rank}(\\symbf X)\\) 个自由度的 \\(t\\) 分布。利用这些结果，可以很容易地建立 Wald 统计量（\\(\\chi^2\\)）和 \\(F\\) 比的分布性质。此外，根据 \\(t\\) 统计量很容易推导出可估函数的标准置信区间公式 \\(\\symbf{k}^{\\prime}\\tilde{\\symbf{\\beta}}\\pm t_{\\nu,\\alpha}\\times\\sqrt{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\hat{\\sigma}^{2}}\\)。这些结果将作为练习。","code":""},{"path":"chap4.html","id":"exe4","chapter":"第 4 章 GLMM 之前的估计和推断基础知识","heading":"练习","text":"对于下列练习，假定 \\(\\symbf{y}\\sim N\\left(\\symbf{X}\\symbf{\\beta},\\symbf{}\\sigma^2\\right)\\)推导出可估函数的标准置信区间公式 \\(\\symbf{k}^{\\prime}\\tilde{\\symbf{\\beta}}\\pm t_{\\nu,\\alpha}\\times\\sqrt{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\hat{\\sigma}^{2}}\\)推导出可估函数的标准置信区间公式 \\(\\symbf{k}^{\\prime}\\tilde{\\symbf{\\beta}}\\pm t_{\\nu,\\alpha}\\times\\sqrt{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{k}\\hat{\\sigma}^{2}}\\)证明 Wald 统计量 \\(\\symbf{y}^{\\prime}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^-{\\symbf{K}}\\left[\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-{\\symbf{K}}\\right]^{-1}\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^-{\\symbf{X}}^{\\prime}\\symbf{y}/\\sigma^2\\) 具有 \\(\\chi^2_{\\operatorname{rank}(\\symbf K)}\\) 分布。证明 Wald 统计量 \\(\\symbf{y}^{\\prime}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^-{\\symbf{K}}\\left[\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-{\\symbf{K}}\\right]^{-1}\\symbf{K}^{\\prime}(\\symbf{X}'\\symbf{X})^-{\\symbf{X}}^{\\prime}\\symbf{y}/\\sigma^2\\) 具有 \\(\\chi^2_{\\operatorname{rank}(\\symbf K)}\\) 分布。证明具有估计方差的 Wald 统计量除以 \\(\\operatorname{rank}(\\symbf K)\\)，即 \\(\\left\\{\\symbf{y}^{\\prime}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{K}\\left[\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}(\\symbf{X}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\symbf{y}\\big/\\hat{\\sigma}^{2}\\right\\}\\Big/\\operatorname{rank}(\\symbf{K})\\)，具有 \\(F_{(\\nu_1,\\nu_2)}\\) 分布，其中 \\(\\nu_1=\\operatorname{rank}(\\symbf K),\\nu_2=N-\\operatorname{rank}(\\symbf X)\\)证明具有估计方差的 Wald 统计量除以 \\(\\operatorname{rank}(\\symbf K)\\)，即 \\(\\left\\{\\symbf{y}^{\\prime}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{K}\\left[\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}(\\symbf{X}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\symbf{y}\\big/\\hat{\\sigma}^{2}\\right\\}\\Big/\\operatorname{rank}(\\symbf{K})\\)，具有 \\(F_{(\\nu_1,\\nu_2)}\\) 分布，其中 \\(\\nu_1=\\operatorname{rank}(\\symbf K),\\nu_2=N-\\operatorname{rank}(\\symbf X)\\)对于下列练习，假定一个具有四种处理的单向处理设计的线性模型，\\(\\eta+\\tau_i,=1,2,3,4\\)。为每个函数写出隐含的 \\(\\symbf{k'\\beta}\\)，并说明它是可估还是不可估的。\\(\\eta\\)\\(\\eta\\)\\(2\\tau_1-\\tau_2-\\tau_3\\)\\(2\\tau_1-\\tau_2-\\tau_3\\)\\(\\left(\\frac{1}{2}\\right)\\left(\\tau_1+\\tau_2\\right)-\\left(\\frac{1}{2}\\right)\\left(\\tau_3+\\tau_4\\right)\\)\\(\\left(\\frac{1}{2}\\right)\\left(\\tau_1+\\tau_2\\right)-\\left(\\frac{1}{2}\\right)\\left(\\tau_3+\\tau_4\\right)\\)\\(\\eta+\\left(\\frac14\\right)\\sum_{=1}^4\\tau_i\\)\\(\\eta+\\left(\\frac14\\right)\\sum_{=1}^4\\tau_i\\)\\(2\\eta+\\tau_2+\\tau_3\\)\\(2\\eta+\\tau_2+\\tau_3\\)\\(\\sum_{=1}^4\\tau_i\\)\\(\\sum_{=1}^4\\tau_i\\)\\(\\tau_1-\\tau_2-\\tau_3\\)\\(\\tau_1-\\tau_2-\\tau_3\\)","code":""},{"path":"chap5.html","id":"chap5","chapter":"第 5 章 GLMM 估计","heading":"第 5 章 GLMM 估计","text":"","code":""},{"path":"chap5.html","id":"sec5-1","chapter":"第 5 章 GLMM 估计","heading":"5.1 介绍","text":"在第 4 章中，我们介绍了经典 LM 基本的估计和推断概念。在接下来的三章中，我们将基于这些概念介绍 GLMM 的估计和推断理论和方法。本章的重点是估计。回想，一个完全指定的线性模型包括线性预测器 \\(\\symbf{X\\beta}+\\symbf{Zb}\\)，或简单地为 \\(\\symbf{X\\beta}\\)（如果没有随机效应）取条件于随机效应的观测 \\(\\symbf y\\mid \\symbf b\\) 的分布（如果存在随机效应）连接函数 \\(\\symbf\\eta=g(\\symbf\\mu\\mid \\symbf b)\\)，其中 \\(\\symbf\\mu\\mid \\symbf b=E(\\symbf y\\mid \\symbf b)\\)随机效应分布 \\(\\symbf b\\sim N(\\symbf 0,\\symbf G)\\)我们的任务包括估计 \\(\\symbf\\beta\\)，\\(Var(\\symbf{y}\\mid\\symbf{b})\\) 的任何不依赖于 \\(\\symbf\\mu\\mid \\symbf b\\) 的分量，如果存在随机效应，则还需估计 \\(\\symbf b\\) 和 \\(\\symbf G\\)。与基于最小二乘的经典线性模型理论不同，广义线性混合模型估计使用最大似然。我们一次进行一项估计，从仅固定效应模型的 \\(\\symbf\\beta\\) 开始，然后是仅高斯模型的 \\(\\symbf b\\) 及其方差和协方差分量，最后将其集成到成熟的 GLMM 估计程序中。在这次旅途中，在每步之前我们都会介绍基本背景；在每步之后，我们将暂停以展示基于似然的 GLMM 估计与经典的基于最小二乘的估计之间的关系。","code":""},{"path":"chap5.html","id":"sec5-2","chapter":"第 5 章 GLMM 估计","heading":"5.2 基本背景","text":"Nelder Wedderburn (1972) 在其基础广义线性模型论文中，将固定效应线性模型从假定具有高斯分布的响应变量扩展到属于指数族的响应变量。我们的第一站将是指数族。然后我们回顾最大似然估计的基本原理，包括 Newton-Raphson 和 Fisher 得分算法。这些方法扩展至拟似然，即使不能指定全似然，也可以指定期望和方差，从而允许使用广义线性模型来处理响应变量。","code":""},{"path":"chap5.html","id":"sec5-2-1","chapter":"第 5 章 GLMM 估计","heading":"5.2.1 指数族","text":"考虑一个随机变量 \\(Y\\)，我们用 \\(E(Y)=\\mu\\) 表示其期望值，用 \\(Var(Y)=\\sigma^2\\) 表示其方差。正如许多数理统计学教材所介绍的那样，如果 \\(Y\\) 的 p.d.f. 可写为\\[f\\begin{pmatrix}y\\mid\\theta\\end{pmatrix}=m(y)r\\begin{pmatrix}\\theta\\end{pmatrix}e^{s(\\theta)t(y)}\\]那么其分布就被认为是指数族 (exponential family) 的一员。其中 \\(\\theta\\) 表示典型参数 (canonical parameter). 遵循 Nelder 等人，这可以更方便地写为\\[f\\left(y\\mid\\theta\\right)=\\exp\\left[\\frac{y\\theta-b\\left(\\theta\\right)}{\\left(\\phi\\right)}+c\\left(y,\\phi\\right)\\right]\\]或\\[\\begin{equation}\n\\log\\left[f\\left(y\\mid\\theta\\right)\\right]=\\ell\\left(\\theta;y,\\phi\\right)=\\frac{y\\theta-b\\left(\\theta\\right)}{\\left(\\phi\\right)}+c\\left(y,\\phi\\right)\n\\tag{5.1}\n\\end{equation}\\]其中 \\(\\phi\\) 表示尺度参数 (scale parameter). 这种形式的优点是明确考虑了尺度参数，并可以很自然地适用于最大似然估计。例如，考虑我们在第 1 章至第 3 章中看到的高斯、二项和泊松概率密度/分布函数：高斯：\\(\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}\\left(y-\\mu\\right)^2}\\)高斯：\\(\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}\\left(y-\\mu\\right)^2}\\)二项：\\(\\binom nyp^y\\left(1-p\\right)^{n-y}\\)二项：\\(\\binom nyp^y\\left(1-p\\right)^{n-y}\\)泊松：\\(\\frac{e^{-\\lambda}\\lambda^y}{y!}\\)泊松：\\(\\frac{e^{-\\lambda}\\lambda^y}{y!}\\)它们的对数似然分别为高斯：\\[-\\log\\left(\\sigma\\sqrt{2\\pi}\\right)-\\frac{y^2-2y\\mu+\\mu^2}{2\\sigma^2}=\\frac{y\\mu-\\left(\\mu^2/2\\right)}{\\sigma^2}-\\left\\{\\frac{y^2}{2\\sigma^2}+\\log\\left(\\sigma\\sqrt{2\\pi}\\right)\\right\\}\\]高斯：\\[-\\log\\left(\\sigma\\sqrt{2\\pi}\\right)-\\frac{y^2-2y\\mu+\\mu^2}{2\\sigma^2}=\\frac{y\\mu-\\left(\\mu^2/2\\right)}{\\sigma^2}-\\left\\{\\frac{y^2}{2\\sigma^2}+\\log\\left(\\sigma\\sqrt{2\\pi}\\right)\\right\\}\\]二项：\\[\\log\\binom{n}{y}+y\\log(p)+(n-y)\\log(1-p)=y\\log\\left(\\frac{p}{1-p}\\right)+n\\log\\left(1-p\\right)+\\log\\binom n y\\]二项：\\[\\log\\binom{n}{y}+y\\log(p)+(n-y)\\log(1-p)=y\\log\\left(\\frac{p}{1-p}\\right)+n\\log\\left(1-p\\right)+\\log\\binom n y\\]泊松：\\[-\\lambda+y\\log\\left(\\lambda\\right)-\\log\\left(y!\\right)=y\\log\\left(\\lambda\\right)-\\lambda-\\log\\left(y!\\right)\\]泊松：\\[-\\lambda+y\\log\\left(\\lambda\\right)-\\log\\left(y!\\right)=y\\log\\left(\\lambda\\right)-\\lambda-\\log\\left(y!\\right)\\]根据对数似然可看出，通过识别其中的元素，这些分布都属于指数族。我们在这本书中将遇到的大多数分布，包括负二项、多项和伽马分布，都可以以类似的方法表示。有些分布（例如贝塔分布）也可表明属于指数族，只是必须使用标准数理统计教材中的表示形式，而不是 (5.1) 中所示的较简单的形式。重要提示：典型参数始终是分布的期望值的函数。高斯分布、二项分布和泊松分布是这样的分布的例子：它们的典型参数恰好是涉及这些分布的广义线性模型的标准连接函数。","code":""},{"path":"chap5.html","id":"基本术语和结果","chapter":"第 5 章 GLMM 估计","heading":"基本术语和结果","text":"指数族的如下性质对于估计和推断是至关重要的。得分函数 (score function)，记作 \\(S(θ)\\)，定义为对数似然的导数，即\\[S\\left(\\theta\\right)=\\frac{\\partial\\ell\\left(\\theta;y,\\phi\\right)}{\\partial\\theta}\\]得分函数的方差称为信息 (information)，记为 \\(Var\\left[S\\left(\\theta\\right)\\right]=\\mathcal J\\left(\\theta\\right)\\)。得分函数具有一个重要的性质，即它的期望值为零。以下是几个重要的结果：\\[\\begin{equation}\nE(Y)=\\mu=\\frac{\\partial b(\\theta)}{\\partial\\theta}\n\\tag{5.2}\n\\end{equation}\\]由于 \\(E{\\left[S\\left(\\theta\\right)\\right]}=E{\\left[\\frac{y-\\left(\\partial b\\left(\\theta\\right)/\\partial\\theta\\right)}{\\left(\\phi\\right)}\\right]}=0\\) 因此 \\(E(y)-(\\partial b(\\theta)/\\partial\\theta)=0\\) 从而得证。\\[\\begin{equation}\nE\\binom{\\partial S}{\\partial\\theta}=-E[S(\\theta)]^2\n\\tag{5.3}\n\\end{equation}\\]根据定义，\\[E\\left(\\frac{\\partial S}{\\partial\\theta}\\right)=E\\left(\\frac{\\partial^2\\log f\\left(y;\\theta\\right)}{\\partial\\theta^2}\\right)=\\int\\left[\\partial\\left(\\frac{\\partial\\log f\\left(y;\\theta\\right)}{\\partial\\theta}\\right)\\bigg/\\partial\\theta\\right]f\\left(y;\\theta\\right)dy\\]这可写为\\[\\int\\left[\\partial\\left(\\frac{\\partial f\\left(y;\\theta\\right)/\\partial\\theta}{f\\left(y;\\theta\\right)}\\right)\\bigg/\\partial\\theta\\right]f\\left(y;\\theta\\right)dy=\\int-\\frac{\\partial f\\left(y;\\theta\\right)/\\partial\\theta}{f\\left(y;\\theta\\right)f\\left(y;\\theta\\right)}\\;\\;\\partial f\\left(y;\\theta\\right)/\\partial\\theta\\;\\;f\\left(y;\\theta\\right)dy\\]重写后得到\\[\\int-\\left(\\frac{\\partial f\\left(y;\\theta\\right)/\\partial\\theta}{f\\left(y;\\theta\\right)}\\right)\\left(\\frac{\\partial f\\left(y;\\theta\\right)/\\partial\\theta}{f\\left(y;\\theta\\right)}\\right)f\\left(y;\\theta\\right)dy\\]或等价地\\[-\\int\\biggl(\\frac{\\partial\\log f\\left(y;\\theta\\right)}{\\partial\\theta}\\biggr)\\biggl(\\frac{\\partial\\log f\\left(y;\\theta\\right)}{\\partial\\theta}\\biggr)f\\left(y;\\theta\\right)dy\\]这就是 \\(-E{\\left[S\\left(\\theta\\right)\\right]}^2\\)。\\[\\begin{equation}\nVar\\left[S(\\theta)\\right]=-E\\left(\\frac{\\partial S}{\\partial\\theta}\\right)\n\\tag{5.4}\n\\end{equation}\\]根据定义，\\(Var\\left[S\\left(\\theta\\right)\\right]=E\\left[S\\left(\\theta\\right)\\right]^2-\\left\\{E\\left[S\\left(\\theta\\right)\\right]\\right\\}^2\\)，根据 \\(E{\\left[S\\left(\\theta\\right)\\right]}=0\\)，这可简化为 \\(E{\\left[S\\left(\\theta\\right)\\right]}^2\\)。再根据之前的结果，\\(E\\bigl[S(\\theta)\\bigr]^2=-E\\left(\\frac{\\partial S}{\\partial\\theta}\\right)\\)，从而得证。\\[\\begin{equation}\nVar\\left(Y\\right)=\\left(\\phi\\right)\\left[\\frac{\\partial^2b\\left(\\theta\\right)}{\\partial\\theta^2}\\right]\n\\tag{5.5}\n\\end{equation}\\]根据定义，\\(Var(Y)=E(T-\\mu)^2\\)，利用 (5.1) 这可写为\\[E\\left\\{\\left[\\left(\\phi\\right)\\right]\\frac{y-\\left[\\partial b\\left(\\theta\\right)/\\partial\\left(\\theta\\right)\\right]}{\\left(\\phi\\right)}\\right\\}^2=\\left[\\left(\\phi\\right)\\right]^2E\\left[S\\left(\\theta\\right)\\right]^2\\]再利用 (5.3) 得到\\[-\\left[\\left(\\phi\\right)\\right]^2E\\left[\\frac{\\partial S}{\\partial\\theta}\\right]=\\left[\\left(\\phi\\right)\\right]^2\\left[\\frac{\\partial^2b\\left(\\theta\\right)/\\partial\\theta^2}{\\left(\\phi\\right)}\\right]=(\\phi)\\left[\\frac{\\partial^2b\\left(\\theta\\right)}{\\partial\\theta^2}\\right]\\]从而得证。\n二阶导 \\(\\frac{\\partial^2b(\\theta)}{\\partial\\theta^2}\\) 称为方差函数 (variance function)，记作 \\(V(\\mu)\\)。这是因为 \\(V(\\mu)=\\frac{\\partial^2b\\left(\\theta\\right)}{\\partial\\theta^2}\\) 描述了 \\(Y\\) 的方差对其期望值的依赖性。例如，对于正态分布或高斯分布，\\(V(\\mu)=1\\)，这意味着方差根本不取决于均值。对于二项和泊松分布，方差函数分别为 \\(V\\left(p\\right)=p\\left(1-p\\right)\\) 和 \\(V(\\lambda)=\\lambda\\)；对于二项分布和泊松分布，尺度参数是已知常数，则方差完全由期望值决定。对于其他分布，例如贝塔、伽马和负二项，方差取决于均值和尺度参数（第 2 章的附录 ）。","code":""},{"path":"chap5.html","id":"sec5-2-2","chapter":"第 5 章 GLMM 估计","heading":"5.2.2 最大似然估计","text":"最大似然估计 (maximum likelihood estimation) 的基本思想是确定使似然最大化的模型参数值。在指数族的背景中，给定观测 \\(y\\) 和尺度参数 \\(\\phi\\)，通过关于典型参数 \\(\\theta\\) 最大化对数似然函数 \\(\\ell\\left(\\theta;y,\\phi\\right)\\) 来实现这一点。乍一看，这种最大化似乎与广义线性模型 \\(\\symbf\\eta=\\symbf{X\\beta}\\) 有点脱节。然而经过思考，我们可以看到一个逻辑流程，在我们针对模型的每个细节逐步推导估计过程、最终达到广义线性混合模型的过程中，我们需要牢记这一流程。流程如下：根据模型定义，参数向量 \\(\\symbf\\beta\\) 决定了连接函数 \\(\\symbf \\eta\\)；反过来，连接函数又决定了均值 \\(\\symbf\\mu = g^{-1}(\\symbf\\eta)\\)，这里 \\(g^{-1}(\\cdot)\\) 表示连接函数的逆函数。最后，回想一下，典型参数是均值的函数——典型参数更富信息的表示为 \\(\\theta\\mu\\)。综合这些内容，我们发现指数族对数似然的一般形式 (5.1) 可重写为\\[\\begin{equation}\n\\ell\\left(\\beta;y,\\phi\\right)=\\frac{y\\left\\{\\theta{\\left[g^{-1}\\left(X\\beta\\right)\\right]}\\right\\}-b\\left(\\left\\{\\theta{\\left[g^{-1}\\left(X\\beta\\right)\\right]}\\right\\}\\right)}{\\left(\\phi\\right)}+c\\left(y,\\phi\\right)\n\\tag{5.6}\n\\end{equation}\\]现在很明显，关于 \\(\\symbf\\beta\\) 的最大化是有意义的。虽然式 (5.6) 捕获了对数似然的基本形式，但为了反映我们将对数据向量进行观测并根据这些数据估计参数向量，我们需要更准确地表述它。因此，对数似然需要使用矩阵语言来表述。具体地，\\[\\begin{equation}\n\\ell\\left(\\symbf{\\theta};\\symbf{y},\\phi\\right)=\\symbf{y}^{\\prime}\\symbf{}\\symbf{\\theta}-\\symbf{1}^{\\prime}\\symbf{}b\\left(\\symbf{\\theta}\\right)+c\\left(\\symbf{y},\\phi\\right)\n\\tag{5.7}\n\\end{equation}\\]其中 \\(\\symbf{}=\\operatorname{diag}\\left[1/(\\phi_i)\\right]\\) 为 \\(n×n\\) 对角阵（假定有 \\(n\\) 个观测），其第 \\(\\) 个元素是第 \\(\\) 个观测 \\(y_i\\) 的尺度参数值，\\(\\symbf\\theta\\) 是典型参数向量，\\(\\symbf 1'\\) 是 \\(1×n\\) 全一向量，\\(b(\\symbf\\theta)\\) 是通过将函数 \\(b(\\cdot)\\) 应用于 \\(\\symbf\\theta\\) 的每个元素而定义的 \\(n × 1\\) 向量，而 \\(c (\\symbf y,\\phi)\\) 是不依赖于 \\(\\symbf\\theta\\) 的剩余项。我们可以像 (5.6) 那样用 \\(\\symbf\\beta\\) 表示 \\(\\symbf\\theta\\)，只是此处没有展示。固定效应广义线性模型的最大似然估计涉及到令导数\\[\\frac{\\partial\\ell\\left(\\symbf{\\theta};\\symbf{y},\\phi\\right)}{\\partial\\symbf{\\beta}^{\\prime}}\\]等于零（认识到 \\(\\symbf\\theta\\) 取决于 \\(\\symbf\\beta\\)）并求解 \\(\\symbf\\beta\\)。","code":""},{"path":"chap5.html","id":"sec5-2-3","chapter":"第 5 章 GLMM 估计","heading":"5.2.3 Newton-Raphson 和 Fisher 得分","text":"从 5.3 节开始，我们将认识到广义以及混合线性模型的估计需要迭代程序。为实现这一点，Newton-Raphson 和 Fisher 得分 (Fisher scoring) 是两种在线性模型方法论中占有突出地位的方法。这两个程序都基于对数似然的二阶泰勒级数近似。方便起见，我们将对数似然简记为 \\(\\ell(\\theta)\\)。其在 \\(\\tilde\\theta\\) 处计算的泰勒级数近似为\\[\\begin{equation}\n\\ell\\left(\\theta\\right)\\cong\\ell\\left(\\tilde{\\theta}\\right)+\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}\\Bigg|_{{\\theta=\\tilde{\\theta}}}\\left(\\theta-\\tilde{\\theta}\\right)+\\left(\\frac{1}{2}\\right)\\frac{\\partial^{2}\\ell\\left(\\theta\\right)}{\\partial\\theta^{2}}\\Bigg|_{{\\theta=\\tilde{\\theta}}}\\left(\\theta-\\tilde{\\theta}\\right)^{2}\n\\tag{5.8}\n\\end{equation}\\]关于 \\(\\theta\\) 微分得到\\[\\begin{aligned}\n\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}& \\cong\\frac{\\partial\\left\\{\\ell\\left(\\tilde{\\theta}\\right)+\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}\\right|_{{\\theta=\\tilde{\\theta}}}\\left(\\theta-\\tilde{\\theta}\\right)+\\left.\\frac{\\partial^{2}\\ell\\left(\\theta\\right)}{\\partial\\theta^{2}}\\right|_{{\\theta=\\tilde{\\theta}}}\\left(\\theta-\\tilde{\\theta}\\right)^{2}}{\\partial\\theta}  \\\\\n&=0+\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}\\Bigg|_{\\theta=\\tilde{\\theta}}+\\frac{\\partial^2\\ell\\left(\\theta\\right)}{\\partial\\theta^2}\\Bigg|_{\\theta=\\tilde{\\theta}}\\left(\\theta-\\tilde{\\theta}\\right)\n\\end{aligned}\\]令 \\(\\frac{\\partial\\ell(\\theta)}{\\partial\\theta}=0\\) 并重写后得到\\[\\theta\\equiv\\tilde{\\theta}-\\left[\\left.\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}\\right|_{\\theta=\\tilde{\\theta}}\\right]\\left[\\left.\\frac{\\partial^2\\ell\\left(\\theta\\right)}{\\partial\\theta^2}\\right|_{\\theta=\\tilde{\\theta}}\\right]^{-1}\\]上述推导可用矩阵写出（此处未显示——留作练习）。如此，我们将\\[\\left.\\frac{\\partial\\ell\\left(\\symbf{\\theta}\\right)}{\\partial\\symbf{\\theta}}\\right|_{\\symbf{\\theta}=\\tilde{\\symbf{\\theta}}}\\]视为在 \\(\\tilde{\\symbf\\theta}\\) 处计算的得分向量 (score vector)，并将\\[\\left.\\frac{\\partial\\ell^2\\left(\\symbf\\theta\\right)}{\\partial\\symbf\\theta\\partial\\symbf\\theta^{\\prime}}\\right|_{\\symbf\\theta=\\tilde{\\symbf\\theta}}\\]视为在 \\(\\tilde{\\symbf\\theta}\\) 处计算的 Hessian 矩阵。将它们分别表示为 \\(\\symbf{s}\\left(\\tilde{\\symbf{\\theta}}\\right)\\) 以及 \\(\\symbf{H}\\left(\\tilde{\\symbf{\\theta}}\\right)\\) 我们有\\[\\begin{equation}\n\\symbf\\theta\\cong\\tilde{\\symbf\\theta}-{\\left[\\symbf{H}{\\left(\\tilde{\\symbf\\theta}\\right)}\\right]}^{-1}\\symbf{s}{\\left(\\tilde{\\symbf\\theta}\\right)}\n\\tag{5.9}\n\\end{equation}\\]这定义了基本的 Newton-Raphson 算法。为了实现它，我们采用 \\(\\symbf \\theta\\) 的当前估计，记为 \\(\\tilde{\\symbf\\theta}\\)，使用它来更新得分向量和 Hessian 矩阵，使用式 (5.9) 计算新的 \\(\\symbf \\theta\\) 值，在下一次更新中使用新值作为 \\(\\tilde{\\symbf \\theta}\\)。此过程一直持续到差值 \\(\\left(\\symbf\\theta-\\tilde{\\symbf\\theta}\\right)\\) 小到可接受为止。Fisher 得分使用信息矩阵代替 Hessian 矩阵。根据 (5.4) ，Hessian 矩阵的期望 \\(E\\left[\\symbf{H}(\\symbf{\\theta})\\right]=-Var\\left[\\symbf{s}(\\symbf{\\theta})\\right]=-\\symbf{\\mathcal J}\\left(\\symbf\\theta\\right)\\)。代入 (5.9) 得到\\[\\begin{equation}\n\\symbf{\\theta}\\cong\\tilde{\\symbf{\\theta}}+\\left[\\symbf{\\mathcal J}\\left(\\tilde{\\symbf{\\theta}}\\right)\\right]^{-1}\\symbf{s}\\left(\\tilde{\\symbf{\\theta}}\\right)\n\\tag{5.10}\n\\end{equation}\\]这定义了 Fisher 得分算法。它与 Newton-Raphson 算法完全一样，只是我们使用信息矩阵而不是 Hessian 矩阵。这两种算法都不适用于所有情况。对于我们将在本书中考虑的大多数例子，这两种程序在速度和结果方面是无法区分的。两者最终都会得到相同的解，但对于给定的模型或数据集，其中一种可能比另一种更快地收敛到解。计算机软件程序通常使用其中一个作为默认值，但允许你选择另一个，甚至允许你在迭代过程中的某个时刻切换。在给定情况下哪种算法效果最好通常需要反复试验。","code":""},{"path":"chap5.html","id":"sec5-2-4","chapter":"第 5 章 GLMM 估计","heading":"5.2.4 拟似然","text":"最大似然估计依赖于\\[\\frac{\\partial\\ell\\left(\\symbf\\theta;\\symbf y,\\phi\\right)}{\\partial\\symbf\\theta}\\]其标量形式为\\[\\frac{y-\\mu}{(\\phi)}\\]因此，虽然项 \\(c(y,\\phi)\\) 是指数族似然的完整指定所必需的，但它不是最大似然估计所必需的。只有\\[\\frac{y\\theta-b\\left(\\theta\\right)}{\\left(\\phi\\right)}\\]是必需的。Wedderburn (1974) 通过拟似然 (quasi-likelihood) 理论正式化了该思想。拟似然定义为\\[\\int^\\mu\\frac{y-t}{v(t)(\\phi)}dt\\]其中 \\(v (t)\\) 对应于方差函数的形式，\\((\\phi)\\) 表示尺度参数函数。例如，令 \\(v(t)=t\\) 以及 \\((\\phi)=1\\) 得到\\[\\int^\\mu\\frac{y-t}tdt=y\\log(\\mu)-\\mu \\]这是省略了 \\(c(y,\\phi)=-\\log(y!)\\) 的泊松对数似然。类似地，令 \\(v(t)=1\\) 以及 \\((\\phi)=\\phi^2\\) 得到\\[\\int^\\mu\\frac{y-t}{\\phi^2}dt=\\frac{y\\mu-{\\mu^2}/2}{\\phi^2}\\]这是高斯对数似然的“拟似然”部分。Wedderburn (1974) 表明，为指数族开发的广义线性模型估计与推断理论，也适用于那些响应变量的均值和方差可以指定的模型，即使它们并不与一个已知的似然函数相关联。拟似然的重要应用包括针对非高斯重复测量数据的过度分散 (overdispersion) 以及相关误差 (correlated error) 模型，这两者都在本书第三篇讨论。例如，计数数据通常假定为具有泊松分布。泊松要求期望值和方差相等，即 \\(E(y)=\\lambda=Var(y)\\)。然而在实践中，我们经常观察到过度分散，即样本方差远大于均值，因此远大于理论表明的值。对过度分散的计数数据进行建模的一种常见方法是将假定的方差乘以尺度参数。以拟似然来说，我们定义了 \\(\\upsilon(t)=t\\)，就像定义泊松分布一样，但将尺度参数更改为 \\((\\phi)=\\phi\\)。由此得到的拟似然为\\[\\int^\\mu\\frac{y-t}{t\\phi}=\\frac{y\\log(\\mu)-\\mu}\\phi\\]因此，\\(E(y)=\\mu\\)，但 \\(Var(y)=\\phi\\mu\\)。这种结构不存在实际的概率分布，但在许多情况下，它对计数数据的分布进行了充分的建模，并且拟似然对于广义线性模型估计和推断目的来说是良定的 (well-defined).","code":""},{"path":"chap5.html","id":"sec5-3","chapter":"第 5 章 GLMM 估计","heading":"5.3 仅固定效应","text":"本节开发的估计方程适用于具有线性预测器 \\(\\symbf\\eta=\\symbf{X\\beta}\\) 的所有模型。虽然我们将这些称为广义线性模型 (GLM) 估计方程，但它们同样适用于经典的“一般”线性模型。回想一下，后者只是具有独立、同方差高斯数据的 GLM 的一个特例。类似地，第 4 章中介绍的经典 OLS 估计方程只是 GLM 估计方程的特例。","code":""},{"path":"chap5.html","id":"标量形式","chapter":"第 5 章 GLMM 估计","heading":"标量形式","text":"最终，我们想要一组矩阵方程来估计 \\(\\symbf\\beta\\)，但简便起见，我们从对数似然的标量形式，式 (5.1)，或更好地，式 (5.6) 开始，它显示了模型参数如何嵌入典型参数。使用链式法则，我们可将用于最大似然估计的导数写为\\[\\frac{\\partial\\ell\\left(\\theta\\left[g^{-1}\\left(X\\beta\\right)\\right];y,\\phi\\right)}{\\partial\\beta}=\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}\\frac{\\partial\\theta}{\\partial\\mu}\\frac{\\partial\\mu}{\\partial\\eta}\\frac{\\partial\\eta}{\\partial\\beta}\\]现在我们注意到：\\[\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}=\\frac{y-\\left[\\partial b\\left(\\theta\\right)/\\partial\\theta\\right]}{\\left(\\phi\\right)}=\\frac{y-\\mu}{\\left(\\phi\\right)}\\]\\[\\frac{\\partial\\ell\\left(\\theta\\right)}{\\partial\\theta}=\\frac{y-\\left[\\partial b\\left(\\theta\\right)/\\partial\\theta\\right]}{\\left(\\phi\\right)}=\\frac{y-\\mu}{\\left(\\phi\\right)}\\]\\[\\frac{\\partial\\theta}{\\partial\\mu}=\\left(\\frac{\\partial\\mu}{\\partial\\theta}\\right)^{-1}=\\left(\\frac{\\partial\\left[\\partial b\\left(\\theta\\right)/\\partial\\theta\\right]}{\\partial\\theta}\\right)^{-1}=\\left(\\frac{\\partial^2b\\left(\\theta\\right)}{\\partial\\theta^2}\\right)^{-1}=\\frac{1}{V\\left(\\mu\\right)}\\]\\[\\frac{\\partial\\theta}{\\partial\\mu}=\\left(\\frac{\\partial\\mu}{\\partial\\theta}\\right)^{-1}=\\left(\\frac{\\partial\\left[\\partial b\\left(\\theta\\right)/\\partial\\theta\\right]}{\\partial\\theta}\\right)^{-1}=\\left(\\frac{\\partial^2b\\left(\\theta\\right)}{\\partial\\theta^2}\\right)^{-1}=\\frac{1}{V\\left(\\mu\\right)}\\]\\[\\frac{\\partial\\eta}{\\partial\\beta}=\\frac{\\partial(X\\beta)}{\\partial\\beta}=X\\]\\[\\frac{\\partial\\eta}{\\partial\\beta}=\\frac{\\partial(X\\beta)}{\\partial\\beta}=X\\]注意，这些结果同样适用于拟似然。得到\\[\\frac{\\partial\\ell\\left(\\theta{\\left[g^{-1}\\left(X\\beta\\right)\\right]};y,\\phi\\right)}{\\partial\\beta}=\\frac{y-\\mu}{(\\phi)}{\\left(\\frac1{V(\\mu)}\\right)}{\\left(\\frac{\\partial\\mu}{\\partial\\eta}\\right)}X\\]现在回想 (5.5)：\\((\\phi)V(\\mu)=Var(y)\\) 从而给出\\[\\begin{equation}\n\\frac{\\partial\\ell\\left(\\theta{\\left[g^{-1}\\left(X\\beta\\right)\\right]};y,\\phi\\right)}{\\partial\\beta}=(y-\\mu){\\left(\\frac1{V(y)}\\right)}{\\left(\\frac{\\partial\\mu}{\\partial\\eta}\\right)}X\n\\tag{5.11}\n\\end{equation}\\]","code":""},{"path":"chap5.html","id":"矩阵形式","chapter":"第 5 章 GLMM 估计","heading":"矩阵形式","text":"我们现在准备开发矩阵形式的估计方程。我们可将 (5.11) 写为\\[\\begin{equation}\n\\frac{\\partial\\ell(\\symbf{\\theta})}{\\partial\\symbf{\\beta}'}=\\symbf{X}\\symbf{D}^{-1}\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{\\mu}\\right)\n\\tag{5.12}\n\\end{equation}\\]其中 \\(\\symbf y\\) 为 \\(n×1\\) 观测向量，\\(\\ell(\\symbf\\theta)\\) 是与观测相关的对数似然的 \\(n×1\\) 值向量，\\(\\symbf{V}=\\operatorname{diag}\\left[Var(y_i)\\right]\\) 为 \\(n×n\\) 观测矩阵。\\(\\symbf{D}=\\operatorname{diag}\\left[\\frac{\\partial\\eta_i}{\\partial\\mu_i}\\right]\\) 为 \\(n×n\\) 导数矩阵，\\(\\symbf\\mu\\) 为 \\(n×1\\) 均值向量。令 \\(\\symbf{W}=\\left(\\symbf{D}\\symbf{V}\\symbf{D}\\right)^{-1}\\)。我们可将 (5.12) 重写为（同时注意到它是得分向量）\\[\\begin{equation}\n\\symbf{s}\\left(\\symbf{\\theta}\\right)=\\frac{\\partial\\ell\\left(\\symbf{\\theta}\\right)}{\\partial\\symbf{\\theta}^{\\prime}}=\\symbf{X}^{\\prime}\\symbf{D}^{-1}\\symbf{V}^{-1}\\left(\\symbf{D}^{-1}\\symbf{D}\\right)\\left(\\symbf{y}-\\symbf{\\mu}\\right)=\\symbf{X}^{\\prime}\\symbf{W}\\symbf{D}\\left(\\symbf{y}-\\symbf{\\mu}\\right)\n\\tag{5.13}\n\\end{equation}\\]我们使用 Fisher 得分来完成估计方程的构建。为此，我们需要信息矩阵。根据定义，信息矩阵为\\[\\begin{align}\nVar\\left[\\symbf{s}\\left(\\symbf{\\theta}\\right)\\right]&=\\symbf{X'WD}\\left[Var\\left(\\symbf{y}-\\symbf{\\mu}\\right)\\right]\\symbf{DWX}\\\\&=\\symbf{X'WDVDWX}\\\\&=\\symbf{X'WW}^{-1}\\symbf{WX}=\\symbf{X'WX}\n\\tag{5.14}\n\\end{align}\\]","code":""},{"path":"chap5.html","id":"sec5-3-1","chapter":"第 5 章 GLMM 估计","heading":"5.3.1 GLM 估计方程","text":"将信息矩阵代入 Fisher 得分方程 (5.14) 得到 \\(\\symbf{\\beta}=\\tilde{\\symbf{\\beta}}+\\left(\\symbf{X}^{\\prime}\\tilde{\\symbf{W}}\\symbf{X}\\right)^{-1}\\symbf{X}^{\\prime}\\tilde{\\symbf{W}}\\tilde{\\symbf{D}}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\)，其中 \\(\\tilde{\\symbf W},\\tilde{\\symbf D}\\) 和 \\(\\tilde{\\symbf\\mu}\\) 表示 \\({\\symbf W},{\\symbf D}\\) 和 \\({\\symbf\\mu}\\) 在 \\(\\tilde{\\symbf\\beta}\\) 处的值。将方程两边同时乘以 \\(\\symbf{X}^{\\prime}\\tilde{\\symbf{W}}\\symbf{X}\\) 得到 \\(\\symbf{X^{\\prime}\\tilde{W}X\\beta=X^{\\prime}\\tilde{W}X\\tilde{\\beta}+X^{\\prime}W\\tilde{D}\\left(y-\\tilde{\\mu}\\right)}\\)。这就给出了 GLM 估计方程：\\[\\begin{equation}\n\\symbf{X^{\\prime}\\tilde WX\\beta}=\\symbf{X^{\\prime}\\tilde{W}y^*}\n\\tag{5.15}\n\\end{equation}\\]其中 \\(\\symbf{y}^*=\\symbf{X}\\tilde{\\symbf{\\beta}}+\\tilde{\\symbf{D}}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)=\\tilde{\\symbf{\\eta}}+\\tilde{\\symbf{D}}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\)。以 GLM 术语，\\(\\symbf y^*\\) 称为伪变量 (pseudo-variable).我们通过迭代实现 GLM 估计方程 (5.15). 一个起始的 \\(\\tilde{\\symbf\\beta}\\) 值允许我们初始化方程，得出 \\({\\symbf\\beta}\\) 一个新的估计。我们将此作为更新的 \\(\\tilde{\\symbf\\beta}\\) 并以这种方式继续，直到差值 \\(\\left(\\symbf\\beta-\\tilde{\\symbf\\beta}\\right)\\) 可忽略不计，此时我们说估计过程已经收敛。","code":""},{"path":"chap5.html","id":"sec5-3-2","chapter":"第 5 章 GLMM 估计","heading":"5.3.2 与最小二乘估计的关系","text":"这里有两个与最小二乘估计的重要关联。首先，它预示了我们将在 5.5 节为广义线性混合模型开发的伪似然估计方程。其次，它指出了这样的事实：高斯线性模型的经典最小二乘估计实际上是广义线性模型估计的一个特例。","code":""},{"path":"chap5.html","id":"伪似然","chapter":"第 5 章 GLMM 估计","heading":"伪似然","text":"考虑伪变量 \\(\\symbf y^*\\)。其期望和方差分别为\\(E\\left(\\symbf{y}^*\\right)=E{\\left[\\symbf{X}\\tilde{\\symbf{\\beta}}+\\tilde{\\symbf{D}}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\right]}=\\symbf{X}\\symbf{\\beta}\\)\\(E\\left(\\symbf{y}^*\\right)=E{\\left[\\symbf{X}\\tilde{\\symbf{\\beta}}+\\tilde{\\symbf{D}}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\right]}=\\symbf{X}\\symbf{\\beta}\\)\\(Var\\left(\\symbf{y}^*\\right)=Var\\left[\\symbf{X}\\tilde{\\symbf{\\beta}}+\\tilde{\\symbf{D}}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\right]=\\tilde{\\symbf{D}}\\left[Var\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\right]\\tilde{\\symbf{D}}=\\tilde{\\symbf{D}}\\tilde{\\symbf{V}}\\tilde{\\symbf{D}}=\\tilde{\\symbf{W}}^{-1}\\)\\(Var\\left(\\symbf{y}^*\\right)=Var\\left[\\symbf{X}\\tilde{\\symbf{\\beta}}+\\tilde{\\symbf{D}}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\right]=\\tilde{\\symbf{D}}\\left[Var\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\right]\\tilde{\\symbf{D}}=\\tilde{\\symbf{D}}\\tilde{\\symbf{V}}\\tilde{\\symbf{D}}=\\tilde{\\symbf{W}}^{-1}\\)由此可从估计方程获得 \\(\\symbf\\beta\\) 的广义最小二乘估计 (generalized least squares estimator)，它由逆方差加权：\\[\\symbf{X}'\\Big[Var\\big(\\symbf{y}^*\\big)\\Big]^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{X}'\\Big[Var\\big(\\symbf{y}^*\\big)\\Big]^{-1}\\symbf{y}^*\\Rightarrow\\symbf{X}'\\symbf{W}\\symbf{X}\\symbf{\\beta}=\\symbf{X}'\\symbf{W}\\symbf{y}^*\\]这与 (5.15) 中的 GLM 估计方程相同。","code":""},{"path":"chap5.html","id":"高斯线性模型和普通最小二乘","chapter":"第 5 章 GLMM 估计","heading":"高斯线性模型和普通最小二乘","text":"经典线性模型理论的发展是基于最小二乘的。具有独立、同方差数据的线性模型线性预测器：\\(\\symbf\\eta=\\symbf{X\\beta}\\)连接：\\(\\symbf\\eta=\\symbf\\mu\\)分布：\\(\\symbf y\\sim N(\\symbf \\mu,\\symbf \\sigma^2)\\)在经典线性模型文献中被称为“一般”线性模型。使用普通最小二乘 (ordinary least squares)，即最小化 \\(\\symbf y\\) 与 \\(\\hat{\\symbf y}\\) 之差的平方和进行估计。因为高斯模型使用恒等连接（尽管在前 GLM 时代，任何人都不会有连接函数这一概念），那么 \\(\\hat{\\symbf{y}}=\\symbf{X}\\hat{\\symbf{\\beta}}\\)。普通最小二乘估计是通过最小化 \\((\\symbf{y}-\\symbf{X}\\symbf{\\beta})^{\\prime}(\\symbf{y}-\\symbf{X}\\symbf{\\beta})\\) 得到的，该过程得到普通最小二乘方程 (ordinary least squares equations)\\[\\begin{equation}\n\\symbf{X^{\\prime}X\\beta}=\\symbf{X^{\\prime}y}\n\\tag{5.16}\n\\end{equation}\\]在线性模型文献中也称为正规方程 (normal equations). 注意，在恒等连接下，导数矩阵 \\(\\symbf D\\) 简化为单位矩阵。因此，\\(\\symbf W\\) 简化为 \\((\\symbf \\sigma^2)^{-1}\\)，\\(\\symbf y^*\\) 简化为 \\(\\symbf y\\)，GLM 估计方程成为 \\(\\symbf{X}^{\\prime}\\left(\\symbf{}\\sigma^2\\right)^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{X}^{\\prime}\\left(\\symbf{}\\sigma^2\\right)^{-1}\\symbf{y}\\)，也就是 \\(\\symbf X^{\\prime}\\symbf X\\beta=\\symbf X^{\\prime}\\symbf y\\)。对于具有非平凡方差结构的高斯模型，即具有分布 \\(\\symbf y\\sim N(\\symbf\\mu,\\symbf V)\\)，我们知道普通最小二乘估计的效率不如广义最小二乘估计。由于高斯模型使用恒等连接，\\(\\symbf W\\) 简化为 \\(\\symbf V^{−1}\\)，\\(\\symbf y^*=\\symbf y\\)，GLM 方程简化为高斯广义最小二乘 \\(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{y}\\)。因此，我们可看出高斯最小二乘是更一般的 GLM 估计方程的一个特例。","code":""},{"path":"chap5.html","id":"sec5-4","chapter":"第 5 章 GLMM 估计","heading":"5.4 高斯混合模型","text":"在本节中，我们开发了线性混合模型 (LMM) 的估计方程，即具有高斯数据以及线性预测器中具有随机效应的模型。回顾 LMM 的基本特征：线性预测器：\\(\\symbf\\eta=\\symbf{X\\beta}+\\symbf{Zb}\\)分布：\\(\\symbf y\\mid \\symbf b\\sim N(\\symbf\\mu\\mid\\symbf b,\\symbf R);\\;\\symbf b\\sim N(\\symbf 0,\\symbf G)\\)连接：\\(\\symbf\\eta=\\symbf\\mu\\mid\\symbf b\\)（恒等连接）正如我们在第 3 章中看到的，根据 LMM 的分布假设，\\(\\symbf y\\) 的边际分布为 \\(N(\\symbf\\mu,\\symbf V)\\)，其中 \\(\\symbf V=\\symbf{ZGZ'}+\\symbf R\\)，\\(\\symbf{X\\beta}\\) 对 \\(\\symbf\\mu\\) 进行建模。回想第 3 章，LMM 是唯一如此的混合模型：完全指定的条件模型与边际模型具有相同的分布（高斯），并且条件模型与边际模型 \\(\\symbf\\beta\\) 的估计相同。我们在本节中的任务是：1) 开发线性预测器效应 \\(\\symbf\\beta\\) 和 \\(\\symbf b\\) 的估计方程，以及 2) 开发方差-协方差阵 \\(\\symbf G\\) 和 \\(\\symbf R\\) 分量的估计方程。在这一过程中，我们将正式证明来自条件模型与边际模型的固定模型效应 \\(\\symbf\\beta\\) 估计的等价性。","code":""},{"path":"chap5.html","id":"sec5-4-1","chapter":"第 5 章 GLMM 估计","heading":"5.4.1 混合模型方程","text":"根据分布假设（清晰起见，在符号表示上给予一些自由度），关于 \\(\\symbf y\\mid \\symbf b\\) 和 \\(\\symbf b\\) 的对数似然方程分别为：\\[\\ell\\left(\\symbf{y}\\mid\\symbf{b}\\right)=-\\left(\\frac n2\\right)\\mathrm{log}\\left(2\\pi\\right)-\\left(\\frac12\\right)\\mathrm{log}\\left(\\left|\\symbf{R}\\right|\\right)-\\left(\\frac12\\right)\\left(\\symbf{y-X\\beta-Zb}\\right)^{\\prime}\\symbf{R}^{-1}\\left(\\symbf{y-X\\beta-Zb}\\right)\\]以及\\[\\ell(\\symbf{b})=-\\biggl(\\frac{b}{2}\\biggr)\\mathrm{log}\\left(2\\pi\\right)-\\biggl(\\frac{1}{2}\\biggr)\\mathrm{log}\\left(|\\symbf{G}|\\right)-\\biggl(\\frac{1}{2}\\biggr)\\symbf{b}'\\symbf{G}^{-1}\\symbf{b}\\]其中 \\(b\\) 表示随机效应的水平总数。关注“拟似然”部分，即忽略 \\(c(y,\\phi)\\)，我们可将联合对数似然写为\\[\\ell\\left(\\symbf{y},\\symbf{b}\\right)=-{\\left(\\frac12\\right)}{\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}-\\symbf{Z}\\symbf{b}\\right)}^{\\prime}\\symbf{R}^{-1}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}-\\symbf{Z}\\symbf{b}\\right)-{\\left(\\frac12\\right)}\\symbf{b}^{\\prime}\\symbf{G}^{-1}\\symbf{b}\\]现在我们试图获取 \\(\\symbf\\beta\\) 和 \\(\\symbf b\\) 的最大似然估计。首先分别取联合对数似然关于它们的导数\\[\\begin{aligned}\\partial[\\ell(\\symbf{y},\\symbf{b})]/\\partial\\symbf{\\beta}^{\\prime}&=\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{y}-\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{X}\\symbf{\\beta}-\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\symbf{b}\\\\\\\\\\partial[\\ell(\\symbf{y},\\symbf{b})]/\\partial\\symbf{b}^{\\prime}&=\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{y}-\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{X}\\symbf{\\beta}-\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\symbf{b}-\\symbf{G}^{-1}\\symbf{b}\\end{aligned}\\]令它们等于零并求解 \\(\\symbf \\beta\\) 和 \\(\\symbf b\\)，得到 LMM 估计方程，这在 LMM 文献中通常称为混合模型方程 (mixed model equations)：\\[\\begin{equation}\n\\begin{bmatrix}\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{X}&\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\\\\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{X}&\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\end{bmatrix}\\begin{bmatrix}\\symbf{\\beta}\\\\\\symbf{b}\\end{bmatrix}=\\begin{bmatrix}\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{y}\\\\\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{y}\\end{bmatrix}\n\\tag{5.17}\n\\end{equation}\\]若 \\(\\symbf G\\) 和 \\(\\symbf R\\) 的分量是已知的，那么我们可以通过 (5.17) 的单次计算来获得 \\(\\symbf\\beta\\) 和 \\(\\symbf b\\) 的精确估计。但这种情况很少见，这意味着求解 (5.17) 还需要估计 \\(\\symbf G\\) 和 \\(\\symbf R\\)。这将求解混合模型方程转化为一个迭代过程：\\(\\symbf G\\) 和 \\(\\symbf R\\) 的起始值能够实现 (5.17) 的初始解；我们使用 \\(\\symbf\\beta\\) 和 \\(\\symbf b\\) 的估计结果来更新 \\(\\symbf G\\) 和 \\(\\symbf R\\)。该过程一直持续到我们达到收敛为止。我们在 5.4.3 节讨论 \\(\\symbf G\\) 和 \\(\\symbf R\\) 的估计。","code":""},{"path":"chap5.html","id":"sec5-4-2","chapter":"第 5 章 GLMM 估计","heading":"5.4.2 与最小二乘的联系","text":"LM（假设高斯数据的仅固定效应模型）最小二乘估计方程是 (5.17) 的特例。我们很容易看到这一点，回想 LM 的线性预测器为 \\(\\symbf\\eta = \\symbf{X\\beta}\\)，LM 一般的分布形式为 \\(\\symbf y\\sim N(\\symbf\\mu,\\symbf V)\\)。请注意，在 LMM 中， \\(Var\\left(\\symbf{y}\\right)=\\symbf{V}=\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{R}\\)。LM 就是移除 \\(\\symbf{Zb}\\) 和 \\(\\symbf G\\) 的 LMM，因此对于 LM 有 \\(\\symbf V =\\symbf R\\)。从 (5.17) 中移除 \\(\\symbf{Zb}\\) 和 \\(\\symbf G\\) 得到 \\(\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{y}\\)，或等价地，广义最小二乘估计 \\(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{y}\\)。若假定观测独立且同方差，即经典的“一般”线性模型，即 \\(\\symbf V=\\symbf R=\\symbf \\sigma^2\\)，则 (5.17) 简化为“正规方程” \\(\\symbf{X'X\\beta}=\\symbf{X'y}\\)。对于 LMM 的边际形式，线性预测器为 \\(\\symbf η=\\symbf{X\\beta}\\)，假定分布为 \\(\\symbf V=\\symbf{ZGZ'}+\\symbf R\\)。因此，使用 LMM 的边际形式的 \\(\\symbf\\beta\\) 的估计方程由广义最小二乘估计给出\\[\\begin{equation}\n\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{X}\\symbf{V}^{-1}\\symbf{y}\\text{ 或等价地 }\\symbf{X}^{\\prime}\\left(\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{R}\\right)^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{X}\\left(\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{R}\\right)^{-1}\\symbf{y}\n\\tag{5.18}\n\\end{equation}\\]这里的重要结果是，混合模型估计方程 (5.17) 与边际 LMM 的广义最小二乘 (5.18) 的 \\(\\symbf\\beta\\) 估计是相同的。Searle (1971) 给出了证明，为了读者的方便，并考虑到其结果的重要性，本文给出了 Searle 证明的基本流程。我们可将混合模型方程 (5.17) 写为两个等式\\[\\begin{equation}\n\\begin{aligned}\\symbf{X^{\\prime}R}^{-1}\\symbf{X\\beta}+\\symbf{X^{\\prime}R}^{-1}\\symbf{Zb}&=\\symbf{X^{\\prime}R}^{-1}\\symbf{y}\\quad\\quad\\text(5.19.1)\\\\\n\\symbf{Z'R}^{-1}\\symbf{X\\beta}+\\left(\\symbf{Z'R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)\\symbf{b}&=\\symbf{Z'R}^{-1}\\symbf{y}\\quad\\quad\\text(5.19.2)\n\\end{aligned}\n\\tag{5.19}\n\\end{equation}\\]求解 (5.19.2) 中的 \\(\\symbf b\\) 得到 \\(\\symbf{b}=\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{y}-\\symbf{Z}'\\symbf{R}^{-1}\\symbf{X}\\symbf{\\beta}\\right)\\)。代入 (5.19.1) 得到 \\(\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{X}\\symbf{\\beta}+\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\left[\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{y}-\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{X}\\symbf{\\beta}\\right)\\right]=\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{y}\\)。重新排列项，我们将其重写为\\[\\small\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{X\\beta}-\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{X\\beta}=\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{y}-\\symbf{X}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{y}\\]或\\[\\small\\symbf{X}^{\\prime}\\left[\\symbf{R}^{-1}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\right]\\symbf{X}\\symbf{\\beta}=\\symbf{X}^{\\prime}\\left[\\symbf{R}^{-1}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\right]\\symbf{y}\\]请注意，最后一个表达式是具有权重矩阵 \\(\\symbf{R}^{-1}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\symbf{Z}'\\symbf{R}^{-1}\\) 的广义最小二乘的形式。现在需要证明该权重矩阵就是 \\(\\symbf V^{−1}\\)。我们通过证明权重矩阵与 \\(\\symbf V\\) 之积是单位矩阵来实现。注意到 \\(\\symbf V=\\symbf{ZGZ'}+\\symbf R\\)，此乘积为\\[\\begin{aligned}\n&\\left[\\symbf{R}^{-1}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\right](\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{R}) \\\\\n=\\;&\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{R}^{-1}\\symbf{R}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}-\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{R}\\right) \\\\\n=\\;&\\symbf{R}^{-1}\\symbf{Z}\\symbf{GZ}^{\\prime}+\\symbf{}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{Z}^{\\prime}\\right) \\\\\n=\\;&\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}+\\symbf{}\\right)\\symbf{Z}'\\\\\n=\\;&\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{G}^{-1}+\\symbf{G}^{-1}\\right)\\symbf{G}\\symbf{Z}^{\\prime} \\\\\n=\\;&\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{Z}'+\\symbf{}-\\symbf{R}^{-1}\\symbf{Z}\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)\\symbf{G}\\symbf{Z}' \\\\\n=\\;&\\symbf{R}^{-1}\\symbf{Z}\\symbf{GZ}^{\\prime}+\\symbf{}-\\symbf{R}^{-1}\\symbf{Z}\\symbf{GZ}^{\\prime}=\\symbf{}\n\\end{aligned}\\]从而得证。类似的推导可应用于 \\(\\symbf b\\) 的估计方程。根据 (5.19.1)\\[\\symbf{b}=\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{y}-\\symbf{Z}'\\symbf{R}^{-1}\\symbf{X}\\symbf{\\beta}\\right)\\]重新排列项并利用 \\(\\symbf{V}\\symbf{V}^{-1}=\\left(\\symbf{Z}\\symbf{G}\\symbf{Z}'+\\symbf{R}\\right)\\symbf{V}^{-1}\\) 得到\\[\\begin{align}\n&\\left(\\symbf{Z'R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\symbf{Z'R}^{-1}\\left(\\symbf{ZGZ'}+\\symbf{R}\\right)\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X\\beta}\\right) \\\\\n=&\\;\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}'\\symbf{R}^{-1}\\symbf{Z}\\symbf{G}\\symbf{Z}'+\\symbf{G}^{-1}\\symbf{G}\\symbf{Z}'\\symbf{}\\right)\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right) \\\\\n=&\\;\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)^{-1}\\left(\\symbf{Z}^{\\prime}\\symbf{R}^{-1}\\symbf{Z}+\\symbf{G}^{-1}\\right)\\symbf{G}\\symbf{Z}^{\\prime}\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right) \\\\\n=&\\;\\symbf{GZ}^{\\prime}\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X\\beta}\\right)\n\\tag{5.20}\n\\end{align}\\]Searle et al. (1992), Robinson (1991) et al. 表明 \\(\\symbf b\\) 的最佳线性预测为\\[E\\left(\\symbf{b}\\mid\\symbf{y}\\right)=E\\left(\\symbf{b}\\right)+Cov\\left(\\symbf{b},\\symbf{y}'\\right)\\begin{bmatrix}Var\\left(\\symbf{y}\\right)\\end{bmatrix}(\\symbf{y}-\\symbf{X}\\symbf{\\beta})\\]对于 LMM，\\(\\symbf y\\) 与 \\(\\symbf b\\) 的联合分布为\\[\\begin{bmatrix}\\symbf{y}\\\\\\symbf{b}\\end{bmatrix}\\sim N\\left(\\begin{bmatrix}\\symbf{X\\beta}\\\\\\symbf{0}\\end{bmatrix},\\begin{bmatrix}\\symbf{V}&\\symbf{ZG'}\\\\\\symbf{GZ'}&\\symbf{G}\\end{bmatrix}\\right)\\]由此可见，\\(\\symbf b\\) 的最佳线性预测为 \\(\\symbf{GZ}^{\\prime}\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X\\beta}\\right)\\)。Goldberger (1962) 将混合模型解命名为“最佳线性无偏预测” (“best linear unbiased predictor”)，Henderson 使用缩写词 BLUP，这是混合模型领域广泛使用的缩写词。我们将在第 ?? 章详细探讨 BLUP.\\(\\symbf\\beta\\) 和 \\(\\symbf b\\) 的估计之间的差异源于以下事实：\\(\\symbf\\beta\\) 的元素是固定参数，而 \\(\\symbf b\\) 是随机向量。对于前者，目标是产生关于固定目标的最小方差估计；对于后者，目标是最小化随机变量实现值的预测误差。Robinson (1991) 对最佳线性无偏预测所涉及的问题给出了精彩的讨论。","code":""},{"path":"chap5.html","id":"sec5-4-3","chapter":"第 5 章 GLMM 估计","heading":"5.4.3 未知的 \\(\\symbf G\\) 和 \\(\\symbf R\\)：ML 和 REML 方差-协方差分量估计","text":"通常，必须估计 \\(\\symbf G\\) 和 \\(\\symbf R\\) 的分量以获得混合模型方程的解。作为例子，请考虑第 3 章中的两个 LMMs. 示例 3.6 有四处理（固定效应）和八地点（随机效应）。线性预测器为 \\(\\eta_{ij}=\\eta+\\tau_i+L_j\\)，地点效应分布为 \\(L_j\\text{ ..d. }N\\left(0,\\sigma_L^2\\right)\\)。示例 3.7 是一个随机系数回归，其线性预测器为 \\(\\eta_{ij}=\\beta_0+b_{0i}+\\left(\\beta_1+b_{1i}\\right)X_j\\) 以及随机回归系数分布为\\[\\begin{bmatrix}b_{0i}\\\\b_{1i}\\end{bmatrix}\\thicksim N{\\left(\\begin{bmatrix}0\\\\0\\end{bmatrix},\\begin{bmatrix}\\sigma_0^2&\\sigma_{01}\\\\&\\sigma_1^2\\end{bmatrix}\\right)}\\]对于示例 3.6，\\(\\symbf{G}=\\symbf{}_8\\sigma_L^2\\) 以及 \\(\\symbf{R}=\\symbf{}_{32}\\sigma^2\\)。我们需要估计的方差分量为 \\(\\sigma_L^2\\) 和 \\(\\sigma^2\\)。在示例 3.7 中，\\[\\symbf{G}=\\symbf{}_4\\otimes\\begin{bmatrix}\\sigma_0^2&\\sigma_{01}\\\\&\\sigma_1^2\\end{bmatrix}\\]以及 \\(\\symbf{R}=\\symbf{}_{72}\\sigma^2\\)。\\(\\symbf G\\) 和 \\(\\symbf R\\) 的分量包括方差和协方差项：\\(\\sigma^2_0,\\sigma_{01},\\sigma^2_1\\) 和 \\(\\sigma^2\\)。最著名的方差分量估计程序（因为它在统计方法入门课程中广泛教授）使用基于 ANOVA 的期望均方。这些称为 ANOVA 估计。它们可用于仅方差分量 (variance-component-) 的 LMMs（随机效应均为 ..d. 的 LMM）。多地点示例 3.6 是仅方差分量模型的示例。如果多地点数据具有子抽样 (sub-sampling)，从而可能包括随机的处理 × 地点项，那么线性预测器将是 \\(\\eta_{ij}=\\eta+\\tau_i+L_j+\\left(tL\\right)_{ij}\\)，其中 \\(L_j\\text{ iid }N\\left(0,\\sigma_L^2\\right)\\) 以及 \\(\\left(tL\\right)_{ij}\\text{ iid }N\\left(0,\\sigma_{TL}^2\\right)\\)。这仍然是一个仅方差分量模型，因为两个随机模型效应都是 ..d.，每个效应对应一个方差分量。另一方面，随机系数 LMM 不是仅方差分量的模型，因为有一个协方差项 \\(\\sigma_{01}\\) 来解释随机截距和斜率系数之间的相关性。对于示例 3.7 或任何其他在 \\(\\symbf G\\) 或 \\(\\symbf R\\) 中具有协方差或相关项 (correlation terms) 的 LMM，我们不能使用 ANOVA 方法。显然，我们需要一种可用于所有 LMMs 的方法。理想情况下，该方法也可修改从而扩展并服务于 GLMMs. 与估计 \\(\\symbf \\beta\\) 和 \\(\\symbf b\\) 一样，我们转向最大似然——但有所不同。正如我们将看到的，方差分量的最大似然估计是有偏的。当目标是推断方差分量本身时，这本身就是一个问题。然而，推断通常侧重于可估函数 \\(\\symbf{K'\\beta}\\) 或可预测函数 \\(\\symbf{K'\\beta}+\\symbf{M'b}\\) 并且方差-协方差的分量只是达到目的——获得区间估计、检验假设等——的手段。在这种情况下，\\(\\symbf G\\) 和 \\(\\symbf R\\) 的有偏估计会产生连锁反应，使标准误和检验统计量都产生偏差。该问题的解决方案是受限（或残差）最大似然 (restricted – residual – maximum likelihood)，通常缩写为 REML.我们首先使用示例 3.6 中的多地点数据进行 ANOVA 和纯 ML 估计，然后介绍 REML 并展示它在实践中的实现方式。关于方差分量估计，有很多优秀的教材和独立的课程。我们这里的目标不是全面的阐述。我们只关注足够的背景和方法，使我们能够继续手头的任务——学习使用线性统计模型。对更多深度细节感兴趣的读者可参考 Searle et al. (1992) 或 Demidenko (2004) 等人的文本。","code":""},{"path":"chap5.html","id":"anova-估计","chapter":"第 5 章 GLMM 估计","heading":"ANOVA 估计","text":"示例 3.6 中的四处理多地点数据的 ANOVA 表为对于方差分量估计，我们只需要地点和残差的期望均方 (expected mean squares, EMS)，因此这些是唯一展示的 EMS. 残差的均方的期望值为 \\(\\sigma^2\\)，因此残差方差的估计值为 \\(\\hat{\\sigma}^2=MS(\\text{residual})=2.877\\)。地点均方的估计为 \\(\\sigma^2+4\\sigma_L^2\\)。求解 \\(\\sigma_L^2\\) 得到 \\(\\hat{\\sigma}_{L}^{2}=\\left(1/4\\right)\\left[MS\\left(\\mathrm{location}\\right)-MS\\left(\\mathrm{residual}\\right)\\right]=1.7532\\)。这些是方差分量的 ANOVA 估计。对于无缺失均衡数据的仅方差分量 LMMs，EMS 中方差分量的系数非常简单。这些系数的规则出现在大多数介绍性统计方法教材中，其基本理由出现在前面提到的方差分量教材中。如果数据不均衡或缺失，系数会变得更加混乱，但原则上，ANOVA 方法可以与任何仅方差分量 LMM 一起使用。但它只能用于 LMMs 的这个子集。因此，我们接下来讨论基于似然的方差-协方差估计。","code":""},{"path":"chap5.html","id":"最大似然","chapter":"第 5 章 GLMM 估计","heading":"最大似然","text":"首先我们需要建立符号。方便起见，在本节中，术语“协方差分量”和“协方差矩阵”一般指方差和协方差。令 \\(\\symbf\\sigma\\) 表示待估的协方差分量向量。对于我们刚刚考虑的多地点数据，\\(\\symbf\\sigma'=\\begin{bmatrix}\\sigma_L^2&\\sigma^2\\end{bmatrix}\\)。对于随机系数模型，\\(\\symbf\\sigma'=\\begin{bmatrix}\\sigma_0^2&\\sigma_{01}&\\sigma^2_1&\\sigma^2\\end{bmatrix}\\)。令 \\(\\hat{\\symbf\\sigma}\\) 表示协方差分量向量估计，即，\\(\\hat{\\symbf\\sigma}^{\\prime}=\\begin{bmatrix}\\hat{\\sigma}_L^2&\\hat{\\sigma}^2\\end{bmatrix}\\)。由于每个协方差阵—— \\(\\symbf G, \\symbf R\\) 和 \\(\\symbf V\\) ——依赖于 \\(\\symbf\\sigma\\) 的分量，使用符号 \\(\\symbf G(\\symbf\\sigma), \\symbf R(\\symbf\\sigma)\\) 和 \\(\\symbf V(\\symbf\\sigma)\\) 来表示具有已知协方差分量的 \\(\\symbf G, \\symbf R\\) 和 \\(\\symbf V\\)。事实上 \\(\\symbf G(\\symbf\\sigma), \\symbf R(\\symbf\\sigma)\\) 依赖于 \\(\\symbf\\sigma\\) 部分但不是全部元素，但该符号对于我们的使用来说足够清晰。对于协方差分量估计，我们使用符号 \\(\\hat{\\symbf{G}}=\\symbf{G}\\left(\\hat{\\symbf{\\sigma}}\\right),\\hat{\\symbf{R}}=\\symbf{R}\\left(\\hat{\\symbf{\\sigma}}\\right)\\) 和 \\(\\hat{\\symbf{V}}=\\symbf{V}\\left(\\hat{\\symbf{\\sigma}}\\right)\\)，保持使用符号 \\(\\symbf\\sigma\\)，例如 \\(\\symbf{G}\\left(\\hat{\\symbf{\\sigma}}\\right)\\)，除非上下文允许在没有歧义或混淆的情况下使用 \\(\\hat{\\symbf G}\\)。使用这种表示法，我们将 LMM 的边际对数似然写为\\[\\begin{align}\n\\ell\\left(\\symbf\\sigma;\\symbf\\beta,\\symbf{y}\\right)=&-\\biggl(\\frac n2\\biggr)\\mathrm{log}\\left(2\\pi\\right)-\\biggl(\\frac12\\biggr)\\mathrm{log}\\left(\\left|\\symbf{V}\\left(\\symbf{\\sigma}\\right)\\right|\\right)\\\\&-\\biggl(\\frac12\\biggr)\\bigl(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\bigr)^{\\prime}\\bigl[\\symbf{V}\\left(\\symbf{\\sigma}\\right)\\bigr]^{-1}\\bigl(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\bigr)\n\\tag{5.21}\n\\end{align}\\]我们通过令得分向量 \\(\\frac{\\partial\\ell\\left(\\symbf\\sigma;\\symbf\\beta,y\\right)}{\\partial\\symbf\\sigma}\\) 等于零并求解来获得 \\(\\symbf\\sigma\\) 各分量的最大似然估计。注意到 \\(\\frac{\\partial\\ell\\left(\\symbf\\sigma;\\symbf\\beta,y\\right)}{\\partial\\symbf\\sigma}\\) 是一个向量，其维度取决于待估协方差分量的总数。例如，对于随机系数 LMM：\\[\\begin{align}\n\\symbf{s}\\left(\\symbf{\\sigma}\\right)=\\frac{\\partial\\ell\\left(\\symbf{\\sigma};\\symbf{\\beta},y\\right)}{\\partial\\symbf{\\sigma}}=\\begin{bmatrix}\\frac{\\partial\\ell\\left(\\symbf{\\sigma};\\symbf{\\beta},y\\right)}{\\partial\\symbf{\\sigma}_0^2}\\\\\\frac{\\partial\\ell\\left(\\symbf{\\sigma};\\symbf{\\beta},y\\right)}{\\partial\\sigma_{01}}\\\\\\frac{\\partial\\ell\\left(\\symbf{\\sigma};\\symbf{\\beta},y\\right)}{\\partial\\sigma_1^2}\\\\\\frac{\\partial\\ell\\left(\\symbf{\\sigma};\\symbf{\\beta},y\\right)}{\\partial\\sigma^2}\\end{bmatrix}\n\\tag{5.22}\n\\end{align}\\]一般来说，\\(\\symbf{s}\\left(\\symbf{\\sigma}\\right)\\) 表示关于协方差估计的得分向量，其第 \\(\\) 个元素为 \\(\\frac{\\partial\\ell\\left(\\symbf\\sigma;\\symbf\\beta,y\\right)}{\\partial\\sigma_i}\\)，其中 \\(\\sigma_i\\) 表示向量 \\(\\symbf\\sigma\\) 的第 \\(\\) 个元素。例如，对于随机系数模型，\\(\\sigma_1\\equiv\\sigma_0^2,\\sigma_2\\equiv\\sigma_{01},\\sigma_3\\equiv\\sigma_1^2\\) 以及 \\(\\sigma_4\\equiv\\sigma^2\\)。虽然仅方差分量 LMMs 的最大似然估计方程有特定的形式，但我们的目标是给出一种可用于 LMMs 的一般方法。为此，我们使用 Newton-Raphson 和 Fisher 得分算法。这意味着我们需要 Hessian 矩阵—— \\(\\symbf H(\\symbf\\sigma)\\) ——用于 Newton-Raphson 以及信息矩阵—— \\(\\symbf (\\symbf\\sigma)\\) ——用于 Fisher 得分。Harville (1977) 推导了这些矩阵的元素。我们还需要得分向量的具体形式。它们都有两种形式，一种为最大似然（此处所示），另一种为 REML，如下所示。用于 \\(\\symbf\\sigma\\) 最大似然估计的得分向量的第 \\(\\) 个元素为\\[\\begin{align}\ns_{}\\left(\\symbf\\sigma\\right)=&\\;\\frac{\\partial\\ell\\left(\\symbf{\\sigma};\\symbf{y},\\symbf{\\beta}\\right)}{\\partial\\sigma_i}\\\\=&\\;-\\biggl(\\frac{1}{2}\\biggr)\\operatorname{trace}\\biggl[\\symbf{V}^{-1}\\biggl(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\biggr)\\biggr]\\\\&+\\left(\\frac12\\right)(\\symbf{y}-\\symbf{X}\\symbf{\\beta})^{\\prime}\\symbf{V}^{-1}\\left(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\right)\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)\n\\tag{5.23}\n\\end{align}\\]用于 \\(\\symbf\\sigma\\) 最大似然估计的 Hessian 矩阵的第 \\(\\) 个元素为\\[\\begin{align}\n\\frac{\\partial^2\\ell\\left(\\symbf{\\sigma};\\symbf{y},\\symbf{\\beta}\\right)}{\\partial\\sigma_i\\partial\\sigma_j} =&\\;-\\biggl(\\frac{1}{2}\\biggr)\\operatorname{trace}\\Biggl[\\symbf{V}^{-1}\\biggl(\\frac{\\partial^2\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i\\partial\\sigma_j}\\biggr)-\\symbf{V}^{-1}\\biggl(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\biggr)\\symbf{V}^{-1}\\biggl(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_j}\\biggr)\\Biggr]\\\\&\\;+\\scriptsize\\left(\\frac12\\right)\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)^{\\prime}\\symbf{V}^{-1}\\left[\\left(\\frac{\\partial^2\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i\\partial\\sigma_j}\\right)-2\\left(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\right)\\symbf{V}^{-1}\\left(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_j}\\right)\\right]\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)\n\\tag{5.24}\n\\end{align}\\]方便起见，我们可以将该元素表示为 \\(H_{ij}(\\symbf\\sigma)\\) 。用于 \\(\\symbf\\sigma\\) 最大似然估计的信息矩阵的第 \\(\\) 个元素为\\[\\begin{align}\n\\symbf{\\mathcal J}_{ij}\\left(\\symbf{\\sigma}\\right)=-E\\Bigg(\\frac{\\partial^2\\ell\\left(\\symbf{\\sigma};\\symbf{y},\\symbf{\\beta}\\right)}{\\partial\\sigma_i\\partial\\sigma_j}\\Bigg)=\\Bigg(\\frac{1}{2}\\Bigg)tr\\Bigg[\\symbf{V}^{-1}\\Bigg(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\Bigg)\\symbf{V}^{-1}\\Bigg(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_j}\\Bigg)\\Bigg]\n\\tag{5.25}\n\\end{align}\\]如果我们对随机系数模型实施 Newton-Raphson 程序，则估计方程为\\[\\begin{align}\n\\begin{bmatrix}\\sigma_1\\\\\\sigma_2\\\\\\sigma_3\\\\\\sigma_4\\end{bmatrix}=\\begin{bmatrix}\\tilde{\\sigma}_1\\\\\\tilde{\\sigma}_2\\\\\\tilde{\\sigma}_3\\\\\\tilde{\\sigma}_4\\end{bmatrix}-\n\\begin{bmatrix}H_{11}\\left(\\tilde{\\symbf\\sigma}\\right)&H_{12}\\left(\\tilde{\\symbf\\sigma}\\right)&H_{13}\\left(\\tilde{\\symbf\\sigma}\\right)&H_{14}\\left(\\tilde{\\symbf\\sigma}\\right)\\\\&H_{22}\\left(\\tilde{\\symbf\\sigma}\\right)&H_{23}\\left(\\tilde{\\symbf\\sigma}\\right)&H_{24}\\left(\\tilde{\\symbf\\sigma}\\right)\\\\&&H_{33}\\left(\\tilde{\\symbf\\sigma}\\right)&H_{34}\\left(\\tilde{\\symbf\\sigma}\\right)\\\\&&&H_{44}\\left(\\tilde{\\symbf\\sigma}\\right)\\end{bmatrix}^{-1}\\begin{bmatrix}s_1\\left(\\tilde{\\symbf\\sigma}\\right)\\\\s_2\\left(\\tilde{\\symbf\\sigma}\\right)\\\\s_3\\left(\\tilde{\\symbf\\sigma}\\right)\\\\s_4\\left(\\tilde{\\symbf\\sigma}\\right)\\end{bmatrix}\n\\tag{5.26}\n\\end{align}\\]\\(\\tilde{\\symbf\\sigma}\\) 及其元素 \\(\\tilde\\sigma_i\\) 表示来自先前迭代的协方差分量值。对于 Fisher 得分，将每个 \\(H_{ij}(\\symbf\\sigma)\\) 替换为 \\(\\mathcal J_{ij}(\\symbf\\sigma)\\)，并将符号从负改为正。建立程序的关键是确定导数。首先将随机系数模型的 \\(\\symbf V(\\symbf\\sigma)\\) 显式地写为矩阵项\\[\\symbf{V}(\\symbf{\\sigma})=\\begin{bmatrix}\\symbf{Z}_0&\\symbf{Z}_1\\end{bmatrix}\\Bigg(\\symbf I_4\\otimes\\begin{bmatrix}\\sigma_0^2&\\sigma_{01}\\\\\\sigma_{01}&\\sigma_1^2\\end{bmatrix}\\Bigg)\\Bigg[\\begin{matrix}\\symbf{Z}_0'\\\\\\symbf{Z}_1'\\end{matrix}\\Bigg]+\\symbf{}\\sigma^2\\]其中，\\(\\symbf Z_0\\) 和 \\(\\symbf Z_1\\) 分别是与随机截距效应和随机斜率效应相关的 \\(\\symbf Z\\) 的子矩阵（第 1 章的矩阵部分推导过）。所得导数为：\\[\\begin{aligned}\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_1}&=\\symbf{Z}_0\\symbf{Z}_0^{\\prime},\\quad\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_2}=\\symbf{Z}_1\\symbf{Z}_0^{\\prime}+\\symbf{Z}_0\\symbf{Z}_1^{\\prime}\\\\\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_3}&=\\symbf{Z}_1\\symbf{Z}_1^{\\prime},\\quad\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_4}=\\symbf{}\\end{aligned}\\]所有二阶导均为零。继续四处理多地点示例，最大似然估计为 \\(\\hat{\\sigma}_L^2=1.52\\) 和 \\(\\hat{\\sigma}^2=2.52\\)（ANOVA 估计分别为 1.75 和 2.88）。回想第 3 章，默认情况下，PROC GLIMMIX 生成的协方差参数估计等于 ANOVA 估计（这种情况发生在具有均衡数据的仅方差分量 LMM 中，但一般不会发生）。这是我们第一次说明与最大似然相关的向下偏差。回想你第一次遇到方差估计的情况：单个总体中随机样本的样本方差的常用公式：\\[S^2=\\frac{\\sum_i\\left(y_i-\\bar{y}\\right)^2}{n-1}\\]可能会有人问，若样本均值为 \\(\\bar{y}=\\frac{\\sum_iy}n\\)，我们为什么不将样本方差除以 \\(n\\)？碰巧的是 \\(\\frac{\\sum_i\\left(y_i-\\bar{y}\\right)^2}n\\) 是 \\(y_i\\mathrm{~..d.~}N(\\mu,\\sigma^2)\\) 时的最大似然估计。若除以 \\(n\\) 而非 \\(n-1\\)，你会得到一个向下偏差的估计。","code":""},{"path":"chap5.html","id":"受限最大似然","chapter":"第 5 章 GLMM 估计","heading":"受限最大似然","text":"事实证明，样本方差是你与 REML 估计的第一次相遇。最大似然方差估计的向下偏差有什么影响？置信区间和检验统计量都取决于标准误，而标准误又取决于方差分量估计。向下偏向的方差估计意味着向下偏差的标准误，这反过来又意味着过窄的可信区间（覆盖范围不足）和向上偏差的检验统计量（膨胀的 类错误率）。例如，在多地点示例中，使用 ANOVA 或 REML 方差估计对相等处理效应总体检验的 \\(F\\) 值为 7.04，但如果使用最大似然方差估计，则 \\(F\\) 值为 8.04. 虽然这两者的 \\(p\\) 值在大多数 \\(\\alpha\\) 标准下都具有统计学意义，但 ANOVA 或 REML 估计显然有可能产生与基于 ML 的结果不同的结论。作为偏差影响严重性的一个例子，Littell et al. (2006) 以 Cochran Cox (1957) 的一个不完全区组设计为例作为他们的 SAS\nMixed Models, 2nd ed 的开始。该不完全区组模型与本节的多地点示例相同：高斯数据、代替随机地点效应的随机区组效应以及固定处理。下表展示了方差估计以及由此产生的总体处理的 \\(F\\) 和 \\(p\\) 值。使用 Cochran Cox 示例中的设计运行 1000 次模拟，其中所有处理均值相等且方差比为 \\(\\sigma_B^2/\\sigma^2=0.5\\) 或 \\(\\sigma_B^2/\\sigma^2=5\\)，假定 \\(\\alpha = 0.05\\)，结果显示方差比对结果几乎没有影响，对于使用 ANOVA 和 REML 方差估计计算的 \\(F\\) 检验，观测拒绝率范围为 4% 到 6%（ANOVA 和 REML 之间几乎没有差别），但当使用 ML 方差估计时，拒绝率总是大于 20%（高达 28%）。这是 ML 方差估计的向下偏差对 LMMs 的典型影响，也是 REML 成为 LMM 方差估计事实上的金标准的原因。REML 思想最早出现在 20 世纪 50 年代的统计文献中。Patterson Thompson (1971) 提出了用于 LMMs 的综合 REML 理论。REML 的基本思想是考虑模型的固定效应后对似然进行最大化。我们不是最大化 \\(\\symbf{y}\\sim N\\big(\\symbf{X\\beta},\\symbf{V}\\big)\\) 的似然，而是根据 \\(\\symbf{K'y}\\) 的似然获得估计，其中 \\(\\symbf K\\) 是任何满足 \\(E(\\symbf{K'y})=\\symbf 0\\) 的矩阵，因此 \\(\\symbf{K'y}\\sim N\\left(\\symbf 0, \\symbf{K'VK}\\right)\\)。这在 \\(\\symbf\\sigma\\) 的估计中有效地移除了固定效应。\\(\\symbf{K'y}\\) 的似然称为 REML 似然。通常使用普通最小二乘残差算子 \\(\\symbf{-X}(\\symbf{X}^{\\prime}\\symbf{X})^{-}\\symbf{X}^{\\prime}\\)，并注意到 \\(E\\left\\{\\left[\\symbf{-X}(\\symbf{X'X})^-\\symbf{X'}\\right]\\symbf{y}\\right\\}=\\left[\\symbf{-X}(\\symbf{X'X})^-\\symbf{X'}\\right]\\symbf{X\\beta}=0\\) 以及假定 \\(\\left(\\symbf{X}^{\\prime}\\symbf{X}\\right)^-\\) 为 \\(\\symbf{X}^{\\prime}\\symbf{X}\\) 的 \\(\\symbf G^{(2)}\\)（自反）广义逆13。REML 对数似然为\\[\\begin{align}\n\\ell_R\\left(\\symbf{\\sigma};\\symbf{y}\\right)=&\\;-\\biggl(\\frac{n-p}{2}\\biggr)\\mathrm{log}\\left(2\\pi\\right)-\\biggl(\\frac{1}{2}\\biggr)\\mathrm{log}\\left(\\left|\\symbf{V}\\left(\\symbf{\\sigma}\\right)\\right|\\right)-\\biggl(\\frac{1}{2}\\biggr)\\mathrm{log}\\left(\\left|\\symbf{X}^{\\prime}\\left[\\symbf{V}\\left(\\symbf{\\sigma}\\right)\\right]^{-1}\\symbf{X}\\right|\\right)\\\\&\\;-\\biggl(\\frac{1}{2}\\biggr)\\symbf{r}'\\left[\\symbf{V}(\\symbf{\\sigma})\\right]^{-1}\\symbf{r}\n\\tag{5.27}\n\\end{align}\\]其中 \\(p=\\operatorname{rank}(\\symbf{X})\\) 以及 \\(\\symbf{r}=\\symbf{y}-\\symbf{X}\\Big(\\symbf{X}'\\Big[\\symbf{V}(\\symbf{\\sigma})\\Big]^{-1}\\symbf{X}\\Big)^-\\symbf{X}'\\Big[\\symbf{V}(\\symbf{\\sigma})\\Big]^{-1}\\symbf{y}\\)。请注意，\\(\\symbf r\\) 可视为 \\(\\symbf{y}-\\symbf{X}\\hat{\\symbf{\\beta}}_{ML}\\)，其中 \\(\\hat{\\symbf{\\beta}}_{ML}\\) 是 \\(\\symbf\\beta\\) 的最大似然估计。与最大似然一样，我们使用 Newton-Raphson 或 Fisher 得分来实现 REML. 从 REML 对数似然导出的得分向量和 Hessian 矩阵以及信息矩阵（同样遵循 Harville, 1977）为：得分的第 \\(\\) 个元素：\\[\\begin{align}\ns_{}\\left(\\symbf{\\sigma}\\right)=&\\;\\frac{\\partial\\ell_R\\left(\\symbf{\\sigma};\\symbf{y}\\right)}{\\partial\\sigma_i}=-\\Bigg(\\frac12\\Bigg)\\operatorname{trace}\\Bigg[\\symbf{P}\\Bigg(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\Bigg)\\Bigg]\\\\&+\\left(\\frac12\\right)(\\symbf{y}-\\symbf{X}\\symbf{\\beta})^{\\prime}\\symbf{V}^{-1}\\left(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\right)\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)\n\\tag{5.28}\n\\end{align}\\]\n其中 \\(\\symbf{P}=\\begin{bmatrix}\\symbf{V}(\\symbf{\\sigma})\\end{bmatrix}^{-1}-\\begin{bmatrix}\\symbf{V}(\\symbf{\\sigma})\\end{bmatrix}^{-1}\\symbf{X}\\bigg(\\symbf{X}'\\bigg[\\symbf{V}(\\symbf{\\sigma})\\bigg]^{-1}\\symbf{X}\\bigg)^{-}\\symbf{X}'\\bigg[\\symbf{V}(\\symbf{\\sigma})\\bigg]^{-1}\\)。Hessian 的第 \\(ij\\) 个元素：\\[\\begin{align}\n\\frac{\\partial^2\\ell_R\\left(\\symbf{\\sigma};\\symbf{y}\\right)}{\\partial\\sigma_i\\partial\\sigma_j} =&\\;-\\Bigg(\\frac{1}{2}\\Bigg)\\operatorname{trace}\\Bigg[\\symbf{P}\\Bigg(\\frac{\\partial^2\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i\\partial\\sigma_j}\\Bigg)-\\symbf{P}\\Bigg(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\Bigg)\\symbf{P}\\Bigg(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_j}\\Bigg)\\Bigg]  \\\\\n&\\;\\scriptsize+\\left(\\frac12\\right)\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)^{\\prime}\\symbf{V}^{-1}\\left[\\left(\\frac{\\partial^2\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i\\partial\\sigma_j}\\right)-2\\left(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\right)\\symbf{P}\\left(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_j}\\right)\\right]\\symbf{V}^{-1}\\left(\\symbf{y}-\\symbf{X}\\symbf{\\beta}\\right)\n\\tag{5.29}\n\\end{align}\\]信息的第 \\(ij\\) 个元素：\\[\\begin{align}\n\\symbf{\\mathcal J}_{ij}\\left(\\symbf{\\sigma}\\right)=-E\\Bigg(\\frac{\\partial^2\\ell_R\\left(\\symbf{\\sigma};\\symbf{y}\\right)}{\\partial\\sigma_i\\partial\\sigma_j}\\Bigg)=\\Bigg(\\frac12\\Bigg)\\operatorname{trace}\\Bigg[\\symbf{P}\\Bigg(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_i}\\Bigg)\\symbf{P}\\Bigg(\\frac{\\partial\\symbf{V}(\\symbf{\\sigma})}{\\partial\\sigma_j}\\Bigg)\\Bigg]\n\\tag{5.30}\n\\end{align}\\]除了在指定的地方用 \\(\\symbf P\\) 替换 \\(\\symbf V^{−1}\\) 之外，估计过程完全按照用于最大似然的 Newton-Raphson 或 Fisher 得分程序进行。总之，LMM 估计迭代地进行。该过程从 \\(\\symbf\\sigma\\) 的初始值——记为 \\(\\hat{\\symbf\\sigma}\\) ——开始。由此，计算 \\(\\symbf G(\\hat{\\symbf\\sigma})\\) 和 \\(\\symbf R(\\hat{\\symbf\\sigma})\\)，这使我们能够构建混合模型方程并计算 \\(\\symbf\\beta\\) 和 \\(\\symbf b\\) 的解。我们使用它们来确定得分向量和 Hessian 矩阵或信息矩阵的新值，从而使我们能够更新 \\(\\hat{\\symbf\\sigma}\\)。以这种方式继续估计直到收敛。","code":""},{"path":"chap5.html","id":"sec5-5","chapter":"第 5 章 GLMM 估计","heading":"5.5 广义线性混合模型","text":"现在我们来讨论本书中最具雄心的线性模型推广—— GLMM. 在这里，我们结合了广义线性模型和混合线性模型的基本特征。根据 LMM，我们允许线性预测器中存在随机模型效应。根据 GLM，我们放弃了经典线性模型的正态性假定——我们只假定以随机效应为条件的观测具有属于指数族的分布或至少具有拟似然。综上所述，GLMM 的基本特征是：线性预测器：\\(\\symbf{\\eta}=\\symbf{X}\\symbf{\\beta}+\\symbf{Z}\\symbf{b}\\)线性预测器：\\(\\symbf{\\eta}=\\symbf{X}\\symbf{\\beta}+\\symbf{Z}\\symbf{b}\\)分布：\\(\\symbf{b}\\sim N(\\symbf{0},\\symbf{G})\\)分布：\\(\\symbf{b}\\sim N(\\symbf{0},\\symbf{G})\\)分布或拟似然：\\(E\\big(\\symbf{y}\\mid\\symbf{b}\\big)=\\symbf{\\mu}\\mid\\symbf{b}\\)；\\(Var\\big(\\symbf{y}\\mid\\symbf{b}\\big)=\\symbf V_\\mu^{1/2}\\symbf{}\\symbf{V}_\\mu^{1/2}\\) 其中\n\\[\\symbf{V}_\\mu^{1/2}=\\operatorname{diag}\\bigg[\\sqrt{V(\\mu)}\\bigg]=\\operatorname{diag}\\bigg[\\sqrt{\\frac{\\partial^2b\\left(\\theta\\right)}{\\partial\\theta^2}}\\bigg]\\]\n以及\n\\[\\symbf{}=\\operatorname{diag}\\begin{bmatrix}1\\Big/(\\phi)\\end{bmatrix}\\]\n并且 \\(\\symbf y\\mid \\symbf b\\) 要么具有属于指数族的分布，要么具有期望值且（协）方差可用拟似然来表征。分布或拟似然：\\(E\\big(\\symbf{y}\\mid\\symbf{b}\\big)=\\symbf{\\mu}\\mid\\symbf{b}\\)；\\(Var\\big(\\symbf{y}\\mid\\symbf{b}\\big)=\\symbf V_\\mu^{1/2}\\symbf{}\\symbf{V}_\\mu^{1/2}\\) 其中\n\\[\\symbf{V}_\\mu^{1/2}=\\operatorname{diag}\\bigg[\\sqrt{V(\\mu)}\\bigg]=\\operatorname{diag}\\bigg[\\sqrt{\\frac{\\partial^2b\\left(\\theta\\right)}{\\partial\\theta^2}}\\bigg]\\]\n以及\n\\[\\symbf{}=\\operatorname{diag}\\begin{bmatrix}1\\Big/(\\phi)\\end{bmatrix}\\]\n并且 \\(\\symbf y\\mid \\symbf b\\) 要么具有属于指数族的分布，要么具有期望值且（协）方差可用拟似然来表征。连接：\\(\\symbf{\\eta}=g\\left(\\symbf{\\mu}\\mid\\symbf{b}\\right)\\) 或等价地 \\(\\symbf{X\\beta}+\\symbf{Zb}=h(\\symbf{\\eta})\\)。通常 \\(h(\\cdot)=g^{-1}(\\cdot)\\)。连接：\\(\\symbf{\\eta}=g\\left(\\symbf{\\mu}\\mid\\symbf{b}\\right)\\) 或等价地 \\(\\symbf{X\\beta}+\\symbf{Zb}=h(\\symbf{\\eta})\\)。通常 \\(h(\\cdot)=g^{-1}(\\cdot)\\)。以随机效应为条件的观测的拟似然为 \\(q\\ell(\\symbf y\\mid\\symbf b)=\\symbf{y'\\theta}-1'\\symbf{}b(\\symbf{\\theta})\\) 以及随机效应分布的对数似然为\\[\\ell\\left(\\symbf{b}\\right)=-{\\left(\\frac b2\\right)}\\mathrm{log}\\left(2\\pi\\right)-{\\left(\\frac12\\right)}\\mathrm{log}\\left(\\left|\\symbf{G}\\right|\\right)-{\\left(\\frac12\\right)}\\symbf{b}^{\\prime}\\symbf{G}^{-1}\\symbf{b}\\]因此，联合对数（拟）似然为 \\(\\ell(\\symbf{b})+q\\ell(\\symbf{y}\\mid\\symbf{b})\\)，边际（拟）似然为\\[\\begin{align}\n&\\;\\iint_{\\symbf{b}}[q\\ell\\left(\\symbf{y}\\mid\\symbf{b}\\right)+\\ell\\left(\\symbf{b}\\right)]\\,d\\symbf{b}=q\\ell\\left(\\symbf{y}\\right)\\\\=&\\;\\iint_{\\symbf{b}}\\left[\\symbf{y}^{\\prime}\\symbf{}\\symbf{\\theta}-1^{\\prime}\\symbf{}b\\left(\\symbf{\\theta}\\right)-\\left(\\frac{b}{2}\\right)\\mathrm{log}\\left(2\\pi\\right)-\\left(\\frac{1}{2}\\right)\\mathrm{log}\\left(\\left|\\symbf{G}\\right|\\right)-\\left(\\frac{1}{2}\\right)\\symbf{b}^{\\prime}\\symbf{G}^{-1}\\symbf{b}\\right]d\\symbf{b}\n\\tag{5.31}\n\\end{align}\\]如果 \\(\\symbf y\\mid\\symbf b\\) 有一个真正的分布，我们可以加上 \\(c(\\phi,\\symbf y)\\) 项以给出一个真对数似然。虽然 \\(q\\ell ( \\symbf y)\\) 或 \\(\\ell (\\symbf y)\\) 是良定的，但除了少数例外，进一步简化是不可能的。因此我们试图将一个本质上难以处理的函数最小化，这是一个棘手的问题。LMM 就是这样的例外——因为 \\(\\symbf y\\mid \\symbf b\\) 是高斯分布，所以联合分布以及由此得到的边际分布也是高斯分布，正如我们在 5.4 节中看到的那样。然而，一般来说，我们需要使用某种形式的近似。我们将在本文中使用的两种替代方案为：线性化 (Linearization)：特别是 PROC GLIMMIX 所使用的伪似然 (pseudo-likelihood) 法。积分近似 (Integral approximation)：PROC GLIMMIX 实现的两种方法是拉普拉斯近似 (Laplace\napproximation) 和自适应高斯-埃尔米特求积 (adaptive Gauss-Hermite quadrature).在本节的其余部分，我们将详细介绍伪似然法。积分近似法的介绍则相对简略。后续章节将重点关注这些方法在实际建模中的使用（以及在了解其局限性后不使用）。","code":""},{"path":"chap5.html","id":"sec5-5-1","chapter":"第 5 章 GLMM 估计","heading":"5.5.1 伪似然","text":"Schall (1991) 以及 Breslow Clayton (1993) 提出了用于 GLMM 估计的基于拟似然的线性化，在文献中称为惩罚拟似然 (penalized-quasi-likelihood, PQL). Wolfinger O’Connell (1993) 从拉普拉斯近似的角度出发，开发了伪似然法。两者的相似之处在于它们扩展了 5.3 节中的伪变量 \\(\\symbf y^*\\) 的概念，并最终得到了混合模型方程 (5.17) 的广义线性模型版本。这里优选伪似然，因为当条件分布属于指数族时，其分布假设是明确的（我们没有拟似然），并且也许更重要的是，伪似然表达了一个思想，即近似函数在很大程度上保留了高斯对数似然的结构，这允许我们使用类似 LMM 的估计方程来估计协方差分量以及线性预测器效应。我们从在 \\(\\tilde{\\symbf\\eta}\\) 处计算逆连接函数的泰勒级数展开开始。这源于 GLMM 的目标：通过 \\(h\\big(\\symbf{X\\beta+Zb}\\big)=h\\big(\\symbf{\\eta}\\big)\\) 对 \\(E\\left(\\symbf{y}\\mid\\symbf{b}\\right)=\\symbf{\\mu}\\mid\\symbf{b}\\) 建模。泰勒级数展开为\\[h\\left(\\symbf{\\eta}\\right)\\cong h\\left(\\tilde{\\symbf{\\eta}}\\right)+\\frac{\\partial h\\left(\\symbf{\\eta}\\right)}{\\partial\\symbf{\\eta}}\\Bigg|_{\\symbf{\\eta}=\\tilde{\\symbf{\\eta}}}\\left(\\symbf{\\eta}-\\tilde{\\symbf{\\eta}}\\right)\\]根据 (5.12) 引入的符号，定义\\[\\symbf{D}=\\operatorname{diag}\\biggl[\\frac{\\partial g\\bigl(\\symbf{\\mu}|\\symbf{b}\\bigr)}{\\partial\\symbf{\\mu}}\\biggr]\\]或等价地\\[\\symbf{D}^{-1}=\\operatorname{diag}\\biggl[\\frac{\\partial h(\\symbf{\\eta})}{\\partial\\symbf{\\eta}}\\biggr]\\]我们可将泰勒级数展开重写为 \\(h\\left(\\symbf{\\eta}\\right)\\cong h\\left(\\tilde{\\symbf{\\eta}}\\right)+\\tilde{\\symbf{D}}^{-1}\\left(\\symbf{X}\\symbf{\\beta}+\\symbf{Z}\\symbf{b}-\\symbf{X}\\tilde{\\symbf{\\beta}}-\\symbf{Z}\\tilde{\\symbf{b}}\\right)\\)，其中 \\(\\tilde{\\symbf D}\\) 为 \\(\\symbf D\\) 在 \\(\\tilde{\\symbf{\\eta}}=\\symbf{X}\\tilde{\\symbf{\\beta}}+\\symbf{Z}\\tilde{\\symbf{b}}\\) 处计算的值。重新排列项后得到 \\(\\tilde{\\symbf{D}}\\bigg[h(\\symbf{\\eta})-h\\big(\\tilde{\\symbf{\\eta}}\\big)\\bigg]+\\symbf{X}\\tilde{\\symbf{\\beta}}+\\symbf{Z}\\tilde{\\symbf{b}}\\cong\\symbf{X\\beta}+\\symbf{Zb}\\)。由此：\\[\\begin{align}\nE\\left(\\symbf{y}^*\\mid\\symbf{b}\\right)=\\tilde{\\symbf{D}}{\\left[h\\left(\\symbf{\\eta}\\right)-h\\left(\\tilde{\\symbf{\\eta}}\\right)\\right]}+\\symbf{X}\\tilde{\\symbf{\\beta}}+\\symbf{Z}\\tilde{\\symbf{b}}\\cong\\symbf{X}\\symbf{\\beta}+\\symbf{Z}\\symbf{b}\n\\tag{5.32}\n\\end{align}\\]以及\\[\\begin{align}\nVar\\left(\\symbf{y}^*\\mid\\symbf{b}\\right)=\\symbf{D}\\symbf{V}_\\mu^{1/2}\\symbf{}\\symbf{V}_\\mu^{1/2}\\symbf{D}\n\\tag{5.33}\n\\end{align}\\]此时你既可以使用广义最小二乘参数，其中 \\(Var\\left(\\symbf{y}^*|\\symbf{b}\\right)\\) 扮演 LMM 中 \\(\\symbf R\\) 的角色，也可以简单地将 \\(\\symbf y^*\\) 视为 LMM 中的响应变量。无论哪种方式，伪似然估计方程 (pseudo-likelihood estimating equations)，也称为广义混合模型方程 (generalized mixed model equations)，是用 \\(\\symbf y^*\\) 替换 \\(\\symbf y\\)、用 \\(\\symbf{DV}_\\mu^{1/2}\\symbf{AV}_\\mu^{1/2}\\symbf{D}\\) 替换 \\(\\symbf R\\) 的混合模型方程。回想，我们在 GLM 估计方程中定义了 \\(\\symbf W=\\left(\\symbf D\\symbf V\\symbf D\\right)^{-1}=\\left(\\symbf D\\symbf V_\\mu^{1/2}\\symbf \\symbf V_\\mu^{1/2}\\symbf D\\right)^{-1}\\)，因此 \\(\\symbf W\\) 替换了 \\(\\symbf R^{−1}\\)，从而伪似然广义混合模型方程为：\\[\\begin{align}\n\\begin{bmatrix}\\symbf{X'WX}&\\symbf{X'WZ}\\\\\\symbf{Z'WX}&\\symbf{Z'WZ+G}^{-1}\\end{bmatrix}\\begin{bmatrix}\\symbf{\\beta}\\\\\\symbf{b}\\end{bmatrix}=\\begin{bmatrix}\\symbf{X'Wy}^*\\\\\\symbf{Z'Wy}^*\\end{bmatrix}\n\\tag{5.34}\n\\end{align}\\]","code":""},{"path":"chap5.html","id":"方差-协方差的伪似然估计-5-5-2","chapter":"第 5 章 GLMM 估计","heading":"5.5.2 方差-协方差的伪似然估计 {5-5-2}","text":"以下几个结果对协方差分量估计是有用的。“边际”伪方差为 \\(Var\\left(\\symbf{y}^*\\right)=\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{W}^{-1}\\)，或 \\(\\symbf{ZGZ}^{\\prime}+\\symbf{DV_\\mu}^{1/2}\\symbf{AV_\\mu}^{1/2}\\symbf{D}\\)。记为\n\\[\\begin{align}\nV^*(\\symbf\\sigma)\n\\tag{5.35}\n\\end{align}\\]。“边际”伪方差为 \\(Var\\left(\\symbf{y}^*\\right)=\\symbf{Z}\\symbf{G}\\symbf{Z}^{\\prime}+\\symbf{W}^{-1}\\)，或 \\(\\symbf{ZGZ}^{\\prime}+\\symbf{DV_\\mu}^{1/2}\\symbf{AV_\\mu}^{1/2}\\symbf{D}\\)。记为\n\\[\\begin{align}\nV^*(\\symbf\\sigma)\n\\tag{5.35}\n\\end{align}\\]。伪对数似然函数为伪对数似然函数为\\[\\begin{align}\np\\ell\\left(\\symbf{\\beta},\\symbf{\\sigma};\\symbf{y}^*\\right)&=-\\biggl(\\frac n2\\biggr)\\mathrm{log}\\left(2\\pi\\right)-\\biggl(\\frac12\\biggr)\\mathrm{log}\\left(\\left|\\symbf{V}^*(\\symbf{\\sigma})\\right|\\right)\\\\&-\\biggl(\\frac12\\biggr)\\bigl(\\symbf{y}^*-\\symbf{X}\\symbf{\\beta}\\bigr)^{\\prime}\\bigl[\\symbf{V}^*(\\symbf{\\sigma})\\bigr]^{-1}\\bigl(\\symbf{y}^*-\\symbf{X}\\symbf{\\beta}\\bigr)\n\\tag{5.36}\n\\end{align}\\]REML 伪对数似然为\\[\\begin{align}\np\\ell_{_R}\\left(\\symbf\\sigma;\\symbf{y}^*\\right)=\\;&-\\biggl(\\frac{n-p}{2}\\biggr)\\mathrm{log}\\left(2\\pi\\right)-\\left(\\frac12\\right)\\mathrm{log}\\left(\\left|\\symbf{V}^*(\\symbf\\sigma)\\right|\\right)\\\\\\;&-\\left(\\frac12\\right)\\mathrm{log}\\left(\\left|\\symbf{X}^{\\prime}\\left[\\symbf{V}^*(\\symbf\\sigma)\\right]^{-1}\\symbf{X}\\right|\\right)-\\left(\\frac12\\right)\\left(\\symbf{r}^*\\right)^{\\prime}\\left[\\symbf{V}^*(\\symbf\\sigma)\\right]^{-1}\\symbf{r}^*\n\\tag{5.37}\n\\end{align}\\]由此，式 (5.24) 至 (5.29) 中给出的 ML 和 REML 得分向量项以及 ML 和 REML Hessian 和信息矩阵项可用于 GLMM 协方差分量估计。与 ML 和 REML 伪对数似然一样，简单地将 \\(\\symbf y\\) 替换为 \\(\\symbf y^*\\)，将 \\(\\symbf V(\\symbf\\sigma)\\) 替换为 \\(\\symbf V^*(\\symbf\\sigma)\\)。PROC GLIMMIX 使用缩写 RSPL 来指代使用 REML 版本的得分向量以及用于协方差估计的 Hessian 或信息矩阵实现的伪似然 (PL). 其全称为 Restricted Subject-specific Pseudo-Likelihood. 缩写 MSPL—— Maximum Subject-specific Pseudo-Likelihood ——表示使用 ML 版本的得分向量以及 Hessian 或信息矩阵进行协方差估计来实现的 PL. 从现在起我们将使用缩写 PL，仅在专门提及 GLIMMIX 程序时才会使用 GLIMMIX 相应的缩写。PL 与 LMM 估计的相似性赋予了它一些理想的特性：易于实现，以及能够适应所有非高斯形式的线性混合模型。PL 可视为线性模型真正通用的估计算法。对于高斯混合模型，PL 简化为混合模型方程 (5.17)；对于仅固定效应的非高斯模型，PL 简化为 GLM 估计方程 (5.15). 对于高斯仅固定效应模型，PL 简化为 GLS 估计方程，若 \\(\\symbf V =\\symbf \\sigma^2\\) 则进一步简化为普通最小二乘方程。换言之，正如 LMM, GLM 和 LM 都是 GLMM 的特例一样，它们的估计方程都是 PL 的特例。在第 6 章中我们还将看到，PL 允许我们将大多数与 LMM 相关的推断技术的推广到 GLMM. 在后续章节中，我们将更详细地探讨这些能力。不幸的是，PL 并不适用所有情况。PL 所依赖的线性化并不总能很好地逼近似然，从而得出可用的估计。当每个观察单元的伯努利试验数量较小时（极端情况是二进制数据），二项 GLMM 存在一个已知问题。另一类问题出现在参数接近分布极值时的双参数指数分布（例如贝塔和负二项）（例如，贝塔的期望比例接近 0 或 1，或者负二项的期望计数接近 0）。有关 PL 性能的深入评估，请参阅 Stroup Claassen (2020). 当我们继续展开后续章节时，我们将给出更多细节。这里的要点是：PL 不能用作 GLMM 一刀切式的估计方法。积分近似法为问题模型提供了替代方案。","code":""},{"path":"chap5.html","id":"sec5-5-3","chapter":"第 5 章 GLMM 估计","heading":"5.5.3 积分近似：拉普拉斯和求积","text":"对于问题模型，直接最大化似然提供了一种替代方案，它通常解决与基于 PL 的估计相关的问题。回想 (5.31)，边际似然涉及对高斯与指数族（拟）似然之积的积分，该乘积通常十分混乱且无法进一步简化。因此，虽然直接最大化通常是不可能的，但我们可用积分近似来逼近。两种有用的方法是高斯-埃尔米特求积 (Gauss-Hermite quadrature) 和拉普拉斯近似 (Laplace approximation).这些近似的基本思想如下：高斯-埃尔米特求积：\n\\[\\int f(x)e^{-x^2}dx\\cong\\sum_kw_kf\\left(x_k\\right)\\]\n其中 \\(w_k\\) 是权重，\\(x_k\\) 是横坐标。对于给定的 \\(k\\)，\\(w_k\\) 和 \\(x_k\\) 的值在标准数学表格参考书中给出，例如 Zwillinger (2018).高斯-埃尔米特求积：\n\\[\\int f(x)e^{-x^2}dx\\cong\\sum_kw_kf\\left(x_k\\right)\\]\n其中 \\(w_k\\) 是权重，\\(x_k\\) 是横坐标。对于给定的 \\(k\\)，\\(w_k\\) 和 \\(x_k\\) 的值在标准数学表格参考书中给出，例如 Zwillinger (2018).拉普拉斯近似：\n\\[\\int e^{h(x)}dx\\cong(2\\pi)^{-1/2}e^{h(\\tilde{x})}\\left|\\frac{\\partial^2h(x)}{\\partial x^2}\\Bigg|_{x=\\tilde{x}}\\right|^{-1/2}\\]\n其中 \\(\\tilde x\\) 表示最大化 \\(h(x)\\) 的 \\(x\\) 值。拉普拉斯近似：\n\\[\\int e^{h(x)}dx\\cong(2\\pi)^{-1/2}e^{h(\\tilde{x})}\\left|\\frac{\\partial^2h(x)}{\\partial x^2}\\Bigg|_{x=\\tilde{x}}\\right|^{-1/2}\\]\n其中 \\(\\tilde x\\) 表示最大化 \\(h(x)\\) 的 \\(x\\) 值。在第 1 — 3 章中，我们研究了几个 \\(y\\mid b\\sim \\text{Binomial}\\) 以及 \\(b\\sim\\text{Gaussina}\\) 的 GLMM 示例。 让我们使用此设定的一个非常简单的版本，即 \\(y\\mid b\\sim \\text{Binary}(\\pi)\\) 和 \\(b\\sim N(0,\\sigma^2)\\) 来说明这些近似。随着 GLMM 复杂度的增加，每种近似都可能变得非常复杂并且计算量很大，但这个简单的说明将满足我们的目的——这只是为了对近似的工作原理有一个概念性的了解。我们从高斯埃尔米特求积开始。观测的条件 p.d.f. 为 \\(f(y\\mid b)=p^y(1-p)^{1-y}\\)，其中 \\(y=0\\) 或 \\(1\\) 以及 \\(p=\\Pr\\{Y=1\\}\\)。随机效应的 p.d.f 为\\(f\\left(b\\right)=\\left(2\\pi\\right)^{-1/2}\\sigma^{-1}e^{\\small-\\frac{b^2}2}\\)。令 \\(w\\) 表示对应于连接的随机变量，我们有 \\(w\\sim N\\left(\\eta,\\sigma^2\\right)\\) 和 \\(p=\\frac{e^w}{1+e^w}\\)。我们将所得联合 p.d.f. 写为\\[\\left(\\frac{e^w}{1+e^w}\\right)^y\\left(\\frac1{1+e^w}\\right)^{1-y}\\left(2\\pi\\right)^{-1/2}\\sigma^{-1}e^{\\small-\\frac{(w-\\eta)^2}{2\\sigma^2}}\\]现在令 \\(z =\\frac{w-\\eta}{\\sigma}\\)，代入上式得到\\[\\left(\\frac{e^{\\eta-\\sigma z}}{1+e^{\\eta-\\sigma z}}\\right)^{y}\\left(\\frac{1}{1+e^{\\eta-\\sigma z}}\\right)^{1-y}\\left(2\\pi\\right)^{-1/2}\\sigma^{-1}e^{{-\\frac{z^{2}}{2}}}\\]现在我们可把边际似然写为\\[\\begin{align}\n\\int_{-\\infty}^\\infty\\left(\\frac{e^{(\\eta-\\sigma z)y}}{1+e^{\\eta-\\sigma z}}\\right)(2\\pi)^{-1/2}e^{-\\frac{z^2}2}dz\n\\tag{5.38}\n\\end{align}\\]最后，令 \\(x^2=z^2/2\\)，这得到\\[\\int_{-\\infty}^\\infty\\left(\\frac{e^{\\left(\\eta-\\sigma\\sqrt{2}x\\right)y}}{1+e^{\\eta-\\sigma\\sqrt{2}x}}\\right)(\\pi)^{-1/2}e^{-x^2}dx\\]我们现在认识到\\[f\\left(x\\right)=\\left(\\frac{e^{\\left(\\eta-\\sigma\\sqrt{2}x\\right)y}}{1+e^{\\eta-\\sigma\\sqrt{2}x}}\\right)\\left(\\pi\\right)^{-1/2}\\]是高斯-埃尔米特求积的必要函数：边际似然近似等于\\[\\sum_kw_k\\left(\\frac{e^{\\left(\\eta-\\sigma\\sqrt{2}x_k\\right)y}}{1+e^{\\eta-\\sigma\\sqrt{2}x_k}}\\right)(\\pi)^{-1/2}\\]对于拉普拉斯近似，我们回到 (5.38) 给出的边际分布。令 \\(h(z)=(\\eta+\\sigma z)y-\\log(1+e^{\\eta+\\sigma z})-\\frac12z^2\\) 给出拉普拉斯近似的理想形式，其中\\[\\frac{\\partial^2h(z)}{\\partial z^2}=-\\left(\\frac{\\sigma^2e^{\\eta+\\sigma z}}{\\left(1+e^{\\eta+\\sigma z}\\right)^2}+1\\right)\\]随着求积点数 \\(k\\) 的增加，高斯-埃尔米特求积的近似变得更加准确。然而，增加 \\(k\\) 也会增加程序的计算负担以至于无法实现。此外，存在一个收益递减点，这取决于数据和拟合的模型。许多实现求积的软件包，包括 GLIMMIX 程序，都是自适应的，这意味着它们有数据驱动的决策规则来选择名义上最优的求积点数，并且在迭代过程中使用相关估计的当前值对横坐标进行中心化和缩放。有关详细信息，请参阅 Pinheiro Bates (1995) 以及 SAS PROC GLIMMIX documentation (SAS Institute, 2019). 可在 SAS 中使用 PROC 语句中的 QPOINT 选项来覆盖求积点数的自适应规则。例如在某些情况下，模型的复杂性使得求积点选择程序在计算上不可行。在其他情况下，查看指定数量的求积点会发生什么是很有意义的。拉普拉斯近似可证明等价于具有一个求积点的高斯-埃尔米特程序。拉普拉斯程序的计算强度比求积程序小，并且在可使用的模型方面更加灵活。","code":""},{"path":"chap5.html","id":"sec5-5-4","chapter":"第 5 章 GLMM 估计","heading":"5.5.4 在实践中选择伪似然还是积分近似？","text":"GLMM 实践者可使用伪似然法和积分近似法，但问题是：应该使用哪一种方法？答案取决于两个考虑因素。第一个是协方差分量估计中的偏差及其对用于假设检验和区间估计的推断统计量的影响。第二个问题是是否需要模型选择工具，例如信息准则（在第 7 章中介绍并广泛用于具有序列或空间相关性的模型（第 ?? 章和第 ?? 章）。我们在 5.4.3 节中看到，在具有高斯数据的线性混合模型中，REML 是估计协方差分量的首选方法，因为它避免了 ML 估计的向下偏差及其对推断统计量的后续影响。然而，残差似然仅与高斯数据结合才有意义。对于具有非高斯数据的GLMM，没有对应于残差似然的概念。两种积分近似法——高斯-埃尔米特求积法和拉普拉斯法——都从近似似然中获得参数估计，包括协方差分量。因此，两者仅产生最大似然估计。另一方面，伪似然的 REML 版本 (REML-PL) 模拟了残差似然并获得类似 REML 的协方差分量估计。对于具有非高斯数据的 GLMM，REML 与 LM 协方差分量估计存在相同的问题，但更微妙。请参阅 Stroup Claassen (2020) 进行深入探索。Stroup Claassen 通过模拟证明，如果条件分布 \\(f ( \\symbf y \\mid \\symbf b)\\) 相当对称，REML-PL 会产生更准确的协变量分量估计、更好的 类错误控制和置信区间覆盖（即在模拟实验中，95% 置信区间实际包含了所需参数估计的比例），而求积法和拉普拉斯法的表现与仅 ML 法对高斯混合模型的表现非常相似。然而，当 \\(f ( \\symbf y \\mid \\symbf b)\\) 强不对称时，例如，对于 1) 二进制数据，或 2) 负二项数据的期望计数接近零或尺度参数 \\(\\phi\\) 远大于 1，或 3) 期望比例接近 0 或 1 的连续比例（贝塔）数据，伪似然表现不佳，相对于正交和拉普拉斯，在协方差分量估计中表现出相当大的偏差。第 11 章 ?? 节包含一个说明。由此得出的建议是：当有证据表明 \\(f ( \\symbf y \\mid \\symbf b)\\) 强不对称时，使用积分近似，若满足计算要求则使用求积法，否则使用拉普拉斯法。而当 \\(f ( \\symbf y \\mid \\symbf b)\\) 不是强不对称时，使用 REML-PL.最后一点，早期 GLMM 文献给人的印象是应始终首选积分近似法。参见，例如，Breslow Lin (1995), Lin Breslow (1996), Murray et al. (2004), Pinheiro Chao (2006) 以及 Bolker et al. (2009)。此外，SAS PROC GLIMMIX documentation 重点关注“PL 的缺点” (“side PL”). 对于 R 用户来说，R 的主要 GLMM 包 lme4 没有 PL 选项；自适应求积是唯一可用的计算算法。Stroup Claassen (2020) 记录的后续经验表明了截然相反的情况——在许多实际应用中，REML-PL 在许多实际应用中是强烈首选的。本书第三篇中的例子将对这一点进行说明和阐述。对于可比的拟合模型，应使用求积法或拉普拉斯法，而不是伪似然法。由于积分近似程序关注的是真实似然，而不是伪似然线性化，因此从似然中得出的统计量（例如拟合统计量和似然比检验）是良定的，而 PL 则不是。这仅是非高斯 GLMM 的问题；对于 LMM, GLM 和 LM，所有似然都是良定的。然而，对于非高斯 GLMM，积分法和拉普拉斯法计算了似然的合理近似值（或至少它们试图这样做），而 PL 则没有。当我们考虑相关误差 GLMM 时，我们将会看到拉普拉斯近似特别有用，因为它可为某些 GLMM（通常对于求积法过于复杂的模型）定义适当的相关误差模型，并允许使用拟合统计量或似然比统计量比较竞争模型。","code":""},{"path":"chap5.html","id":"exe5","chapter":"第 5 章 GLMM 估计","heading":"练习","text":"文件夹 “GLMM_2nd_Examples_and_Exercises/Files Chapter 5 Exercises” 包含两个文件，每个文件中都有一个 IML 程序，用于说明混合模型的估计方程。这两个文件是：\nTwo_way_MM_noAB.sas\nTwo_way_MM_withAB.sas\n“noAB” 文件展示了一个无 blk × trt 交互作用的模型，而 “withAB” 文件中的模型则有。每个文件都包含了用于拟合模型的 PROC GLIMMIX 语句以及 IML 语句，这些 IML 语句说明了 GLIMMIX 程序实现的计算过程。（在编写矩阵程序以实现第 5 章中介绍的估计方程时，你可以将 GLIMMIX 的运行结果作为“答案指南”来使用）。\n为每个文件中要拟合的模型写出所需的元素。所需元素包括：\n线性预测器\n随机效应的假定分布以及给定随机效应的观测分布\n连接函数\n\n修改每个程序，将用于 REML 方差分量估计的 Newton-Raphson 方程替换为 Fisher 得分算法。验证你的 IML/Fisher 得分程序中固定效应和方差分量估计的解与 GLIMMIX 程序和 IML/Newton-Raphson 程序的解是否一致。\n文件夹 “GLMM_2nd_Examples_and_Exercises/Files Chapter 5 Exercises” 包含两个文件，每个文件中都有一个 IML 程序，用于说明混合模型的估计方程。这两个文件是：Two_way_MM_noAB.sasTwo_way_MM_withAB.sas“noAB” 文件展示了一个无 blk × trt 交互作用的模型，而 “withAB” 文件中的模型则有。每个文件都包含了用于拟合模型的 PROC GLIMMIX 语句以及 IML 语句，这些 IML 语句说明了 GLIMMIX 程序实现的计算过程。（在编写矩阵程序以实现第 5 章中介绍的估计方程时，你可以将 GLIMMIX 的运行结果作为“答案指南”来使用）。为每个文件中要拟合的模型写出所需的元素。所需元素包括：\n线性预测器\n随机效应的假定分布以及给定随机效应的观测分布\n连接函数\n线性预测器随机效应的假定分布以及给定随机效应的观测分布连接函数修改每个程序，将用于 REML 方差分量估计的 Newton-Raphson 方程替换为 Fisher 得分算法。验证你的 IML/Fisher 得分程序中固定效应和方差分量估计的解与 GLIMMIX 程序和 IML/Newton-Raphson 程序的解是否一致。使用文件 Ch5_prob2_3.sas. 编写一个 IML 程序（或者如果你喜欢的话，编写一个等价的矩阵程序）来获取在 “Run 1” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。\n编写 “Run 1” 实现的模型所需的元素。\n验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。\n【提示：对于本习题以及下面的 3, 4, 5 题，你可能会发现使用 1. 中的 IML 程序作为模板很有帮助，按需修改以适应你正在获取的模型中的响应变量和分布】使用文件 Ch5_prob2_3.sas. 编写一个 IML 程序（或者如果你喜欢的话，编写一个等价的矩阵程序）来获取在 “Run 1” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。编写 “Run 1” 实现的模型所需的元素。验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。【提示：对于本习题以及下面的 3, 4, 5 题，你可能会发现使用 1. 中的 IML 程序作为模板很有帮助，按需修改以适应你正在获取的模型中的响应变量和分布】使用文件 Ch5_prob2_3.sas. 编写一个 IML 程序（或者如果你喜欢的话，编写一个等价的矩阵程序）来获取在 “Run 2” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。\n编写 “Run 2” 实现的模型所需的元素。\n验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。\n使用文件 Ch5_prob2_3.sas. 编写一个 IML 程序（或者如果你喜欢的话，编写一个等价的矩阵程序）来获取在 “Run 2” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。编写 “Run 2” 实现的模型所需的元素。验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。使用文件 Ch5_prob4_5.sas. 编写一个 IML 程序（或者如果你更喜欢，编写一个等价的矩阵程序）来获取在 “Run 1” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。\n编写 “Run 1” 实现的模型所需的元素。\n验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。\n使用文件 Ch5_prob4_5.sas. 编写一个 IML 程序（或者如果你更喜欢，编写一个等价的矩阵程序）来获取在 “Run 1” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。编写 “Run 1” 实现的模型所需的元素。验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。使用文件 Ch5_prob4_5.sas. 编写一个 IML 程序（或者如果你更喜欢，编写一个等价的矩阵程序）来获取在 “Run 2” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。\n编写 “Run 2” 实现的模型所需的元素。\n验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。\n使用文件 Ch5_prob4_5.sas. 编写一个 IML 程序（或者如果你更喜欢，编写一个等价的矩阵程序）来获取在 “Run 2” PROC GLIMMIX 程序中实现的模型的模型效应解和方差分量估计。编写 “Run 2” 实现的模型所需的元素。验证你的矩阵程序产生的解和估计与 GLIMMIX 程序产生的解和估计相等。","code":""},{"path":"chap6.html","id":"chap6","chapter":"第 6 章 推断（一）","heading":"第 6 章 推断（一）","text":"","code":""},{"path":"chap6.html","id":"sec6-1","chapter":"第 6 章 推断（一）","heading":"6.1 介绍","text":"在第 5 章中，我们重点讨论了 GLMM 估计。在第 6 - 7 章中，我们将注意力转向推断。第 6 章涉及在线性预测器效应 \\(\\symbf\\beta\\) 和 \\(\\symbf b\\) 上定义的可估和可预测函数。第 7 章涉及 \\(\\symbf\\sigma\\) 的元素：方差和协方差分量。我们所说的推断是指区间估计（置信区间的构建和解释）以及假设检验。对于线性预测器效应，所有推断都始于可估和可预测函数。这些在第 3 章介绍过。第 4 章正式定义了经典 LM 设定的可估函数。在本章中，我们将经典 LM 概念扩展到 GLMM 设定中的可估和可预测函数。其中包括处理均值、差异、几率比、对比、回归模型的预测值、析因处理结构的主/简单/交互效应等。对于混合模型，所有模型都具有总体平均、广义推断形式和个体特定或狭义推断形式。对于具有非恒等连接的模型，以上所有这些都被定义，并且在模型尺度上进行推断。然而，推断结果既可在模型尺度上表达，也可在数据尺度上表达。本章的目标是对“GLMM 黑匣子内部”所发生的事情形成一种强烈的、面向实际应用的意识。在许多情况下，我们会发现存在替代方法，还会发现熟练使用 GLMM 推断要求知晓哪些工具适用于哪种应用场景——以及何时即使可以使用但也不应使用某些工具。我们重点关注证明用于线性模型工作中区间估计和假设检验程序的基本思维过程，但我们会避免严格正式的、基于定理证明的数理统计学和概率论。在其中几个方面，我们借鉴了重要的矩阵理论与方法，其中一些在第 4 章中已介绍，其余部分则在附录“线性模型的矩阵代数”14中给出。","code":""},{"path":"chap6.html","id":"sec6-2","chapter":"第 6 章 推断（一）","heading":"6.2 GLMM 背景下的可估性","text":"回想第 3 章，若 \\(\\symbf{K'\\beta}\\) 可估，则表示为 \\(\\symbf{K'\\beta}+\\symbf{M'b}\\) 的模型效应线性组合称为可预测函数。对于仅固定效应模型，我们只能构造可估函数 \\(\\symbf{K'\\beta}\\)。对于混合模型，我们可以构造可估或可预测函数；令 \\(\\symbf M =\\symbf 0\\) 产生可估函数。我们使用可预测函数进行狭义或特定个体推断，这由 \\(\\symbf M\\) 所定义。我们使用可估函数进行广义或总体平均推断。在本节中，我们将扩展在 GLMM 设定中，\\(\\symbf{K'\\beta}\\) 必须满足的条件才能视为可估的正式标准。在后续小节中，我们将概述用于可估和可预测函数的两种最常见推断形式的策略：区间估计和假设检验。","code":""},{"path":"chap6.html","id":"sec6-2-1","chapter":"第 6 章 推断（一）","heading":"6.2.1 确定可估性的标准","text":"所有可估函数都有形如 \\(\\symbf{K'\\beta}\\) 的一般形式。然而，并不是所有 \\(\\symbf{K'\\beta}\\) 都是可估的。回想第 4 章，为了使 \\(\\symbf{K'\\beta}\\) 可估，我们必须能够将其写为 \\(E(\\symbf y )\\) 的线性组合；正式地\\[\\begin{align}\n\\text{若存在矩阵}\\;\\;\\symbf T \\;\\;使得\\;\\;\\symbf{T}′E(\\symbf y) = \\symbf{K'\\beta}\\;\\;则\\;\\; \\symbf{K'\\beta}\\;\\;可估\n\\tag{6.1}\n\\end{align}\\]GLMM 使用同一概念的扩展。考察候选矩阵可估性的一种方法是使用以下结果\\[\\begin{align}\n\\symbf{K'\\beta}\\;\\;可估当且仅当\\;\\;\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\symbf{X}=\\symbf{K}^{\\prime}\n\\tag{6.2}\n\\end{align}\\]其中 \\((\\symbf{X}^{\\prime}\\symbf{X})^-\\) 表示 \\(\\symbf{X}^{\\prime}\\symbf{X}\\) 的广义逆。遵循 Searle (1971) 我们证明该结论。首先，假设 \\(\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\symbf{X}=\\symbf{K}^{\\prime}\\)。令 \\(\\symbf T'=\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\) 得到 \\(\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\symbf{X}=\\symbf{T}^{\\prime}\\symbf{X}=\\symbf{K}^{\\prime}\\)，由定义可得 \\(\\symbf{K'\\beta}\\) 可估。现假设 \\(\\symbf{K'\\beta}\\) 可估，那么 \\(\\symbf K'=\\symbf{T'X}\\)，两边同时右乘 \\((\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\symbf{X}\\) 得到 \\(\\symbf{K^{\\prime}}(\\symbf{X^{\\prime}}\\symbf{X})^-(\\symbf{X^{\\prime}}\\symbf{X})=\\symbf{T^{\\prime}}\\symbf{X}(\\symbf{X^{\\prime}}\\symbf{X})^-(\\symbf{X^{\\prime}}\\symbf{X})=\\symbf{T^{\\prime}}\\symbf{X}=\\symbf{K^{\\prime}}\\)。从而得证。例如，在具有线性预测器 \\(\\mu+\\tau_i(=1,2)\\) 的两处理模型中，处理均值 \\(\\mu+\\tau_i\\) 和差异 \\(\\tau_1-\\tau_2\\) 可估的，但截距参数 \\(\\mu\\) 和处理效应参数 \\(\\tau_i\\) 本身不可估。我们可以从几个方面来看待这一点。首先，应用定义。要估计 \\(\\mu\\)，我们需要满足\\[\\mu=\\sum_{,j}t_{ij}E\\left(y_{ij}\\right)=\\sum_{,j}t_{ij}\\left(\\mu+\\tau_i\\right)=\\sum_{,j}t_{ij}\\mu+\\sum_{,j}t_{ij}\\tau_i\\]这要求 \\(\\sum_{,j}t_{ij}\\) 等于 \\(1\\)（对于 \\(\\mu\\)）和 \\(0\\)（对于每个 \\(\\tau_i\\)）同时发生，而这不可能。其次，我们可以应用 \\(\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{X})^-\\symbf{X}^{\\prime}\\symbf{X}=\\symbf{K}^{\\prime}\\) 标准。对于 \\(\\mu\\)，\\(\\symbf K'=\\begin{bmatrix}1&0&0\\end{bmatrix}\\)。但 \\(\\symbf{K^{\\prime}(X^{\\prime}X)}^{-}\\symbf{X^{\\prime}X}=\\begin{bmatrix}2/3&1/3&1/3\\end{bmatrix}\\neq\\symbf{K^{\\prime}}\\)。为什么可估性很重要？简言之，在非满秩模型中（例如，所有 ANOVA 型效应模型），对于效应本身的估计方程解没有内在意义。他们的解完全取决于所使用的广义逆，而理论告诉我们，构造广义逆的方法有无限多种。另一方面，可估函数对于广义逆的选择是不变的，因此具有赋值意义。效应估计本身没有任何合理的解释，而可估函数有。","code":""},{"path":"chap6.html","id":"sec6-2-2","chapter":"第 6 章 推断（一）","heading":"6.2.2 可估性与 GLMMs","text":"对于 LMMs，LM 可估性的扩展是直接的。LMM 对条件均值建模，即 \\(E(\\symbf{y}\\mid\\symbf{b})=\\symbf{X\\beta}+\\symbf{Zb}\\)。因此 \\(E\\big(\\symbf{y}\\big)=E\\left[E\\left(\\symbf{y}\\mid\\symbf{b}\\right)\\right]=\\symbf{X}\\symbf{\\beta}\\)。此时，LM 可估性理论直接适用。重要结果：对于混合模型（LMMs 和 GLMMs），可估性仅取决于固定效应 \\(\\symbf{K'\\beta}\\)。随机效应的线性组合与可估性标准无关。当我们考虑区组时，将区组定义为固定或随机效应的影响，以及这些问题扩展到具有多个随机变量来源的设计时（例如，裂区和重复测量数据），这一点变得非常重要。我们将在第 9 章和第 ?? 章详细讨论这些问题。使用非恒等连接的 GLMs 和 GLMMs 不直接对期望值进行建模。因此，LM 和 LMM 中的可估性定义不能按字面意思应用。相反，通过伪变量 \\(\\symbf y^*\\) 来考虑可估性标准。尽管 \\(E(\\symbf y^* )\\) 本身不是期望值，但从概念上可理解为使用 \\(\\symbf{X\\beta+Zb}\\) 对 \\(E(\\symbf y^* \\mid \\symbf b )\\) 进行建模。从某种意义上说，逆连接 \\(h(\\symbf{X\\beta+Zb})\\) 确实建模了 \\(E(\\symbf y \\mid \\symbf b )\\)。按照此思路，在 GLMM（和 GLM）中，若存在满足 \\(\\symbf T'E(\\symbf y^*) =\\symbf{K'\\beta}\\) 的矩阵 \\(\\symbf T\\)，则 \\(\\symbf{K'\\beta}\\) 定义为可估的。应用此定义，可估性完全取决于线性预测器的形式以及给定效应水平组合的观测存在与否。换言之，对于 LM 和 LMM 可估的函数 \\(\\symbf{K'\\beta}\\)，对于 GLM 和 GLMM 也是可估的。类似地，对于 LM 和 LMM 不可估的任何 \\(\\symbf{K'\\beta}\\)，对于 GLM 和 GLMM 也是不可估的。","code":""},{"path":"chap6.html","id":"sec6-3","chapter":"第 6 章 推断（一）","heading":"6.3 检验统计量、区间估计及相关的分布","text":"对 GLMM 位置度量的推断始于可预测函数 \\(\\symbf{K'\\beta}+\\symbf{M'b}\\)，其中 \\(\\symbf{K'\\beta}\\) 满足上文 6.2 节给出的可估性标准。为便于讨论，我们使用符号 \\(\\symbf\\psi=\\symbf{K'\\beta}+\\symbf{M'b}\\) 作为可预测函数的简写。我们所说的推断是指检验一个假设，例如 \\(H_0\\colon\\symbf\\psi=\\symbf\\psi_0\\operatorname{vs.}H_A\\colon\\symbf\\psi\\neq\\symbf\\psi_0\\)，或者获得 \\(\\symbf\\psi\\) 的区间估计。首先考虑 \\(\\symbf K\\) 和 \\(\\symbf M\\) 是向量的情况，因此 \\(\\psi=\\symbf{k'\\beta+m'b}\\) 是标量。基本方法本质上与入门统计学中教授的检验和置信区间方法没有什么不同。具体来说，我们需要两个量：点估计和点估计的标准误。将它们分别表示为 \\(\\hat{\\psi}\\) 和 \\(s.e.(\\hat{\\psi})\\)。一般来说，用于检验和区间估计的推断统计量是统计量 \\(\\frac{\\hat{\\psi}-E(\\hat{\\psi})}{s.e.(\\hat{\\psi})}\\) 的变体。我们将 \\(\\frac{\\hat{\\psi}-E(\\hat{\\psi})}{s.e.(\\hat{\\psi})}\\) 转换为与所需检验或区间估计相关的形式，若存在精确结果，则确定其分布，否则确定其近似分布。根据该基本形式，我们可以执行以下操作。检验一个假设。在原假设 \\(H_0\\colon\\psi=\\psi_0\\) 下，\\(E\\left(\\hat{\\psi}\\right)=\\psi_0\\)。我们使用检验统计量\\[\\begin{align}\n\\frac{\\hat{\\psi}-E(\\hat{\\psi})}{s.e.(\\hat{\\psi})}\n\\tag{6.3}\n\\end{align}\\]或\\[\\begin{align}\n\\left(\\frac{\\hat{\\psi}-E(\\hat{\\psi})}{s.e.(\\hat{\\psi})}\\right)^2\n\\tag{6.4}\n\\end{align}\\]作为评估 \\(H_0\\) 的标准。通常，若 \\(s.e.(\\hat{\\psi})\\) 已知，则 \\(\\frac{\\hat{\\psi}-E(\\hat{\\psi})}{s.e.(\\hat{\\psi})}\\) 的参考分布为 \\(N(0,1)\\) ——标准高斯（正态），而若 \\(s.e.(\\hat{\\psi})\\) 是估计的，则参考分布为 \\(t\\) 分布；而对于已知和估计的 \\(s.e.(\\hat{\\psi})\\)，对于 \\(\\left(\\frac{\\hat{\\psi}-E(\\hat{\\psi})}{s.e.(\\hat{\\psi})}\\right)^2\\)，参考分布分别为 \\(\\chi^2\\) 和 \\(F\\)。构造置信区间。我们使用通用公式\\[\\begin{align}\n\\hat{\\psi}\\pm\\left(“\\text{表格值}\"\\right){\\times}s.e.(\\hat{\\psi})\n\\tag{6.5}\n\\end{align}\\]其中“表格值”对于已知的和估计的标准误分别为 \\(N(0,1)\\) 和 \\(t\\) 分布。当 \\(\\symbf\\psi=\\symbf{K'\\beta}+\\symbf{M'b}\\) 是向量时，为检验假设，我们使用检验统计量 \\(\\left(\\frac{\\hat{\\psi}-E(\\hat{\\psi})}{s.e.(\\hat{\\psi})}\\right)^2\\) 的矩阵形式 \\(\\hat{\\symbf{\\psi}}^{\\prime}\\Big[Var\\big(\\hat{\\symbf{\\psi}}\\big)\\Big]^{-1}\\hat{\\symbf{\\psi}}\\)，这是第 4 章介绍的 Wald 统计量的 GLMM 形式，或 Wald 统计量的修改。以下各节给出了这些统计量的形式，具体取决于模型以及方差或其他尺度参数是否已知。对于 LM 和 LMM 的某些边际形式，我们可以导出精确的分布，如第 4 章所示。分布结果大致适用于其他线性模型：一般的 LMMs 和所有广义模型—— GLM 和 GLMM. 我们现在从 LM 开始开发这些结果。","code":""},{"path":"chap6.html","id":"sec6-3-1","chapter":"第 6 章 推断（一）","heading":"6.3.1 推断统计量的分布（一）—— 具有已知 \\(\\symbf V\\) 的 LM","text":"回想，我们通过线性预测器 \\(\\symbf\\eta=\\symbf\\mu=\\symbf{X\\beta}\\) 和响应变量分布 \\(\\symbf y\\sim N(\\symbf\\mu,\\symbf V)\\) 来指定 LM. 在此模型下，\\(\\symbf y\\sim N(\\symbf{X\\beta},\\symbf V)\\)。回想第 4 章，我们可使用广义最小二乘估计方程 \\(\\symbf{X'V}^{-1}\\symbf{X\\beta}=\\symbf{X'V}^{-1}\\symbf{y}\\) 得到 \\(\\symbf\\beta\\) 的一个解，表示为 \\(\\hat{\\symbf\\beta}\\)。因此 \\(\\tilde{\\symbf{\\beta}}=\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\\symbf{X'V}^{-1}\\symbf{y}\\)，其中 \\(\\left({\\symbf{X^{\\prime}V^{-1}X}}\\right)^-\\) 是 \\({\\symbf{X^{\\prime}V^{-1}X}}\\) 的广义逆。如果我们假定经典的“一般” LM，\\(\\symbf V=\\symbf \\sigma^2\\)，则估计简化为普通最小二乘，即 \\(\\tilde{\\symbf\\beta}=(\\symbf{X'X})^-\\symbf X'\\symbf V\\)。若 \\(\\symbf X\\) 满秩，则存在真逆 \\((\\symbf{X'V}^{-1}\\symbf{X})^{-1}\\)，则解 \\(\\hat{\\symbf\\beta}\\) 是唯一的，可以称为估计，并表示为 \\(\\tilde{\\symbf\\beta}\\)。否则，若 \\(\\symbf X\\) 不满秩，则解 \\(\\hat{\\symbf\\beta}\\) 不唯一，但若 \\(\\symbf{K'\\beta}\\) 可估，则 \\(\\symbf{K}'\\tilde{\\symbf\\beta}\\) 是唯一的。我们重点关注 \\(\\symbf X\\) 不满秩这一更一般的情况。在这种一般情况下，我们讨论的重点是 \\(\\hat{\\symbf{\\psi}}=\\symbf{K^{\\prime}}\\tilde{\\symbf{\\beta}}\\)。这里讨论的广义逆的任何结果也适用于存在真逆的模型，因为后者是前者的特例。在第 4 章中，我们得出了当 \\(\\symbf y \\sim N (\\symbf{X\\beta},\\symbf \\sigma^2)\\) 以及 \\(\\symbf y \\sim N (\\symbf{X\\beta}, \\boldsymbol{\\Sigma}\\sigma^2)\\) 已知 \\(\\sigma^2\\) 和 \\(\\boldsymbol\\Sigma\\) 时 LM 的结果。对于更一般的情况，\\(\\symbf y\\sim N(\\symbf{X\\beta},\\symbf V)\\)，其结果是稍微的扩展，如下所述。我们通过 \\(\\hat{\\symbf{\\psi}}=\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}=\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^{-1}\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{y}\\) 来估计可估函数。对于已知的 \\(\\symbf V\\)，我们可利用以下结论：若 \\(\\symbf y \\sim N (\\symbf\\mu,\\symbf V)\\) 以及 \\(\\symbf \\) 是已知的常数矩阵，则 \\(\\symbf{Ay} = N (\\symbf{\\mu},\\symbf{AVA}')\\)。令 \\(\\symbf{}=\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^-\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\)，假定 \\(\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^-\\) 为自反广义逆（见附录 ??），我们有 \\(E\\left(\\symbf{K}'\\tilde{\\symbf\\beta}\\right)=E\\left(\\symbf{Ay}\\right)=\\symbf{K}^{\\prime}\\big(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\big)^{-}\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{\\mu}=\\symbf{K}^{\\prime}\\big(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\big)^{-}\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\symbf{\\beta}=\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\)。再次假定利用广义逆，我们还有 \\(Var\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)=Var\\left(\\symbf{}\\symbf{y}\\right)\\symbf{~K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^-\\symbf{{X}}^{\\prime}\\symbf{V}^{-1}Var\\left(\\symbf{y}\\right)\\symbf{V}^{-1}\\symbf{X}(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X})^-\\symbf{{K}}=\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X})^-\\symbf{{K}}\\)。因此，对于已知的 \\(\\symbf V\\)：\\[\\begin{align}\n\\hat{\\symbf{\\psi}}=\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\sim N\\left(\\symbf{K}^{\\prime}\\symbf{\\beta},\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\right)\n\\tag{6.6}\n\\end{align}\\]由此可见，式 (6.3) 用于标量 \\(\\psi=\\symbf{k'\\beta}\\) 的检验统计量 \\(\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}\\) 有精确的 \\(N(0,1)\\) 分布，而检验统计量 \\(\\left(\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}\\right)^2\\) 有精确的 \\(\\chi^2_{(1)}\\) 分布。根据标准正态分布确定置信区间表达式 (6.5) 的“表格值”。对于向量 \\(\\symbf{\\psi} = \\symbf{K'}\\tilde{\\symbf\\beta}\\) 以及有关 \\(H_0\\colon \\symbf\\psi= \\symbf 0\\) 的检验，Wald 检验统计量 \\(\\hat{\\symbf{\\psi}}'\\left[Var\\left(\\hat{\\symbf{\\psi}}\\right)\\right]^{-1}\\hat{\\symbf{\\psi}}\\) 具有精确的 \\(\\chi^2_{\\operatorname{rank}(\\symbf\\psi)}\\) 分布。理由如下。根据第 4，\\(\\symbf{v^{\\prime}}\\symbf{}\\symbf{v}\\) 定义为向量 \\(\\symbf v\\) 的二次型。对于 Wald 统计量，\\(\\symbf v = \\hat{\\symbf\\psi}\\) 且 \\(\\symbf = Var(\\hat{\\symbf\\psi})\\)。此外，根据 (6.6)，我们知道 \\(\\hat{\\symbf\\psi}\\) 具有高斯分布。将 \\(\\hat{\\symbf\\psi}\\) 的方差表示为 \\(\\symbf V_{\\hat{\\symbf\\psi}}\\) ，我们知道如果 \\(\\symbf{AV}_{\\hat{\\symbf \\psi}}\\) 是幂等的，则二次型 \\(\\symbf{v}^{\\prime}\\symbf{}\\symbf{v}\\)，即 Wald 统计量 \\(\\hat{\\symbf{\\psi}}^{\\prime}\\Big[Var\\big(\\hat{\\symbf{\\psi}}\\big)\\Big]^{-1}\\hat{\\symbf{\\psi}}\\)，具有非中心卡方分布。对于 LM，\\(\\symbf{V}_{\\hat{\\symbf\\psi}}=\\symbf{K}'\\left(\\symbf{X}'\\symbf{V}^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\) 以及 \\(\\symbf{}=\\left[\\symbf{K}'\\left(\\symbf{X}'\\symbf{V}^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\right]^{-1}\\) 意味着 \\(\\symbf{AV}_{\\hat{\\symbf \\psi}}=\\symbf{}\\)，由此 \\(\\symbf{AV}_{\\hat{\\symbf \\psi}}\\) 是幂等的。因此，对于已知 \\(\\symbf V\\) 的 LM，二次型\\[\\begin{align}\n\\hat{\\symbf{\\psi}}'\\left[Var\\left(\\hat{\\symbf{\\psi}}\\right)\\right]^{-1}\\hat{\\symbf{\\psi}}=\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^{-1}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\sim\\chi_{\\operatorname{rank}(\\symbf{K}),\\varphi}^2\n\\tag{6.7}\n\\end{align}\\]其中令 \\(\\symbf{\\mu}_{\\hat{\\symbf{\\psi}}}=E\\left(\\hat{\\symbf{\\psi}}\\right)\\)\\[\\varphi=\\left(\\frac12\\right)\\symbf\\mu_{\\hat{\\symbf\\psi}}'\\symbf \\symbf\\mu_{\\hat{\\symbf\\psi}}'=\\frac12(\\symbf K'\\tilde{\\symbf\\beta})'\\left[\\symbf K'\\left(\\symbf X'\\symbf V^{-1}\\symbf X\\right)\\symbf K\\right]^{-1}\\symbf K'\\tilde{\\symbf\\beta}\\]表示非中心参数。在原假设下，\\(\\symbf\\psi=\\symbf K^{\\prime}\\symbf\\beta=\\symbf 0\\)，因此 \\(\\phi = 0\\) 并且 Wald 统计量具有中心 \\(\\chi^2_{\\operatorname{rank}(\\symbf{K})}\\) 分布。","code":""},{"path":"chap6.html","id":"sec6-3-2","chapter":"第 6 章 推断（一）","heading":"6.3.2 推断统计量的分布（二）—— 具有估计的 \\(\\symbf V\\) 的 LM","text":"若 \\(\\symbf V\\) 的协方差分量由向量 \\(\\symbf\\sigma\\) 表示，我们必须估计协方差参数向量 \\(\\symbf\\sigma\\) 并使用其估计来估计方差 \\(\\hat{\\symbf{V}}=\\symbf V(\\hat{\\symbf{\\sigma}})\\)。在第 4 章中，我们考虑了 \\(\\symbf{V}=\\symbf{}\\sigma^2\\) 和 \\(\\symbf{V}=\\boldsymbol{\\Sigma}\\sigma^2\\) 两种特殊情况，其中假定 \\(\\boldsymbol\\Sigma\\) 已知。在每种特殊情况下，唯一需要估计的协方差参数是 \\(\\sigma^2\\)。我们首先回顾了这两种特殊情况的结果，然后考虑了更一般的情况：当除 \\(\\sigma^2\\) 之外 \\(\\symbf\\sigma\\) 还涉及其他参数并且 \\(\\symbf\\sigma\\) 的所有参数都必须估计时。","code":""},{"path":"chap6.html","id":"sec6-3-2-1","chapter":"第 6 章 推断（一）","heading":"6.3.2.1 具有未知 \\(\\symbf V\\) 的 LM：情形 1 —— \\(\\symbf{V}=\\boldsymbol{\\Sigma}\\sigma^2\\)","text":"情形 1 的一个明显例子出现在 \\(\\boldsymbol\\Sigma = \\symbf \\) 时——经典的“一般” LM. 当 \\(\\boldsymbol\\Sigma \\ne\\symbf \\) 时，\\(\\boldsymbol\\Sigma\\) 通常描述相关结构。如果对相关结构有足够的了解，可以将其视为常数，情形 1 适用。回顾第 4 章的结果，我们用估计的 \\(\\hat\\sigma^2\\) 替换已知的 \\(\\sigma^2\\)。对于 \\(\\symbf V=\\symbf \\sigma^2\\)，\\(\\hat{\\sigma}^2=\\symbf{y}^{\\prime}\\big[\\symbf{}-\\symbf{X}^{\\prime}\\big(\\symbf{X}^{\\prime}\\symbf{X}\\big)^-{\\symbf{X}}\\big]\\symbf{y}\\Big/\\big[n-\\operatorname{rank}\\big(\\symbf{X}\\big)\\big]\\)，这在 LM 语境下为残差均方 MSR. 对于 \\(\\symbf{V}=\\boldsymbol{\\Sigma}\\sigma^2\\)，\\(\\hat{\\sigma}^2=\\left[\\symbf{y'}\\left(\\symbf{-X}\\left(\\symbf{X}'\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^{-1}\\symbf{X}'\\boldsymbol\\Sigma^{-1}\\right)^{\\prime}\\boldsymbol{\\Sigma}^{-1}\\left(\\symbf{-X}\\left(\\symbf{X}'\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^{-1}\\symbf{X}'\\boldsymbol\\Sigma^{-1}\\right)\\symbf{y}\\right]\\Bigg/\\big[n-\\operatorname{rank}\\big(\\symbf{X}\\big)\\big]\\)。对于标量 \\(\\psi=\\symbf{k'\\beta}\\)，对应于 (6.3), (6.4) 和 (6.5) 所给形式的检验统计量和区间估计及其相关分布为\\[\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}=\\frac{\\symbf{k}'\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)}{\\sqrt{\\symbf{k}'\\left(\\symbf{X}\\left(\\boldsymbol\\Sigma\\hat{\\sigma}^2\\right)^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}}}=\\frac{\\symbf{k}'\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)}{\\sqrt{\\hat{\\sigma}^2\\left[\\symbf{k}'\\left(\\symbf{X}\\left(\\boldsymbol\\Sigma\\right)^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}\\right]}}\\sim t_{\\left[n-\\operatorname{rank}(\\symbf{X})\\right]}\\]\\[\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}=\\frac{\\symbf{k}'\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)}{\\sqrt{\\symbf{k}'\\left(\\symbf{X}\\left(\\boldsymbol\\Sigma\\hat{\\sigma}^2\\right)^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}}}=\\frac{\\symbf{k}'\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)}{\\sqrt{\\hat{\\sigma}^2\\left[\\symbf{k}'\\left(\\symbf{X}\\left(\\boldsymbol\\Sigma\\right)^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}\\right]}}\\sim t_{\\left[n-\\operatorname{rank}(\\symbf{X})\\right]}\\]\\[\\left(\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}\\right)^2=\\left\\{\\left[\\symbf{k}^{\\prime}\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)\\right]^{\\prime}\\left[\\symbf{k}^{\\prime}\\left(\\symbf{X}\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}\\right]^{-1}\\symbf{k}^{\\prime}\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)\\right\\}\\Bigg/\\hat{\\sigma}^2\\sim F_{1,n-\\operatorname{rank}(\\symbf{X})}\\]\\[\\left(\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}\\right)^2=\\left\\{\\left[\\symbf{k}^{\\prime}\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)\\right]^{\\prime}\\left[\\symbf{k}^{\\prime}\\left(\\symbf{X}\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}\\right]^{-1}\\symbf{k}^{\\prime}\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}_0\\right)\\right\\}\\Bigg/\\hat{\\sigma}^2\\sim F_{1,n-\\operatorname{rank}(\\symbf{X})}\\]\\[\\hat{\\psi}\\pm(“\\text{表格值}\")\\times s.e.(\\hat{\\psi})\\text{ 成为 }\\symbf{k}^{\\prime}\\tilde{\\symbf{\\beta}}\\pm t_{\\left[n-\\operatorname{rank}(\\symbf{X}),\\alpha\\right]}\\sqrt{\\hat{\\sigma}^2\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}}\\]\\[\\hat{\\psi}\\pm(“\\text{表格值}\")\\times s.e.(\\hat{\\psi})\\text{ 成为 }\\symbf{k}^{\\prime}\\tilde{\\symbf{\\beta}}\\pm t_{\\left[n-\\operatorname{rank}(\\symbf{X}),\\alpha\\right]}\\sqrt{\\hat{\\sigma}^2\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^{-1}\\symbf{k}}\\]对于向量 \\(\\symbf\\psi=\\symbf{K'\\beta}\\)，我们使用 \\(F\\) 统计量来检验 \\(H_0\\colon \\symbf\\psi=\\symbf 0\\)，我们在第 4 章中将其表示为 Wald 统计量除以 \\(\\operatorname{rank} (\\symbf K)\\) ，即\\[\\begin{align}\n\\left(\\frac{\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\boldsymbol\\Sigma^{-1}\\symbf{X}\\right)^{-1}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}}{\\hat{\\sigma}^2}\\right)\\Bigg/\\operatorname{rank}\\left(\\symbf{K}\\right)\\sim F\\Big[\\operatorname{rank}\\left(\\symbf{K}\\right),n-\\operatorname{rank}\\left(\\symbf{X}\\right)\\Big]\n\\tag{6.8}\n\\end{align}\\]若 \\(\\symbf {V} = \\symbf \\sigma^2\\)，则 (6.8) 可表示为 \\(F = MSH/MSR\\)，其中 \\(MSH\\) 表示假设均方 (mean square hypothesis).","code":""},{"path":"chap6.html","id":"sec6-3-2-2","chapter":"第 6 章 推断（一）","heading":"6.3.2.2 具有未知 \\(\\symbf V\\) 的 LM：情形 2 ——需估计所有协方差分量","text":"情形 2 出现在协方差结构至少包括两个未知项的仅固定效应模型和使用边际形式的混合模型中。在后一种情况下，我们在第 5 章中看到，线性预测器为 \\(\\symbf\\eta=\\symbf\\mu=\\symbf{X\\beta}\\)，在该模型下，观测假定分布为 \\(\\symbf{y}\\thicksim N\\left(\\symbf{X}\\symbf{\\beta},\\symbf{Z}\\symbf{G}\\symbf{Z}'+\\symbf{R}\\right)\\)。对于未知的 \\(\\symbf V=\\symbf{Z}\\symbf{G}\\symbf{Z}'+\\symbf{R}\\)，必须估计至少一个来自 \\(\\symbf G\\) 的分量和一个来自 \\(\\symbf R\\) 的分量。对于仅固定效应模型，\\(\\symbf V=\\symbf R\\) 因此情形 2 意味着 \\(\\symbf R\\) 至少有两个协方差分量。例子包括复合对称（在第 3 章中介绍）和相关误差模型，例如重复测量或空间数据，我们将在第 7 章中介绍，并在第 ?? 章中更详细地讨论。正如我们目前所看到的，推断的两个关键部分是\\(\\symbf{K'\\tilde{\\beta}}\\) 的分布\\(\\symbf{K'\\tilde{\\beta}}\\) 的分布\\(\\frac{\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^{-1}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}}{\\operatorname{rank}(\\symbf{K})}\\) 的分布\\(\\frac{\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^{-1}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}}{\\operatorname{rank}(\\symbf{K})}\\) 的分布后者是 (6.8) 的推广：用 \\(\\hat{\\symbf{V}}=\\symbf V\\left(\\hat{\\symbf{\\sigma}}\\right)\\) 替换 \\(\\boldsymbol\\Sigma \\sigma^2\\)。我们在第 5 章中看到，我们通常通过 REML 估计来获得 \\(\\hat{\\symbf{\\sigma}}\\)。当我们如此做时，所讨论的两个分布不再分别为高斯分布和 \\(F\\) 分布。标准混合模型实践假定：对于标量 \\(\\psi=\\symbf{k'\\beta}\\)，\\(\\begin{align}\\frac{\\symbf{k}^{\\prime}\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}\\right)}{\\sqrt{\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^{-}\\symbf{k}}} \\text{ 近似 } \\sim t_{\\nu_2}\\tag{6.9}\\end{align}\\)对于标量 \\(\\psi=\\symbf{k'\\beta}\\)，\\(\\begin{align}\\frac{\\symbf{k}^{\\prime}\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}\\right)}{\\sqrt{\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^{-}\\symbf{k}}} \\text{ 近似 } \\sim t_{\\nu_2}\\tag{6.9}\\end{align}\\)对于向量 \\(\\symbf\\psi=\\symbf{K'\\beta}+\\symbf{M'b}\\)，\\(\\begin{align}\\frac{\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^{-1}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}}{\\operatorname{rank}(\\symbf{K})} \\text{ 近似 } \\sim F_{\\nu_1,\\nu_2,\\varphi}\\tag{6.10}\\end{align}\\)对于向量 \\(\\symbf\\psi=\\symbf{K'\\beta}+\\symbf{M'b}\\)，\\(\\begin{align}\\frac{\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^{-1}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}}{\\operatorname{rank}(\\symbf{K})} \\text{ 近似 } \\sim F_{\\nu_1,\\nu_2,\\varphi}\\tag{6.10}\\end{align}\\)其中，\\(\\nu_1 =\\operatorname{rank}(\\symbf K)\\) 表示 \\(F\\) 的分子自由度，\\(\\nu_2\\) 表示 \\(t\\) 的自由度和 \\(F\\) 的分母自由度，\\(\\varphi\\) 表示 \\(F\\) 的非中心参数。对于某些仅方差分量混合模型，并且仅针对这些模型的某些可估函数，我们可从框架方差分析中确定分母自由度 \\(\\nu_2\\)，例如，通过遵循第 2 章中描述的“Fisher会怎么做？”过程。对于所有其他情况，我们必须近似 \\(\\nu_2\\)。在 6.4.2 节中，我们将看到如何做到这一点。近似 (6.9) 和 (6.10) 的论证基于渐近理论。当 \\(N \\rightarrow \\infty\\) 时，\\(\\symbf V(\\hat{\\symbf\\sigma})\\rightarrow \\symbf V({\\symbf\\sigma})\\)（假定 \\(\\hat{\\symbf\\sigma}\\) 是使用 ANOVA, ML 或 REML 得到的 \\(\\symbf\\sigma\\) 的一致估计），因此 \\(\\symbf{K'}\\tilde{\\symbf\\beta}\\) 的极限分布为 \\(N\\left(\\symbf{K'\\beta},\\symbf{K'}\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\right)\\)，并且\\[\\frac{\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^{-1}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}}{\\operatorname{rank}(\\symbf{K})}\\]的极限分布为 \\(\\chi^2_{\\nu_1,\\varphi}\\)，并且（我们假定）它沿着当 \\(\\nu_2\\\\infty\\) 时 \\(F_{\\nu_1,\\nu_{2},\\varphi}\\\\chi_{\\nu_1,\\varphi}^2\\) 这条线接近极限。有关渐近理论的更多详细信息，请参阅 Vonesh Chinchilli (1997), Demidenko (2004) 以及 Jiang (2007) et al.在实践中，我们更多关注这些统计量的小样本表现，而不是它们的渐近分布。为什么？因为我们通常处理的数据观测数远小于渐近性质发挥作用所需的数量。自 20 世纪 90 年代初 SAS® 推出 PROC MIXED 以来，大量的模拟研究（其中大多数未发表）在 LMM 从业者中形成了某种口头传统。在 MIXED 推出后，其他混合模型软件开始大量涌现，使模拟研究能够轻松完成并得到广泛应用。到 20 世纪 90 年代末，人们一致认为近似值 (6.9) 和 (6.10) 的表现与其所宣称的一样，但有一点需要注意：这源于 Kacker Harville (1984) 以及 Kenward Roger (1997) 将 \\(\\symbf{K'}\\left(\\symbf{X'}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\) 作为 \\(\\symbf{K'}\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\) 的近似。一般来说，\\(\\symbf{K'}\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\) 具有向下的偏差。我们将在 6.4.3 节中讨论该问题、不解决的后果以及如何处理。","code":""},{"path":"chap6.html","id":"sec6-3-3","chapter":"第 6 章 推断（一）","heading":"6.3.3 推断统计量的分布（三）—— GLM","text":"与 LMM 一样，我们需要考虑两种情形。情形 1：假定的分布属于单参数指数族（例如二项和泊松分布）；均值决定了方差，我们不需要额外的方差估计。情形 2：我们有一个双参数指数族或拟似然；我们必须估计尺度参数。","code":""},{"path":"chap6.html","id":"sec6-3-3-1","chapter":"第 6 章 推断（一）","heading":"6.3.3.1 GLM ——情形 1：无需估计尺度参数","text":"我们使用两个渐近结果：\\(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}} \\sim\\) 近似\\[\\begin{align}\nN\\left(\\symbf{K'\\beta},\\symbf{K'}\\left(\\symbf{X'WX}\\right)^-\\symbf{{K}}\\right)\n\\tag{6.11}\n\\end{align}\\]\\(\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{W}\\symbf{X})^{-}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}} \\sim\\) 近似\\[\\begin{align}\n\\chi_{\\operatorname{rank}(\\symbf{K}),\\varphi}^2\n\\tag{6.12}\n\\end{align}\\]其中 \\(\\symbf W\\) 表示由 (5.13) 定义的伪变量的逆方差，非中心参数 \\(\\varphi=\\frac{1}{2}\\big(\\symbf{K'\\beta}\\big)^{\\prime}\\left[\\symbf{K'}\\big(\\symbf{X'WX}\\big)^-\\symbf{K}\\right]^{-1}\\symbf{K'\\beta}\\)。请注意，(6.11) 和 (6.12) 只是用 \\(\\symbf W\\) 替换 (6.6) 和 (6.7) 的 \\(\\symbf V^{−1}\\)。有兴趣的读者可参考 Wedderburn (1974), Vonesh Chinchilli (1997), Demidenko (2004) 等教材对渐近理论进行更深入的学习。对这些近似的小样本表现所积累的经验表明，它们的表现正如其宣称的那样，这对我们的目的来说就足够了。当我们在后续章节中讨论 GLM 应用时，将会遇到罕见的例外情况。应用这两个结果，我们使用标准高斯分布构建 GLMs 的置信区间并使用卡方分布检验假设。例如\\[\\symbf{k}^{\\prime}\\tilde{\\symbf{\\beta}}\\pm{Z}_{\\alpha/2}\\sqrt{\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{W}\\symbf{X})^{-}\\symbf{k}}\\]给出了在模型尺度上的双侧 \\(100(1−\\alpha)\\%\\) 置信区间。在有意义的情况下，我们通过将逆连接应用于模型尺度的置信限来获得数据尺度的置信限。我们在 6.3.3.2 节中进一步开发了使用 GLMs 进行假设检验的方法。","code":""},{"path":"chap6.html","id":"sec6-3-3-2","chapter":"第 6 章 推断（一）","heading":"6.3.3.2 GLM ——情形 2：估计了尺度参数","text":"这些 GLMs 出现在三种情况中：1) 响应变量属于双参数指数族，必须估计尺度参数；2) 我们定义了一个具有尺度参数的拟似然，以解释过度分散（泊松模型经常发生）；3) 在 \\(\\symbf W\\) 中嵌入一个工作相关矩阵（非高斯相关误差模型的常用策略）。我们在第 ?? 章和第 ?? 章讨论过度分散，在第 ?? 章讨论了相关误差 GLMs. 具有工作相关矩阵的相关误差 GLMs 在本书中称为边际模型，因为它们的目标是 \\(\\symbf y\\) 的边际均值，而不是 \\(E(\\symbf y\\mid \\symbf b)\\)。边际模型有时称为广义估计方程 (Generalized Estimating Equation, GEE) 模型。有关 GEE 的早期开发工作，请参阅 Zeger, Liang Albert (1988)；有关更广泛的讨论，请参阅 Diggle et al. (2002) 和 Hardin Hilbe (2003) 等文本。一般来说，这些模型是 GLMMs 的边际形式——我们在第 3 章中遇到了第一个例子，即具有复合对称工作相关的二项模型。边际模型始终是拟似然模型。基本结果涉及在 (6.10) 和 (6.11) 中用 \\(\\hat{\\symbf W}\\) 替换 \\(\\symbf W\\)，这又意味着我们使用 \\(t\\) 代替 \\(N (0, 1)\\) 以及使用 \\(F\\) 代替 \\(\\chi^2\\)。具体来说，对于标量 \\(\\symbf{k}'\\tilde{\\symbf\\beta}\\)，\\(\\frac{\\symbf{k}^{\\prime}\\left(\\tilde{\\symbf{\\beta}}-\\symbf{\\beta}\\right)}{\\sqrt{\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\hat{\\symbf{W}}\\symbf{X}\\right)^{-}\\symbf{k}}}\\sim\\) 近似\\[\\begin{align}\nt_{\\nu_2}\n\\tag{6.13}\n\\end{align}\\]\\(\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}\\right)^{\\prime}\\left[\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\hat{\\symbf{W}}\\symbf{X})^{-}\\symbf{K}\\right]^{-1}\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}} \\sim\\) 近似\\[\\begin{align}\nF_{\\nu_1,\\nu_2,\\varphi}\n\\tag{6.14}\n\\end{align}\\]对于仅涉及尺度参数估计的模型，\\(\\hat{\\symbf{W}}=\\symbf{DV}_\\mu^1\\hat{\\symbf{}}\\symbf{V}_\\mu^1\\symbf{D}\\)，其中15\\[\\hat{\\symbf{}}=\\operatorname{diag}\\left[\\frac{1}{\\big(\\hat{\\phi}\\big)}\\right]\\]以及 \\(\\hat{\\phi}\\) 表示尺度参数估计。对于具有工作相关的模型，我们将尺度矩阵 \\(\\symbf \\) 替换为工作相关矩阵，用 \\(\\symbf A_W\\) 表示。工作相关模型通常取决于多个参数，并且必须估计每个参数。例如，对于第 3 章介绍的复合对称工作相关，\\[\\symbf{}_W=\\phi{\\begin{bmatrix}1&\\rho&...&\\rho\\\\&1&...&\\rho\\\\&&\\ddots&\\vdots\\\\&&&1\\end{bmatrix}}\\]遵循协方差分量的符号约定，用 \\(\\symbf\\rho\\) 表示工作相关分量向量。例如对于复合对称性，\\(\\symbf\\rho'=\\begin{bmatrix}\\phi&\\rho\\end{bmatrix}\\)。然后我们将工作相关矩阵完全指定为 \\(\\symbf A_W (\\symbf\\rho)\\)，将其估计表示为 \\(\\hat{\\symbf{}}_W=\\symbf{}_W\\left(\\hat{\\symbf{\\rho}}\\right)\\)，其中 \\(\\hat{\\symbf\\rho}\\) 是工作相关分量估计向量，例如 \\(\\hat{\\symbf\\rho}'=\\begin{bmatrix}\\hat\\phi&\\hat\\rho\\end{bmatrix}\\)。我们可以通过将 \\(\\symbf\\sigma\\) 的分量作为第 5 章描述的伪似然方差估计程序中的项来估计它们。","code":""},{"path":"chap6.html","id":"sec6-3-4","chapter":"第 6 章 推断（一）","heading":"6.3.4 推断统计量的分布（四）——混合模型","text":"出于讨论的目的，我们可把高斯 LMM 和 GLMM 放在一起考虑。尽管它们的发展历史不同，但两者都使用相同的渐近结果。LMM 的结果借鉴了至少可追溯到 Eisenhart (1947) 的工作。开创性的论文是 Harville (1976) 和 Laird Ware (1982). GLMM 的结果是新近的，并且倾向于部分为渐近理论、部分为对 GLMM 应用的 LMM 程序的特别修改。与具有未知方差的 LM 和 GLM 一样，大量的模拟工作表明，除了我们将在 6.3 节和 6.4 节中讨论的一些注意事项外，这些近似的表现与宣称的一样。我们以 GLMM 的伪似然形式给出了基本结果，指出 LMM 方程是 LMM 使用恒等连接的特例，并且 LMM 的条件和边际形式的似然是高斯的，易于使用标准似然法来处理。回想 GLMM 估计方程：\\[\\begin{bmatrix}\\symbf{X'WX}&\\symbf{X'WZ}\\\\\\symbf{Z'WX}&\\symbf{Z'WZ+G}^{-1}\\end{bmatrix}\\begin{bmatrix}\\symbf{\\beta}\\\\\\symbf{b}\\end{bmatrix}=\\begin{bmatrix}\\symbf{X'Wy}^*\\\\\\symbf{Z'Wy}^*\\end{bmatrix}\\]其中\\[\\symbf{W}=\\left(\\symbf{DV}_\\mu^{1/2}\\symbf{AV}_\\mu^{1/2}\\symbf{D}\\right)^{-1},\\quad \\symbf{D}=\\frac{\\partial\\symbf{\\mu}}{\\partial\\symbf{\\eta}}\\]以及 \\(\\symbf{y}^{*}=g\\left(\\tilde{\\symbf{\\mu}}\\right)+\\symbf{D}\\left(\\symbf{y}-\\tilde{\\symbf{\\mu}}\\right)\\)。假定 \\(\\symbf{K'\\beta}\\) 满足可估性标准，以下是 \\(\\symbf{K'\\beta}+\\symbf{M'b}\\) 推断的基本结果：\\(\\symbf{K}^{\\prime}\\tilde{\\symbf\\beta}+\\symbf{M}^{\\prime}\\tilde{\\symbf b}\\) 为 \\(\\symbf\\psi=\\symbf K^{\\prime}\\symbf\\beta+\\symbf M^{\\prime}\\symbf b\\) 的最佳线性无偏预测估计 (estimated best linear unbiased predictor, e-BLUP). \\(\\tilde{\\symbf\\beta}\\) 和 \\(\\tilde{\\symbf b}\\) 是 GLMM 估计方程的解。当 \\(\\symbf M =\\symbf 0\\) 时，\\(\\symbf{K}^{\\prime}\\tilde{\\symbf\\beta}\\) 对于高斯模型是 \\(\\symbf {K'\\beta}\\) 的最佳线性无偏估计 (best linear unbiased estimate, BLUE)，对于 GLMM 则为“近似 BLUE” (“approximately BLUE”).","code":""},{"path":"chap6.html","id":"sec6-3-4-1","chapter":"第 6 章 推断（一）","heading":"6.3.4.1 当 \\(\\symbf G\\) 和 \\(\\symbf R\\) 分量向量 \\(\\symbf\\sigma\\) 已知时，以下结果适用","text":"令 \\(\\symbf C\\) 表示 GLMM 估计方程左侧的广义逆，即\\[\\symbf{C=}\\begin{bmatrix}\\symbf{X'WX}&\\symbf{X'WZ}\\\\\\symbf{Z'WX}&\\symbf{Z'WZ+G}^{-1}\\end{bmatrix}^-\\]令 \\(\\symbf{L}'=\\begin{bmatrix}\\symbf{K'}&\\symbf{M'}\\end{bmatrix}\\)\\[\\begin{align}\nVar\\bigg[\\symbf{K'\\tilde{\\symbf\\beta}+M'}\\bigg(\\tilde{\\symbf{b}}-\\symbf{b}\\bigg)\\bigg]=Var\\left(\\symbf{L'}\\left[\\begin{matrix}\\tilde{\\symbf{\\beta}}\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\end{matrix}\\right]\\right)=\\symbf{L'CL}\n\\tag{6.15}\n\\end{align}\\]其中 \\(\\symbf b\\) 是随机效应向量的实现值。该结果的完整推导出现在 Henderson (1975) 的附录中。该推导的摘要在关于重要矩阵结果的附录中提供。若 \\(\\symbf K\\) 和 \\(\\symbf M\\) 以及 \\(\\symbf L\\) 是向量，那么 \\(\\symbf{L'CL}\\) 是标量，\\(\\sqrt{\\symbf{L'CL}}=s.e.\\left(\\hat{{\\psi}}\\right)\\)。\\(\\symbf{K}'\\tilde{\\symbf{\\beta}}+\\symbf M'\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\) 的抽样分布近似为 \\(N(\\symbf{K'\\beta}+\\symbf{M'b},\\symbf{L'CL})\\)，或等价地，近似 \\(N(\\psi,\\symbf{L'CL})\\)，并且由此有：\\(\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}\\) 具有近似 \\(N(0,1)\\) 分布。这意味着：\\(\\frac{\\hat{\\psi}-\\psi_0}{s.e.(\\hat{\\psi})}\\) 具有近似 \\(N(0,1)\\) 分布。这意味着：\\(Z=\\frac{\\hat{\\psi}-\\psi_{0}}{\\sqrt{\\symbf{L'CL}}}\\) 可用作 \\(z\\) 统计量来检验形如 \\(H_0\\colon\\psi=\\psi_0\\) 的假设。\\(Z=\\frac{\\hat{\\psi}-\\psi_{0}}{\\sqrt{\\symbf{L'CL}}}\\) 可用作 \\(z\\) 统计量来检验形如 \\(H_0\\colon\\psi=\\psi_0\\) 的假设。\\(\\psi\\) 的置信区间为 \\(\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}+\\symbf{M}^{\\prime}\\tilde{\\symbf{b}}\\right)\\pm Z_{\\alpha/2}\\sqrt{\\symbf{L}^{\\prime}\\symbf{C}\\symbf{L}}\\)，其中 \\(1-\\alpha\\) 表示置信水平。\\(\\psi\\) 的置信区间为 \\(\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf{\\beta}}+\\symbf{M}^{\\prime}\\tilde{\\symbf{b}}\\right)\\pm Z_{\\alpha/2}\\sqrt{\\symbf{L}^{\\prime}\\symbf{C}\\symbf{L}}\\)，其中 \\(1-\\alpha\\) 表示置信水平。当 \\(\\symbf K\\) 和 \\(\\symbf M\\) 以及 \\(\\symbf L\\) 是多列矩阵而不是向量时，\\(\\symbf{L'}\\begin{bmatrix}\\tilde{\\symbf{\\beta}}\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\end{bmatrix}(\\symbf{L'CL})^{-1}\\begin{bmatrix}\\tilde{\\symbf{\\beta}}'\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)'\\end{bmatrix}\\symbf{L}\\sim\\) 近似当 \\(\\symbf K\\) 和 \\(\\symbf M\\) 以及 \\(\\symbf L\\) 是多列矩阵而不是向量时，\\(\\symbf{L'}\\begin{bmatrix}\\tilde{\\symbf{\\beta}}\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\end{bmatrix}(\\symbf{L'CL})^{-1}\\begin{bmatrix}\\tilde{\\symbf{\\beta}}'\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)'\\end{bmatrix}\\symbf{L}\\sim\\) 近似\\[\\begin{align}\n\\chi^2_{\\operatorname{rank}(\\symbf L)}\n\\tag{6.16}\n\\end{align}\\]重复一遍，以上所有结果都假定协方差分量是已知的，因此 \\(\\symbf W\\) 和 \\(\\symbf C\\) 是已知的。","code":""},{"path":"chap6.html","id":"sec6-3-4-2","chapter":"第 6 章 推断（一）","heading":"6.3.4.2 协方差分量未知时适用的结果","text":"这是比较现实的情况。协方差分量是未知的并且必须进行估计，这在 GLMM 中几乎无一例外。我们在第 4 章看到，在 LMM 中发生这种情况时，我们用估计值 \\(\\hat{\\symbf G}=\\symbf G(\\hat{\\symbf\\sigma})\\) 和 \\(\\hat{\\symbf R}=\\symbf R(\\hat{\\symbf\\sigma})\\) 替换 \\(\\symbf G\\) 和 \\(\\symbf R\\)。在 GLMM 中，若模型具有未知的尺度或工作相关分量需要估计，我们不是用 \\(\\hat{\\symbf R}\\) 替换 \\(\\symbf R\\)，而是用 \\(\\hat{\\symbf W}\\) 替换 \\(\\symbf W\\)。实际上，这意味着用 \\(\\hat{\\symbf C}\\) 替换 \\(\\symbf C\\)，并按需使用 \\(\\hat{\\symbf G},\\hat{\\symbf R}\\) 和 \\(\\hat{\\symbf W}\\)。如此会得到：\\[\\begin{align}Var\\left[\\symbf{K}'\\tilde{\\symbf\\beta}+\\symbf M'\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\right]=Var\\left(\\symbf{L'}\\left[\\begin{matrix}\\tilde{\\symbf{\\beta}}\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\end{matrix}\\right]\\right)\\cong\\symbf{L}'\\hat{\\symbf C}\\symbf L\n\\tag{6.17}\n\\end{align}\\]\\[\\begin{align}Var\\left[\\symbf{K}'\\tilde{\\symbf\\beta}+\\symbf M'\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\right]=Var\\left(\\symbf{L'}\\left[\\begin{matrix}\\tilde{\\symbf{\\beta}}\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\end{matrix}\\right]\\right)\\cong\\symbf{L}'\\hat{\\symbf C}\\symbf L\n\\tag{6.17}\n\\end{align}\\]对于标量 \\(\\symbf{L}'\\hat{\\symbf C}\\symbf L\\)，\n\\[\\begin{align}t=\\frac{\\hat{{\\psi}}-{\\psi}_0}{\\sqrt{\\symbf{L}'\\hat{\\symbf C}\\symbf L}}\\sim t_{{\\nu}_2}\n\\tag{6.18}\n\\end{align}\\]对于标量 \\(\\symbf{L}'\\hat{\\symbf C}\\symbf L\\)，\n\\[\\begin{align}t=\\frac{\\hat{{\\psi}}-{\\psi}_0}{\\sqrt{\\symbf{L}'\\hat{\\symbf C}\\symbf L}}\\sim t_{{\\nu}_2}\n\\tag{6.18}\n\\end{align}\\]其中 \\(\\nu_2\\) 表示估计 \\(\\symbf C\\) 所涉及的自由度。一般来说，我们必须近似 \\(\\nu_2\\) ——参见 6.4.2 节。从 (6.18) 可以看出，标量 \\(\\psi\\) 的双侧置信区间为 \\[\\left(\\symbf{K}^{\\prime}\\tilde{\\symbf\\beta}+\\symbf M^{\\prime}\\tilde{\\symbf b}\\right)\\pm t_{\\nu_2,\\alpha/2}\\sqrt{\\symbf{L^{\\prime}CL}}\\] 其中 \\(1− \\alpha\\) 表示置信水平。假设检验的基本结果为：\\[\\begin{align}\n\\symbf{L'}\\begin{bmatrix}\\tilde{\\symbf{\\beta}}\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\end{bmatrix}(\\symbf{L}'\\hat{\\symbf C}\\symbf{L})^{-1}\\begin{bmatrix}\\tilde{\\symbf{\\beta}}'\\\\\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)'\\end{bmatrix}\\symbf{L}\\Bigg/\\operatorname{rank}(\\symbf L)\\sim \\text{ 近似 } F_{\\nu_1,\\nu_2,\\varphi}\n\\tag{6.19}\n\\end{align}\\]其中 \\(\\nu_1 = \\operatorname{rank} (\\symbf L)\\)，\\(\\nu_2\\) 通常需要近似（见 6.4.2 节）， 表示非中心参数。因为 \\(\\tilde{\\symbf b}-\\symbf b\\) 的期望为零，所以非中心参数与 LM（对于高斯模型和更一般的具有恒等连接的模型）或 GLM（对于具有非恒等连接的模型）的一般情况具有相同的形式。与可估和可预测函数相关的关键统计量的分布到此结束。我们现在转向使用这些结果来检验假设。","code":""},{"path":"chap6.html","id":"sec6-4","chapter":"第 6 章 推断（一）","heading":"6.4 检验方法","text":"线性模型使用两种主要方法（似然比和基于 Wald 的方法）来检验有关可估和可预测函数的假设，即原假设可写为 \\(H_0\\colon:\\symbf\\psi=\\symbf\\psi_0\\) 的检验。在 6.4.1 节中，我们将讨论似然比检验。在 6.4.2 节中，我们讨论了基于 Wald 的检验。对于具有已知方差的 LMs，或者 \\(\\symbf V=\\boldsymbol\\Sigma\\sigma^2\\) 并且 \\(\\sigma^2\\) 是唯一未知的方差分量，我们证明了似然比和基于 Wald 的检验是相同的。对于 GLMMs，只有当我们使用积分近似（例如拉普拉斯和高斯-埃尔米特求积）估计模型效应时，似然比检验才有定义。对于伪似然估计，似然比是未定义的；你可以计算一个伪似然比，但含义可疑，因此不应使用。即使使用积分近似，似然比检验的价值也有限，因为其计算强度很大。因此，Wald 检验通常更实用，因此是真正的16 GLMMs 的首选。对于 GLM 和真正的 LMMs 来说，需要进行权衡。Wald 检验更方便。小样本表现有利于似然比检验，因为当似然比和基于 Wald 的检验之间存在差异时，该差异有利于似然比检验。对于大多数情况，没有明确的证据支持似然比优于基于 Wald 的检验，反之亦然。因此，对于大多数 GLM 和 LMM 应用，便利性成为首要考虑因素。除了检验协方差分量（第 6 章）之外，我们将使用基于 Wald 的统计量来处理第 8 章开始的大多数应用。","code":""},{"path":"chap6.html","id":"sec6-4-1","chapter":"第 6 章 推断（一）","heading":"6.4.1 似然比与偏差","text":"在其最简单的形式中，似然比检验涉及确定 \\(H_0\\) 下的似然，即当可估或可预测函数等于 \\(\\symbf\\psi_0\\) 时，以及使用第 5 章开发的估计算法得出的线性预测器 \\(\\symbf\\eta\\) 的最大似然估计所得到的似然，并使用他们的比值来评估假设。在似然比语言中，我们将原假设下的似然称为“简化模型” (“reduced model”) 下的似然，将备择假设下的似然称为“全” (“full) 模型下的似然。在这里，为清楚起见，我们分别用 \\(f (\\hat{\\symbf\\psi_0};\\symbf y)\\) 和 \\(f (\\hat{\\symbf\\eta};\\symbf y)\\) 表示简化模型（\\(H_0\\)）和全模型的似然。我们定义似然比为\\[\\Lambda=\\frac{f (\\hat{\\symbf\\psi_0};\\symbf y)}{f (\\hat{\\symbf\\eta};\\symbf y)}\\]当 \\(\\symbf y\\) 属于指数族时，似然的一般形式如 (5.1) 所示。Wedderburn (1974) 表明，你可以计算拟似然比（顾名思义，用拟似然替换似然），就好像有真实似然比一样。对于某些分布，例如高斯分布，我们可以确定似然比的精确分布。但对于一般情况，我们不能。对于大多数分布（和拟似然），似然比检验使用众所周知的结果，即 \\(-2\\log(\\Lambda)\\sim\\) 近似 \\(\\chi^2_{\\nu}\\)。对于可估和可预测函数的检验，卡方自由度 \\(\\nu=\\operatorname{rank}(\\symbf K)\\)。这意味着在实践中，似然比统计量为\\[\\begin{align}\n-2\\log\\left(\\Lambda\\right)=2\\ell\\left(\\tilde{\\symbf{\\eta}}\\right)-2\\ell\\left(\\hat{\\symbf{\\psi}}_0\\right)\n\\tag{6.20}\n\\end{align}\\]对 GLMs 很重要的似然比的一个特例称为偏差 (deviance). 偏差定义为 \\(-2\\) 乘以观测向量 \\(\\symbf y\\) 下的似然与在模型 \\(\\hat{\\symbf\\eta}=\\symbf X\\hat{\\symbf\\beta}\\) 下的似然的对数比，即\\[\\text{deviance}\\big(\\hat{\\symbf{\\eta}}\\big)=-2\\log\\Bigg(\\frac{f\\big(\\hat{\\symbf{\\eta}};\\symbf{y}\\big)}{f\\big(\\symbf{y};\\symbf{y}\\big)}\\Bigg)=2\\log\\Bigg[\\ell\\big(\\symbf{y}\\big)-\\ell\\big(\\hat{\\symbf{\\eta}}\\big)\\Bigg]\\]对于 \\(\\symbf y\\) 属于单参数指数族的 GLMs，偏差有明确的解释：模型拟合优度的度量。由于它具有近似卡方分布，因此偏差可用于正式检验拟合优度。我们通过观测数与全模型下参数数量之差来确定自由度，即 \\(N-\\operatorname{rank} (\\symbf X)\\)。对于双参数指数族模型，虽然偏差通常用于评估拟合优度，但由于涉及尺度参数估计，其含义不太明确。对于高斯模型，偏差不是拟合优度的度量。事实上，我们可以证明它是 ANOVA 中的 \\(SS(\\text{residual})\\) 项。对于 GLMs（以及具有已知方差的 LMs），我们可以使用偏差来检验 \\(H_{0}\\colon\\symbf{\\psi}=\\symbf{\\psi}_{0}\\)。我们通过比较全模型的偏差与简化模型的偏差来实现这一点。这给出 \\(\\text{deviance}\\left(\\symbf{\\psi}_0\\right)-\\text{deviance}\\left(\\symbf{\\eta}\\right)=2\\log\\left[\\ell\\left(\\symbf{y}\\right)-\\ell\\left(\\hat{\\symbf{\\psi}}_0\\right)\\right]-2\\log\\left[\\ell\\left(\\symbf{y}\\right)-\\ell\\left(\\hat{\\symbf{\\eta}}\\right)\\right]=2\\log\\left[\\ell\\left(\\hat{\\symbf{\\eta}}\\right)-\\ell\\left(\\hat{\\symbf{\\psi}}_0\\right)\\right]\\)，这是来自 (6.20) 的似然比统计量。由于偏差与似然比统计量之间的关系，似然比方法倾向于鼓励对具有多种效应的模型进行序贯检验 (sequential test). 序贯检验可能是可取的——例如，对于效应具有明显层次的多元回归模型——但它也可能产生无意义的结果，或至少是容易误解的结果。我们将在 6.4.4 节中探讨一些例子来加以说明。","code":""},{"path":"chap6.html","id":"sec6-4-2","chapter":"第 6 章 推断（一）","heading":"6.4.2 Wald 和近似 \\(F\\) 统计量","text":"Wald 统计量是 6.3 节中结果的直接应用。Wald 统计量的一般形式为 \\(\\hat{\\symbf{\\psi}}^{\\prime}\\bigg[Var\\big(\\hat{\\symbf{\\psi}}\\big)\\bigg]^{-1}\\hat{\\symbf{\\psi}}\\)，其中 \\(Var\\big(\\hat{\\symbf{\\psi}}\\big)\\) 已知。线性模型 Wald 统计量最一般的形式是式 (6.16)。式 (6.16) 是方差已知的 GLMM 的 Wald 统计量。式 (6.7) 和式 (6.12) 分别是式 (6.16) 在方差已知的 LM 和尺度参数已知的 GLM 中的特例。Wald 统计量假定具有卡方分布，其自由度由 \\(\\symbf\\psi\\) 的秩确定，即定义 \\(\\symbf\\psi\\) 的矩阵 \\(\\symbf K\\) 的秩。在实践中，我们知道 \\(Var\\big(\\hat{\\symbf{\\psi}}\\big)\\) 通常是未知的，必须进行估计。我们在 6.2 节中看到，我们可用 \\(Var\\big(\\hat{\\symbf{\\psi}}\\big)\\) 方差分量的估计来替换相应的元素，从而获得“估计的” Wald 统计量 \\(\\hat{\\symbf{\\psi}}^{\\prime}\\bigg[\\widehat{Var(\\hat{\\symbf{\\psi}})}\\bigg]^{-1}\\hat{\\symbf{\\psi}}\\)，其中 \\(\\widehat{Var(\\hat{\\symbf{\\psi}})}\\) 表示 \\(Var\\big(\\hat{\\symbf{\\psi}}\\big)\\) 的估计，然后将“估计”的 Wald 统计量除以 \\(\\symbf\\psi\\) 的秩，即 \\(\\hat{\\symbf{\\psi}}^{\\prime}\\bigg[\\widehat{Var(\\hat{\\symbf{\\psi}})}\\bigg]^{-1}\\hat{\\symbf{\\psi}}\\Bigg/\\operatorname{rank}(\\symbf\\psi)\\)，具有近似——在某些情况下是精确的—— \\(F\\) 分布。式 (6.19) 给出了用于线性模型近似 \\(F=\\widehat{\\text{Wald}}\\Big/\\operatorname{rank}(\\symbf K)\\) 统计量的最一般表达式。所有其他形式，式 (6.8), (6.10) 和 (6.14) 都是特殊情况。Wald 和近似 \\(F\\) 在线性模型推断中具有两大优势。首先，它们不需要估计两个不同的模型，而似然比检验需要估计全模型和简化模型（如果我们使用偏差方法，则需要估计数据下的似然）。此外，我们为感兴趣的可估或可预测函数明确定义了 Wald 和近似 \\(F\\)，因此我们确切地知道我们正在检验什么。似然比方法（顺便说一下，还有经典 ANOVA 的缩减平方和）鼓励基于序贯拟合的检验。正如我们将在下一节中看到的，当我们使用序贯检验方法时，并不总是清楚我们实际检验的是什么。最后，对于使用伪似然估计的 GLMMs，不存在似然比统计量。你可以根据全模型的伪似然和简化模型的伪似然计算伪似然比，但每个伪似然都基于不同的 \\(\\symbf y^*\\)，该 \\(\\symbf y^*\\) 对于特定的模型是唯一的。因此，目前还不清楚伪似然比意味着什么。无论如何，这都不是检验假设的合适基础。另一方面，对于 GLMM，Wald 和近似 \\(F\\) 是良定的，并且近似 \\(F\\) 似乎在模拟研究中表现良好。图 6.1 显示了具有 10 个配对的两处理配对设计的模拟结果。线性预测器为 \\(\\mathrm{logit}\\Big(\\pi_{ij}\\Big)=\\eta+\\tau_i+p_j\\)，其中 \\(p_j\\mathrm{~iid~}N\\left(0,\\sigma_P^2\\right)\\) ——该图为 \\(H_0\\colon\\tau_1=\\tau_2\\) 的近似 \\(F\\) 统计量的经验 p.d.f. 与实际的中心 \\(F_{1,9}\\) 的图形。","code":""},{"path":"chap6.html","id":"sec6-4-3","chapter":"第 6 章 推断（一）","heading":"6.4.3 特例：\\(\\symbf V=\\symbf I\\sigma^2\\) 的高斯 LM","text":"当我们引入似然比检验时，我们说在大多数情况下，我们无法确定似然比的精确分布，因此我们依赖于 \\(-2\\log(\\Lambda)\\) 具有近似 \\(\\chi^2\\) 分布这一结果。而高斯 LM 是我们可以确定精确分布的模型。事实上，在一定的条件下，我们可以证明对于高斯 LM，似然比和 Wald 检验是等价的。假设我们要检验 \\(H_0\\colon\\symbf{K^{\\prime}\\beta}=\\symbf{K^{\\prime}\\beta}_0\\)。我们首先计算全模型下的偏差。如此会得到 \\(\\text{deviance}\\left(\\symbf{K^{\\prime}\\tilde{\\beta}}\\right)\\)，对于高斯 LM，该偏差等于全模型下的 \\(SS(\\text{residual})\\)，记作 \\(SSR_F\\)。然后我们计算原假设下的偏差：\\(\\text{deviance}\\left(\\symbf{K}^{\\prime}{\\symbf\\beta_0}\\right)=SSR_0\\)，其中 \\(SSR_0\\) 表示原假设下 \\(SS(\\text{residual})\\)。似然比统计量是两个偏差项之差，以平方和项表示，可写为 \\(SSR_0-SSR_F=SSH\\)，其中 \\(SSH\\) 等于由可估函数 \\(\\symbf {K'\\beta}\\) 定义的对比的平方和。我们知道 \\(SSH\\) 是一个二次型，以及 \\(SSH/\\sigma^2\\sim\\chi_{\\operatorname{rank}(\\symbf{K})}^2\\)。如果我们知道方差，我们就可以使用它并合理地进行卡方检验。如果我们不知道 \\(\\sigma^2\\)，那么使用偏差统计量来检验 \\(H_0\\) 显然是站不住脚的（除非 \\(\\sigma^2\\) 恰好接近 1）。现在，很容易证明 \\(H_0\\colon\\symbf{K^{\\prime}\\beta}=\\symbf{K^{\\prime}\\beta}_0\\) 的 Wald 统计量也是 \\(SSH/\\sigma^2\\)（即对于高斯 LM，Wald 和似然比统计量是相同的），因此我们面临着与未知 \\(\\sigma^2\\) 的似然比统计量相同的困境。我们该怎么办？按照导出式 (6.8) 的过程，我们用 \\(\\hat\\sigma^2\\) 替换 \\(\\sigma^2\\)。\\(\\sigma^2\\) 的 REML 估计恰好为 \\(SSR_F\\Big/[N-\\operatorname{rank}(\\symbf X)]\\)，我们也称为全模型下的 \\(MS(\\text{residual})\\)，记为 \\(MSR_F\\)。用 \\(\\hat\\sigma^2\\) 替换 \\(\\sigma^2\\) 得到 \\(SSH/MSR_F\\)。我们不知道该统计量的分布，但如果我们将它除以 \\(\\operatorname(\\symbf K)\\)，我们就得到 \\(MSH/MSR_F\\)，我们立即意识到其为 \\(F\\) 统计量，或者用更正式的分布理论术语来说，是两个独立卡方随机变量除以它们各自的自由度的比——即具有 \\(F\\) 分布的随机变量。请注意，\\(SSH/MSR_F\\) 是 \\(\\widehat{\\text{Wald}}\\) 的简单形式，\\(F\\) 统计量只是 \\(\\widehat{\\text{Wald}}\\Big/\\operatorname{rank}(\\symbf K)\\)。","code":""},{"path":"chap6.html","id":"sec6-4-4","chapter":"第 6 章 推断（一）","heading":"6.4.4 多效应模型和检验顺序","text":"基于经典方差分析的线性模型理论区分了“部分” (“partial”) 和“序贯” (“sequential”) 平方和。部分平方和也称为“调整的”平方和。SAS® 线性模型程序（GLM, GENMOD, MIXED 和 GLIMMIX）将序贯平方和（或使用序贯程序中隐含的可估函数的程序）称为 “型 SS”，将部分或调整的平方和（或其 GLM, LMM 和 GLMM 类似物）称为 “II 型 SS” 或更常见的 “III 型 SS”。对于经典的“一般” LM，序贯和部分 SS 往往会使学生和从业者感到困惑，而不是促进线性模型方法的合理和适当使用。GLM, LMM 和 GLMM 加剧了这一问题，因为平方和和均方没有意义。当在适当的场景中使用时，序贯地拟合模型的想法确实具有一定的意义——拟合复杂度存在明显的递增顺序的模型，例如多项式回归。当应用于处理效应模型时，序贯和部分失去了意义。对于这些模型，最好从可估函数的角度来思考：哪些 \\(\\symbf K\\) 定义了模型参数的线性组合，这些参数 1) 以清楚、直接和有意义的方式处理实际推断目标，2) 确实是可估的。我们将在第 8 章更详细地讨论这个主题，并提供额外的理论背景。在这里，我们举两个例子来说明这些问题。其意义源于以下事实：似然比检验倾向于鼓励序贯思维，而 Wald 和近似 \\(F\\) 统计量倾向于鼓励可估函数思维。每种方法都有其优点，但每种方法都可能得到胡说八道的结果。了解其中的区别很重要。示例 6.1  (二项响应、多项多元回归) 本例数据在 SAS Data Program Library 显示为 Data Set 6.1. 预测变量为 \\(X\\)，响应变量为二项的。\\(X\\) 的每个水平都有两个二项观测：\\(N\\) 表示给定观测的独立伯努利试验的数量，\\(F\\) 表示“成功”的数量。图 6.2 显示了观测 logits，即 \\(\\log\\left(\\frac{F/N}{1-\\left(F/N\\right)}\\right)\\)，关于 \\(X\\) 的图形。我们可以看到，观测 logits 关于 \\(X\\) 的模式显然是二次的。\n为实现序贯拟合，我们从线性预测器 \\(\\eta=\\beta_0+\\beta_1X\\) 和连接 \\(\\eta=\\operatorname{logit}(\\pi)\\) 开始。对于该仅线性效应 logistic 模型，检验 \\(H_0{\\colon{\\beta}}_1=0\\) 的似然比统计量为 8.49，\\(p\\) 值为 0.0036，Wald 统计量为 8.26，\\(p\\) 值为 0.004。然而，偏差为 95.61，d.f 为 12。表明严重欠拟合——考虑到图 6.2 中观察到的二次模式，这并不奇怪。接下来，我们添加二次项：线性预测器为 \\(\\eta=\\beta_0+\\beta_1X+\\beta_2X^2\\)。现在我们检验 \\(H_0\\colon\\beta_2\\mid\\beta_1=0\\)，即，在线性效应之外，由二次效应解释的额外变异性。此检验的似然比统计量为 63.85，\\(p < 0.0001\\)，Wald 统计量为 58.45，\\(p < 0.0001\\)。偏差为 31.75，d.f 为 11。当我们添加三次、四次和五次项时，没有一个 \\(p\\) 值接近统计显著性。一切都讲得通。另一方面，所谓的“部分”方法包括在所有其他回归效应都得到拟合后，检验每个潜在回归效应的影响。例如，我们将在拟合二次项、三次项、四次项和五次项后，评估添加线性项 \\(\\beta_1X\\) 的效应。这是不合逻辑的，有两个原因：首先，它假定拟合线性预测器 \\(\\eta=\\beta_0+\\beta_2X^2+\\beta_3X^3+\\beta_4X^4+\\beta_5X^5\\) 是有意义的（实际上这样做没有意义）；其次，它假定此时检验 \\(\\beta_1X\\) 是合理的（实际上不是）。该检验的似然比和 Wald 统计量均为 0.30，\\(p\\) 值分别为 0.5822 和 0.5824（对于这些数据，似然比和 Wald 统计量略有不同，但四舍五入到小数点后第二位后可忽略不计）。我们应立即认识到，这里的片面做法纯属无稽之谈。这里要传达的信息是，在考虑效应具有明显层次结构的模型时，序贯拟合是有意义的，并会产生合理的结果，在这种情况下首先是线性的，然后是二次的，然后是三次的，等等。示例 6.2  (三因素仅主效应设计) 本例数据在 SAS Data Program Library 中显示为 Data Set 6.2. 数据来自三因素正交主效应设计。三个因子，这里称为 , B 和 C，分别观测了两个水平，标记为 0 和 1，在八个可能的 \\(×B×C\\) 组合中仅观测了四个组合。正交主效应设计广泛用于发现研究（尤其在其早期阶段）和质量改进（即所谓的“质量源于设计”）等应用。请注意，已观测的处理组合恰好足以估计 , B 和 C 的主效应，但没有交互作用——当已知交互作用可忽略不计，并且建立额外 \\(×B×C\\) 成本高昂的情况下。对于这些数据，重复观测出现在一些但不是所有的处理组合。假定这些数据具有高斯分布。假设我们使用序贯方法来评估处理因素的效应。让我们开始按字母顺序拟合它们。从线性预测器 \\(\\eta_{ijk}=\\eta +\\alpha_i\\) 开始，其中 \\(\\eta_{ijk}\\) 表示第 \\(ijk\\) 个 \\(×B×C\\) 组合的连接函数，\\(\\alpha_i\\) 表示因子 \\(\\) 的效应。第 \\(l\\) 个重复的响应分布为 \\(y_{ijkl}\\sim NI\\left(\\mu_{ijk},\\sigma^2\\right)\\)。根据 6.4.3 节最后的讨论，我们知道该模型的似然比和 Wald 检验是相同的，我们应该使用 \\(F\\) 检验。对于我们的初始模型，有关 效应检验 \\(H_0\\colon\\alpha_0=\\alpha_1\\) 的 \\(F\\) 值为 3.65，\\(p=0.0978\\)。水平之间的平均差异估计为 \\(1.62\\pm 0.85\\)。现在添加因子 B。新的线性预测器为 \\(\\eta_{ijk}=\\eta+\\alpha_i+\\beta_j\\)，其中 \\(\\beta_j\\) 表示 B 的第 \\(j\\) 个水平的效应。检验 \\(H_0\\colon{\\beta}_0={\\beta}_1\\mid\\alpha_i\\)（即，在 之上添加 B 的效应）的结果：\\(F = 0.86,p = 0.4175\\)，B 水平之间的平均差异估计为 \\(0.75\\pm 0.86\\)。最后，我们添加因子 C，结果为 \\(F = 36.64,p = 0.0018\\)，C 水平之间的平均差异估计为 \\(2.10\\pm 0.35\\)。现在我们改变顺序：从 B 开始，然后添加 ，最后添加 C。我们从线性预测器 \\(\\eta_{ijk}=\\eta +\\beta_i\\) 开始。结果为 \\(F = 1.71,p = 0.2328\\)，B 的平均差异为 \\(1.16\\pm 0.89\\)。请注意，这不是我们在 之后拟合 B 时得到的结果。继续，我们现在拟合 。结果为 \\(F = 2.26,p = 0.1831\\)，的平均差异 \\(1.37\\pm 0.91\\)。同样，这也不是我们之前得到的。另外，当我们拟合模型 \\(\\eta_{ijk}=\\eta+\\beta_j+\\alpha_i\\) 并依次进行似然比检验时，若我们未能考虑到 \\(\\sigma^2\\) 未知的事实，我们得到 \\(\\chi^2=1.96,p=0.1612\\)，或者如果我们确实考虑了 \\(\\sigma^2\\) 是估计的，则 \\(F = 2.01,p = 0.2057\\)。显然，我们有一个问题：平均差异和显著性似乎很混乱 (map). 更糟糕的是：如果我们先拟合 B，然后加上 C，得到 B 的结果是 \\(F=3.19,p=0.1242\\)，B 的平均差异为 \\(1.16\\pm 0.65\\)。如果我们使用线性预测器 \\(\\eta_{ijk}=\\eta+\\beta_j+\\gamma_k+\\alpha_i\\) 拟合所有三个效应并按顺序进行检验，则 B 的结果为 \\(F = 13.97,p = 0.0135\\)，B 的平均差异为 \\(1.16\\pm 0.31\\)！根据我们拟合效应的顺序和检验方式，B 的 \\(p\\) 值可以在 0.0135 到 0.891017 之间！显然，这种方法有问题。对于这些数据，一个更好的策略是要清晰地思考我们想要估计的是什么，我们想要检验的是什么，并将这些目标用可估函数来描述。事实证明，序贯策略蕴含着依赖于数据模式的隐含可估函数：这些结果是设计中使用的处理组合以及不等重复模式的产物。在第 8 章中，我们将探讨如何确定各种检验策略中隐含的可估函数。目前，让我们集中精力讨论我们应该怎么做。假设不等重复更多地是偶然发生的（而非预先的设计），我们希望得到因子水平均值的估计，这些估计告诉我们如果采用相等重复会得到什么结果。Searle (1987) 将这些称为“调整的边际均值” (“adjusted marginal means”). SAS® 将其称为 “least squares means”，并在其线性模型程序中使用语法 LSMEANS。不管我们怎么称呼它们，估计均值的策略包括分离出感兴趣的因素水平，并在其他因素的水平上求平均。例如，对于因子 ，定义水平 \\(A_0\\) 的均值的可估函数为 \\(\\eta+\\alpha_0+(\\beta_0+\\beta_1)/2+(\\gamma_0+\\gamma_1)/2\\)。类似地，定义 \\(B_i\\) 均值的可估函数为 \\(\\eta+\\beta_i+(\\alpha_0+\\alpha_1)/2+(\\gamma_0+\\gamma_1)/2\\)。这样，当我们估计平均差时，我们会取边际均值之差，得到 \\(\\alpha_0-\\alpha_1,\\beta_0-\\beta_1\\) 和 \\(\\gamma_0-\\gamma_1\\)，这些似乎都是激发收集这些数据的研究的逻辑焦点。如此做，我们得到如下结果：总的来说，检验效应的策略应像本章开头说的那样：专注于定义可估函数，该可估函数能表达你的目标。从序贯平方和、部分平方和或 1 型/ 2 型/ 3 型假设的角度来思考并不能帮你达到目的，但从 \\(\\symbf{K'\\beta}\\) 的角度思考可以。","code":""},{"path":"chap6.html","id":"sec6-5","chapter":"第 6 章 推断（一）","heading":"6.5 使用基于模型的统计量进行推断","text":"“使用基于模型的统计量进行推断” (“Inference using model-based statistics”) 是指使用 (6.17), (6.18) 和 (6.19) 作为基本工具：基于 \\(t\\) 分布的区间估计、使用近似 \\(F\\) 的假设检验以及两者共同的定义特征——估计可估和可预测函数的方差 \\(\\symbf{L}^{\\prime}\\hat{\\symbf{C}}\\symbf{L}\\)。回想 6.3 节末尾的讨论，实现该方法涉及的两个主要问题是：1) 近似 \\(F\\) 的分母自由度（因此也包括 \\(t\\) 的自由度）通常必须近似，以及 2) 除了均衡仅方差分量混合模型外，\\(\\symbf{L}^{\\prime}\\hat{\\symbf{C}}\\symbf{L}\\) 是 \\(\\symbf{L}^{\\prime}{\\symbf{C}}\\symbf{L}\\) 的有偏估计。在本节中，我们将探讨这些问题：6.5.1 节奠定基础，第 6.5.2 节具体讨论自由度近似，6.5.3 节提出了标准的偏差调整策略。","code":""},{"path":"chap6.html","id":"sec6-5-1","chapter":"第 6 章 推断（一）","heading":"6.5.1 朴素统计量和自由度","text":"式 (6.17) 的方差估计 \\(\\symbf{L}^{\\prime}\\hat{\\symbf{C}}\\symbf{L}\\)，称为 \\(\\symbf{K'\\tilde{\\beta}}+\\symbf{M'}\\left(\\tilde{\\symbf{b}}-\\symbf{b}\\right)\\) 方差的“朴素” (“naive”) 估计。对于 \\(\\symbf k\\) 和 \\(]symbf m\\) 为向量的可估或可预测函数，\\[\\sqrt{(\\symbf{k^{\\prime}}\\quad\\symbf{m^{\\prime}})\\hat{\\symbf{C}}{\\begin{bmatrix}\\symbf{k}\\\\\\symbf{m}\\end{bmatrix}}}\\]称为朴素标准误。从这个意义上说，我们可以将 (6.19) 称为朴素 \\(F\\) 统计量，尽管该术语很少在混合模型领域中使用。我们称这些估计为“朴素”，因为 Kackar Harville (1984) 表明，除了均衡仅方差分量和某些边际模型（例如复合对称性，这些模型具有等价的仅方差分量条件模型形式）外的混合模型，该估计有向下的偏差，即 \\(E\\left(\\symbf{L}^{\\prime}\\hat{\\symbf{C}}\\symbf{L}\\right)<\\symbf{L}^{\\prime}{\\symbf{C}}\\symbf{L}\\)18。除非我们纠正这种偏差，否则我们的置信区间太窄——因此覆盖率较低——检验统计量膨胀——因此 类错误率过高。分母自由度也可能是一个问题。在第 2 章中，我们开发了“Fisher会怎么做？”框架 ANOVA 策略，用于构建线性预测器并区分固定和随机模型效应。编写框架 ANOVA 需要列出每个效应的自由度。这样做为确定分母自由度提供了指导。对于某些模型的一些可估函数，这种策略给了我们精确的自由度。在其他情况下，这些自由度提供了合理的指导方针，但并不精确。随着模型协方差结构复杂性的增加，框架 ANOVA 自由度和用于 (6.19) 的“真实” \\(\\nu_2\\) 之间的差异往往会增加。“真实”加了引号，因为 (6.19) 是近似的 \\(F\\)，所以根据定义，\\(\\nu_2\\) 也必须是近似的。我们把框架 ANOVA 自由度称为“朴素”自由度。我们所说的“真实” \\(\\nu_2\\) 实际上是指高质量近似—— 6.5.2 节中给出的 Satterthwaite 近似。首先，让我们通过一个简单的仅方差分量的 LMM 示例来清楚地理解激发 Satterthwaite 近似的问题。考虑一个均衡双因素数据集，其中对每个 \\(×B\\) 组合进行重复观测。假定 的水平是有意选择的，而 B 的水平是通过对 B 的可能水平的总体进行随机抽样得到的。我们可将 LMM 描述为：线性预测器：\\(\\eta_{ij}=\\eta+\\alpha_i+b_j+(ab)_{ij}\\)，其中 \\(\\alpha_i\\) 表示 的第 \\(\\) 个效应（固定），\\(b_j\\) 表示 B 的第 \\(j\\) 个效应（随机），\\((ab)_{ij}\\) 表示交互效应（也是随机的）。假定 \\(=1,2,\\ldots,\\) 以及 \\(j=1,2,\\ldots,b\\)。线性预测器：\\(\\eta_{ij}=\\eta+\\alpha_i+b_j+(ab)_{ij}\\)，其中 \\(\\alpha_i\\) 表示 的第 \\(\\) 个效应（固定），\\(b_j\\) 表示 B 的第 \\(j\\) 个效应（随机），\\((ab)_{ij}\\) 表示交互效应（也是随机的）。假定 \\(=1,2,\\ldots,\\) 以及 \\(j=1,2,\\ldots,b\\)。分布：\\(b_j\\mathrm{~iid~}N\\left(0,\\sigma_B^2\\right),(ab)_{ij}\\mathrm{~iid~}N\\left(0,\\sigma_{AB}^2\\right)\\) 以及 \\(y_{ijk}\\mid b_j,(ab)_{ij}\\sim NI\\left(\\mu_{ij},\\sigma^2\\right)\\)。其中，\\(y_{ijk}\\) 表示第 \\(ij\\) 个 AB 组合的第 \\(k\\) 个观测。分布：\\(b_j\\mathrm{~iid~}N\\left(0,\\sigma_B^2\\right),(ab)_{ij}\\mathrm{~iid~}N\\left(0,\\sigma_{AB}^2\\right)\\) 以及 \\(y_{ijk}\\mid b_j,(ab)_{ij}\\sim NI\\left(\\mu_{ij},\\sigma^2\\right)\\)。其中，\\(y_{ijk}\\) 表示第 \\(ij\\) 个 AB 组合的第 \\(k\\) 个观测。连接：恒等。连接：恒等。该模型的 ANOVA 为考虑用于估计 差异的可估函数：\\(\\alpha_i-\\alpha_{'}\\)，我们通过 \\(\\bar{y}_{\\cdot\\cdot}-\\bar{y}_{'\\cdot\\cdot}=1/nb\\sum_{j,k}y_{ijk}-1/nb\\sum_{j,k}y_{'jk}\\) 来估计该差异。以模型形式表达为 \\(1/nb\\sum_{j,k}\\left(\\eta+\\alpha_i+b_j+(ab)_{ij}\\right)-1/nb\\sum_{j,k}\\left(\\eta+\\alpha_{^{\\prime}}+b_j+(ab)_{^{\\prime}j}\\right)\\)，我们可以很容易地证明 \\(Var\\left(\\hat{\\alpha}_i-\\hat{\\alpha}_{'}\\right)=\\left[2\\left(\\sigma^2+n\\sigma_{AB}^2\\right)\\right]/nb\\)，由 \\(2{\\left[MS\\left(AB\\right)\\right]}/nb\\)19 来估计。其中 \\(MS(AB)\\) 是用于检验 \\(H_0\\colon\\alpha_i=\\alpha_{'}\\) 的 \\(F\\) 比中的分母卡方，也是构建置信区间所需 \\(t\\) 分布的分母卡方。因此，AB 的自由度为 \\(\\nu_2=(-1)(b-1)\\)。现在考虑 的一个水平的均值。可估函数为 \\(\\eta+\\alpha_i\\)，我们通过 \\(\\bar{y}_{\\cdot\\cdot}=1/nb\\sum_{j,k}y_{ijk}\\) 来估计。以模型形式编写，我们可再次证明\n\\(Var\\left(\\hat{{\\eta}}+\\hat{{\\alpha}}_i\\right)=\\left[n\\Big(\\sigma_B^2+\\sigma_{AB}^2\\Big)+\\sigma^2\\right]/nb\\)。方差分量的 ANOVA 估计如下：\\(\\hat{\\sigma}_B^2=\\begin{pmatrix}1/nb\\end{pmatrix}\\begin{bmatrix}MS(B)-MS(AB)\\end{bmatrix},\\quad\\hat{\\sigma}_{AB}^2=\\begin{pmatrix}1/n\\end{pmatrix}\\begin{bmatrix}MS(AB)-MSR\\end{bmatrix}\\) 以及 \\(\\hat{\\sigma}^2=MSR\\)。经过一些代数运算可得到 \\(Var\\left(\\hat{\\eta}+\\hat{\\alpha}_i\\right)\\) 的估计为 \\(\\begin{pmatrix}1/b\\end{pmatrix}\\begin{bmatrix}MS\\begin{pmatrix}B\\end{pmatrix}+\\begin{pmatrix}b-1\\end{pmatrix}MS(AB)\\end{bmatrix}\\)。现在我们有了两个卡方随机变量的线性组合：对于 \\(\\nu_2\\) 我们应该使用哪个自由度？\\((b - 1)\\) 还是 \\((-1)(b-1)\\)？还是两者的加权平均？我们将在下一节中讨论这个问题。","code":""},{"path":"chap6.html","id":"sec6-5-2","chapter":"第 6 章 推断（一）","heading":"6.5.2 Satterthwaite 自由度近似","text":"我们在上一节中悬而未决的问题由 Satterthwaite (1941, 1946) 解决。Satterthwaite 表明，给定比值\\[\\frac{X_{num}^2/\\nu_1}{X_2^*/\\nu_2^*}\\]其中 \\(X_{num}^2\\sim\\chi_{\\nu_1}^2\\)，\\(X_2^*\\) 是均独立于 \\(X_{num}^2\\) 的卡方随机变量的线性组合，那么 \\(X_2^*\\sim \\text{ 近似 } \\chi^2_{\\nu_2^*}\\)，其中\\[\\begin{align}\n\\nu_2^*\\cong\\frac{\\left(\\sum_mc_mX_m^2\\right)^2}{\\sum_m\\frac{\\left(c_mX_m^2\\right)}{df_m}}\n\\tag{6.21}\n\\end{align}\\]其中 \\(X^2_m\\) 表示 \\(\\chi^2_{df_m}\\) 随机变量20。我们称式 (6.21) 为 Satterthwaite 近似。\\(c_m\\) 表示线性组合中的常数，\\(df_m\\) 表示相应 \\(X_m^2\\) 的自由度。在我们的示例中，Satterthwaite 近似自由度为\\[\\nu_2^*\\cong\\frac{\\left[MS\\left(\\mathrm{B}\\right)+\\left(b-1\\right)MS\\left(\\mathrm{AB}\\right)\\right]^2}{\\frac{\\left[MS\\left(\\mathrm{B}\\right)\\right]^2}{b-1}+\\frac{\\left[MS\\left(\\mathrm{AB}\\right)\\right]^2}{(-1)(b-1)}}\\]我们可以构造 \\(\\eta+\\alpha_i\\) 的置信区间：\\[\\bar{y}_{\\cdot\\cdot}\\pm t_{{\\nu_{2}^{*},\\alpha/2}}\\sqrt{\\frac{MS\\left(\\mathrm{B}\\right)+\\left(b-1\\right)MS\\left(\\mathrm{AB}\\right)}{b}}\\]对于仅方差分量的高斯 LMM，我们可以使用 (6.21) 中所示的 Satterthwaite 近似形式，但不能用于更复杂的 LMMs 或 GLMMs. Geisbrecht Burns (1985) 扩展了 (6.21)，以包含一般 LMM 的可估函数。当 \\(\\symbf k\\) 是向量时，我们可以将该近似写为 \\(\\nu_2\\cong2\\Big\\{E\\Big[\\symbf k'\\big(\\symbf X'\\symbf V^{-1}\\symbf X\\big)^-\\symbf k\\Big]\\Big\\}^2\\Bigg/Var\\Big[\\symbf k'\\big(\\symbf X'\\symbf V^{-1}\\symbf X\\big)^-\\symbf k\\Big]\\)。在实践中，我们需要用 \\(\\hat{\\symbf V}\\) 替换 \\(\\symbf V\\)，因为不可避免地我们有一个未知的 \\(\\symbf V\\)。此外，分母方差项不存在“好的”表达式。Geisbrecht Burns 使用近似 \\(Var\\left[\\symbf{k'}\\left(\\symbf{X'}\\symbf{V}^{-1}\\symbf{X}\\right)^-\\symbf{k}\\right]\\cong\\symbf{g'}\\symbf{V}_A\\left({\\hat{\\symbf\\sigma}}\\right)\\symbf{g}\\)，其中\\[\\symbf{g}=\\left.\\frac{\\partial\\left[\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{V}\\left(\\symbf{\\sigma}\\right)^{-1}\\symbf{X}\\right)\\symbf{k}\\right]}{\\partial\\symbf{\\sigma}^{\\prime}}\\right|_{\\symbf\\sigma=\\hat{\\symbf\\sigma}}\\]以及 \\(\\symbf{V}_A\\left({\\hat{\\symbf\\sigma}}\\right)\\) 为协方差参数估计 \\(\\hat{\\symbf \\sigma}\\) 的渐近协方差阵。使用众所周知的方差分量结果（参见 Searle et al., 1992），我们将渐近方差的第 \\(ij\\) 个元素写为\\[V_{,ij}=2\\times\\left[\\operatorname{trace}\\left\\{\\symbf{P}{\\left(\\frac{\\partial V(\\symbf{\\sigma})}{\\partial\\sigma_i}\\right)}\\symbf{P}{\\left(\\frac{\\partial V(\\symbf{\\sigma})}{\\partial\\sigma_j}\\right)}\\right\\}\\right]^{-1}\\]其中，回想式 (5.27)，\\(\\symbf{P}=\\left[\\symbf{V}(\\symbf{\\sigma})\\right]^{-1}-\\left[\\symbf{V}(\\symbf{\\sigma})\\right]^{-1}\\symbf{X}\\left(\\symbf{X}^{\\prime}\\left[\\symbf{V}(\\symbf{\\sigma})\\right]^{-1}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\left[\\symbf{V}(\\symbf{\\sigma})\\right]^{-1}\\)。实际计算得到的 Satterthwaite 近似为\\[\\begin{align}\n\\nu_2\\cong\\frac{2\\left[\\symbf{k}^{\\prime}\\left(\\symbf{X}^{\\prime}\\symbf{V}\\left(\\symbf{\\hat{\\sigma}}\\right)^{-1}\\symbf{X}\\right)^{-}\\symbf{k}\\right]^2}{\\symbf{g}^{\\prime}\\symbf{V}_A\\left(\\symbf{\\hat{\\sigma}}\\right)\\symbf{g}}\n\\tag{6.22}\n\\end{align}\\]对于在 GLMMs 上定义的可估函数，我们可以使用 (6.22) 的特别版本。为此，我们将 \\(\\left[\\symbf{V}(\\symbf{\\hat{\\sigma}})\\right]^{-1}\\) 替换为 \\(\\symbf V^*\\)，或更准确地说，替换为第 5 章定义的 \\(\\hat{\\symbf V}^*\\)。请注意，我们只能将 GLMMs 的 Satterthwaite 近似与伪似然估计结合使用，因为所需元素仅对 PL 语境中的 GLMM 有意义。对于可预测函数，Satterthwaite 近似的扩展是直接的：我们用协方差分量估计（和/或 GLMMs 的工作相关系数，当它们可用时）将式 (6.22) 分子中的 \\(\\symbf{k}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X})^-\\symbf{{k}}\\) 替换为 \\(\\symbf L \\symbf C'\\symbf L\\)。这些式子可扩展到多维 \\(\\symbf K\\) 和 \\(\\symbf L\\)——此处省略细节。","code":""},{"path":"chap6.html","id":"sec6-5-3","chapter":"第 6 章 推断（一）","heading":"6.5.3 基于模型的标准误差和检验统计量的偏差校正","text":"对于具有均衡数据的仅方差分量 LMMs 及其复合对称边际模型等价物，\\(\\symbf{K}′\\tilde{\\symbf\\beta}\\) 的协方差估计如 (6.8) 和 (6.9) 所示，即 \\(\\symbf{K}^{\\prime}(\\symbf{X}^{\\prime}\\symbf{V}(\\hat{\\symbf\\sigma})^{-1}\\symbf{X})^-\\symbf{\\bar{K}}\\)，为 \\(Var\\left(\\symbf{K}'\\tilde{\\symbf{\\beta}}\\right)=\\symbf{K'}\\left(\\symbf{X'V}\\left(\\symbf{\\sigma}\\right)^{-1}\\symbf{X}\\right)^-\\symbf{{K}}\\) 的无偏估计。否则，对于所有其他 LMM 和 GLMM 情况，\\(E\\left(\\symbf{L}^{\\prime}\\hat{\\symbf{C}}\\symbf{L}\\right)<\\symbf{L}^{\\prime}{\\symbf{C}}\\symbf{L}\\)。Kackar Harville (1984) 首次记录了这一点。后来的出版物，特别是 Prasad Rao (1990) 和 Harville Jeske (1992) 探讨了这个问题，特别是关于可预测函数的方差估计。Kenward Roger (1997) 开发了一个偏差校正项，该项作为选项添加到 MIXED 程序中，并包含在 GLIMMIX 程序中。Guerin Stroup (2000) 通过涉及纵向数据的几个相关误差模型的模拟研究，研究了 PROC MIXED 在有和没有 Kenward-Roger 选项的 LMM 近似 \\(F\\) 的 类错误率。他们发现，使用“朴素”自由度（如本节早些时候所述）的未校正 类错误率膨胀了，通常在名义 \\(\\alpha = 0.05\\) 时，错误率处于 15% 的范围内。Satterthwaite 自由度校正本身对 类错误率膨胀几乎没有影响。Kenward-Roger 校正始终能将 类错误率控制在 4% 至 6% 的范围内。Kenward Roger 在他们的研究中发现了类似的小样本表现，并在他们 1997 年的论文中达到了高潮。后续的工作——主要基于大量未公开发表的模拟研究的口头传统——已经证实了 Kenward-Roger 校正对 LMMs 的有效性，这已成为一个既定的事实。虽然从技术上讲，Kenward-Roger 校正是一个选项，但我们可将其视为 LMMs 推断中推荐的默认标准操作程序。我们简要地描述该校正。有兴趣的读者应参阅原始论文以了解更多详细信息。Kenward Roger 关注 \\(\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^-\\)。遵循 Kackar Harville，他们得到了我们可用可估函数项来表征的结果：\\[E\\bigg[\\symbf{K}^{\\prime}\\bigg(\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\symbf{X}\\bigg)^{-1}\\symbf{K}\\bigg]=\\symbf{K}^{\\prime}\\big(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\big)^{-}\\symbf{K}+\\frac{1}{2}\\sum\\operatorname{cov}\\big(\\sigma_{},\\sigma_{j}\\big)\\symbf{K}^{\\prime}\\frac{\\partial^{2}\\bigg[\\big(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\big)^{-}\\bigg]}{\\partial\\sigma_{}\\partial\\sigma_{j}}\\symbf{K}\\]其中 \\(\\sigma_i\\) 和 \\(\\sigma_j\\) 表示协方差向量 \\(\\symbf\\sigma\\) 的第 \\(\\) 和第 \\(j\\) 个元素，\\(\\operatorname{cov}(\\sigma_i,\\sigma_j)\\) 为渐近协方差阵的第 \\(ij\\) 个元素——我们之前用 \\(\\symbf V_A\\left(\\hat{\\symbf\\sigma}\\right)\\) 表示。Kenward Roger 使用如下结果来获得偏差调整：\\[\\begin{align}\n&\\small\\left(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\right)_{KR-adj}=\\\\&\\small\\left(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\right)_N+2\\left(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\right)_N\\left\\{\\sum_{,j}\\symbf{V}_{,ij}\\left(\\symbf{Q}_{ij}-\\symbf{X}^{\\prime}\\frac{\\partial\\symbf{V}^{-1}}{\\partial\\sigma_i}\\symbf{X}\\left[\\left(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\right)_N\\right]\\symbf{X}^{\\prime}\\frac{\\partial\\symbf{V}^{-1}}{\\partial\\sigma_j}\\symbf{X}-\\frac14\\symbf{T}_{ij}\\right)\\right\\}\\left(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\right)_N\n\\tag{6.23}\n\\end{align}\\]其中 \\(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}=\\left(\\symbf{X}'\\hat{\\symbf{V}}^{-1}\\symbf{X}\\right)^-\\)，\\(\\left(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\right)_{KR-adj}\\) 表示 Kenward-Roger 调整的 \\(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\)，\\(\\left(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\right)_{N}\\) 表示朴素 \\(\\hat{\\symbf{V}}_{\\hat{\\symbf{\\beta}}}\\)，\\(\\symbf{Q}_{ij}=\\symbf{X}'\\frac{\\partial\\symbf{V}^{-1}}{\\partial\\sigma_i}\\hat{\\symbf{V}}^{-1}\\frac{\\partial\\symbf{V}^{-1}}{\\partial\\sigma_j}\\symbf{X}\\)，以及 \\(\\symbf{T}_{ij}=\\symbf{X}^{\\prime}\\hat{\\symbf{V}}^{-1}\\frac{\\partial^2\\symbf V(\\symbf{\\sigma})}{\\partial\\sigma_i\\partial\\sigma_j}\\symbf{V}^{-1}\\symbf{X}\\)。Kackar Harville 的表达式类似于 (6.23)，但没有 \\(\\symbf T_{ij}\\) 项。对于 \\(\\frac{\\partial^2\\symbf V(\\sigma)}{\\partial\\sigma_i\\partial\\sigma_j}=0\\) 的模型，Kackar-Harville 和 Kenward-Roger 校正是相同的。Kenward-Roger 调整是使用 LMM 为可估函数开发的。然而，如果我们用 \\(\\symbf V^*\\) 替换 \\(\\symbf V(\\symbf\\sigma,\\symbf\\rho)\\)，其中 \\(\\symbf V^*\\) 是第 5 章中为 PL 定义的伪变量的方差，并且 \\(\\symbf\\sigma\\) 和 \\(\\symbf\\rho\\) 分别表示协方差和工作相关分量向量，那么 Kenward-Roger 调整可适用于 GLMM 推断。这种适应显然是特别的 (ad hoc)，但非正式的模拟研究一致表明，只要 PL 线性化准确，它就能表现良好。我们所称的“表现良好”是指平均调整的标准误与所研究的 \\(\\symbf K′\\symbf\\beta\\) 抽样分布的观测标准差一致，并且功效和 类错误特征近似与理论一致。作者熟悉二项和泊松 GLMM 的研究——这些研究一致表明，除非二项的集群尺寸非常小，否则这种特别的 Kenward-Roger 程序是准确的。但对于双参数指数家族的成员，似乎存在问题。请注意，与 Satterthwaite 近似一样，Kenward-Roger 程序不能与积分近似法、拉普拉斯或高斯-埃尔米特求积法一起使用。在下一节中，我们将讨论替代方法。","code":""},{"path":"chap6.html","id":"sec6-6","chapter":"第 6 章 推断（一）","heading":"6.6 使用经验标准误进行推断","text":"当 Kackar Harville 在 LMM 背景下研究标准误偏差问题时，Liang Zeger (1986) 在他们正在开发的 GEE 理论和方法背景下遇到了类似的问题。他们提出了一种称为“三明治估计” (“sandwich estimator”) 的解决方案，在线性模型文献中也称为“经验”或“稳健”标准误估计。对于具有相关误差的 LMMs 和 GLMMs（将在后续章节中考虑），三明治估计具有明显的优势，即与 6.3 节和 6.4 节中讨论的基于模型的标准误相比，更不容易受到 \\(\\symbf V\\) 或 \\(\\symbf V^*\\) 错误指定的影响。然而，三明治估计对于较小的数据集有不同的偏差。“较小的数据集”包括农业、动物健康和质量改进等领域中常见的实验——本质上为受试者少于数千名的实验。未校正的三明治估计存在向下偏差：对于非常大的数据集，偏差可以忽略不计，但随着重复观测数量的减少，偏差急剧增加。对于许多学科的典型实验，未校正的三明治估计是不可用的。然而，对于不适合 PL 的 GLMM，Kenward-Roger 调整的一些替代方法是必不可少的。Morel et al. (2003) 开发了一种针对三明治估计的小样本偏差校正。在 6.6.1 节中，我们介绍了三明治估计的基本思想及其基本结构。在 6.6.2 节中，我们描述了 Morel et al. 的偏差校正。","code":""},{"path":"chap6.html","id":"sec6-6-1","chapter":"第 6 章 推断（一）","heading":"6.6.1 三明治（又名稳健或经验）估计","text":"我们从 LMM 的边际形式开始。我们知道 \\(\\hat{\\symbf{\\beta}}=\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-{\\symbf{X'V}}^{-1}\\symbf{y}\\)。因此\\[\\begin{align}\nVar\\left(\\hat{\\symbf{\\beta}}\\right)=\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^-\\symbf{X}^{\\prime}\\symbf{V}^{-1}Var\\left(\\symbf{y}\\right)\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^-\n\\tag{6.24}\n\\end{align}\\]我们通过用 \\(\\symbf V\\) 替换 \\(Var(\\symbf y)\\) 来导出基于模型的标准误和检验统计量。对于未知的 \\(\\symbf V\\)，我们获得协方差分量的估计，确定 \\(\\hat{\\symbf V}\\) 并用它替换 \\(\\symbf V\\)，随之带来我们在本章中探讨的所有结果。对于 GLM，\\(\\hat{\\symbf W}\\) 替换 \\(\\hat{\\symbf V}\\)，对于 GLMM，\\(\\hat{\\symbf V}^*\\) 替换 \\(\\hat{\\symbf V}\\)，但思想是相同的。估计 \\(Var(\\symbf y)\\) 的另一种方法是使用平方和和叉乘矩阵\\[\\sum_i\\Big(\\symbf{y}_i-\\symbf{X}_i\\hat{\\symbf{\\beta}}_i\\Big)\\Big(\\symbf{y}_i-\\symbf{X}_i\\hat{\\symbf{\\beta}}_i\\Big)'\\]其中，\\(\\) 代表个体，例如区组、地点、集群等。将其插入 (6.24) 即可得到三明治估计\\[\\begin{align}\nVar\\left(\\tilde{\\symbf{B}}\\right)=\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\\symbf{X'V}^{-1}\\left[\\sum_{=1}^m\\left(\\symbf{y}_i-\\symbf{X}_i\\tilde{\\symbf{\\beta}}_i\\right)\\left(\\symbf{y}_i-\\symbf{X}_i\\tilde{\\symbf{\\beta}}_i\\right)^{\\prime}\\right]\\symbf{X'V}^{-1}\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\n\\tag{6.25}\n\\end{align}\\]之所以如此命名，是因为 \\(\\sum_{=1}^m\\left(\\symbf{y}_i-\\symbf{X}_i\\tilde{\\symbf{\\beta}}_i\\right)\\left(\\symbf{y}_i-\\symbf{X}_i\\tilde{\\symbf{\\beta}}_i\\right)^{\\prime}\\) 被“夹”在 \\(\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\\symbf{X'V}^{-1}\\) 项之间。请注意，三明治估计要求我们能够根据个体定义协方差结构。例如，回想第 2 章，在多地点实验（随机地点）中，我们可以将地点方差定义为单个实体，例如 \\(Var\\left(\\symbf{L}\\right)=\\symbf{}_m\\sigma_L^2\\)，其中 \\(m\\) 是地点数，或者，我们也可以将其定义为随机截距结构，其中每个地点作为个体，即 \\(\\symbf I_m \\otimes\\symbf{1}\\sigma_L^2\\)。它们是等价的，但我们需要使用后一种形式来获得三明治估计。用 SAS PROC GLIMMIX 术语来说，对于区组设计，语句会产生一条错误消息，而将 random block 替换为 random intercept / subject=block; 将得到使用三明治估计的标准误、区间估计和检验统计量。请注意，PROC 语句中的 EMPIRICAL 选项调用三明治估计。","code":"proc glimmix empirical;\n class treatment block;\n model y=treatment;\n random block;"},{"path":"chap6.html","id":"sec6-6-2","chapter":"第 6 章 推断（一）","heading":"6.6.2 三明治估计的偏差校正","text":"Morel et al. (2003) 提出了三明治估计的改进。我们可将 (6.25) 视为“朴素”三明治估计，原因有二。其一，在混合模型中，残差的平方和和叉乘应考虑设计或样本量，具体为\\[\\frac{N-1}{N-k}\\frac m{m-1}\\sum_{=1}^m\\left(\\symbf{y}_i-\\symbf{X}_i\\tilde{\\symbf{\\beta}}_i\\right)\\left(\\symbf{y}_i-\\symbf{X}_i\\tilde{\\symbf{\\beta}}_i\\right)^\\prime \\]其中 \\(N\\) 表示观测总数，\\(m\\) 表示个体数，\\(k=\\operatorname{rank}(\\symbf X)\\)。例如，在 Stroup et al. (2018) 中呈现的 Cochran Cox (1957) 的例子，我们在第 5 章中介绍了 ML 和 REML 方差估计，有 15 个区组，每个区组有 4 个观测，由此 \\(N=60\\) 个观测和 15 个处理，因此 \\(k=\\operatorname{rank}(\\symbf X)=15\\) 和 \\(m=15\\) 个个体，因为区组是个体。其二，“朴素”三明治估计隐含地要求我们假定 \\(\\sum_i\\left(\\symbf{y}_i-\\symbf{X}_i\\tilde{\\symbf{\\beta}}_i\\right)=\\symbf{0}\\)，这不一定对所有 GLMMs 都是正确的。Morel et al. 提出了一个校正项 \\(\\symbf{\\delta}_m\\symbf{\\phi}\\symbf{V}(\\symbf{\\hat{\\sigma}})\\) 来考虑这一点，其中 \\(\\delta_m=\\min[0.5,k/(m-k)]\\)，以及 \\(\\phi=\\max\\left[1,\\operatorname{trace}\\left\\{\\left(\\symbf{X}^{\\prime}\\symbf{V}^{-1}\\symbf{X}\\right)^{-}\\sum_i\\left[\\symbf{V}_i^{-1}\\left(\\symbf{y}_i-\\symbf{X}_i\\symbf{\\beta}_i\\right)\\left(\\symbf{y}_i-\\symbf{X}_i\\symbf{\\beta}_i\\right)^{\\prime}\\symbf{V}_i^{-1}\\right]\\right\\}\\right]\\)。对于广义模型（GLM 和 GLMM），用 \\(\\symbf V^*\\) 替换 \\(\\symbf V\\)，用 \\(\\symbf y^*\\) 替换 \\(\\symbf y\\)，以及用逆连接 \\(\\tilde{\\symbf \\mu}_i=h(\\symbf X_i\\symbf\\beta_i)\\) 替换 \\(\\symbf X_i\\symbf\\beta_i\\)。在 Cochran Cox 的数据中，\\(\\frac k{m-k}=\\frac{15}{15-15}\\)，这制造了一个计算 \\(\\delta_m\\) 的问题。为了避免此类问题，PROC GLMMIX 将决策规则重新表示为\\[\\delta_m=\\begin{cases}\\frac{k}{m-k}\\;\\text{   }\\;m>k\\left(d+1\\right)\\\\1/d\\text{ otherwise}\\end{cases}\\]此外，GLIMMIX 在默认情况下设置 \\(d=2\\)，但允许你选择 \\(d\\ge 1\\)，并通过将 1 替换为 \\(0<r<1\\) 作为 \\(\\phi\\) 的最大值来改变 \\(\\phi\\) 的值。为给定模型确定“正确”的 \\(d\\) 和 \\(r\\) 值需要通过模拟进行反复试验。从而，用 GLMM 边际模型术语表示的校正三明治估计为\\[\\begin{align}\nVar\\left(\\tilde{\\symbf{\\beta}}\\right) =&\\;\\left(\\symbf{X'V}^{*-1}\\symbf{X}\\right)^-\\symbf{X'V}^{*-1}\\left[\\sum_{=1}^mC\\left(\\symbf{y}_i-\\tilde{\\symbf{\\mu}}_i\\right)\\left(\\symbf{y}_i-\\tilde{\\symbf{\\mu}}_i\\right)^{\\prime}+\\delta_n\\phi\\symbf{V}^*\\right]  \\\\\n&\\;\\symbf{X}^{\\prime}\\symbf{V}^{*-1}\\left(\\symbf{X}^{\\prime}\\symbf{V}^{*-1}\\symbf{X}\\right)^-\n\\tag{6.26}\n\\end{align}\\]其中 \\(C=\\frac{N-1}{N-k}\\frac m{m-1}\\) 以及 \\(\\tilde{\\symbf \\mu}_i=h(\\symbf X_i\\symbf \\beta_i)\\)。作为各种协方差估计和标准误选项如何影响结果的示例，以下是关于 Cochran Cox 数据中处理的总体 \\(F\\) 值及处理差异平均标准误的影响总结21，这些影响分别来自于 ML 与 REML、基于模型的标准误（带或不带 Kenward-Roger 校正）以及三明治（经验）标准误（有无 Morel et al. 的偏差校正）。用于生成这些数据的 SAS 程序呈现在 SAS Data Program Library 中。请注意未校正三明治估计的极端偏差。REML 与 ML 的差异对于标准误不再重要——它们都向下偏差——并且所得的 \\(F\\) 值严重向上偏差，以至于数值本身不再具有任何意义。基于模型和偏差校正的三明治统计量显示了预期的 REML 与 ML 差异：标准误显示出向下偏差，\\(F\\) 值显示出 ML 相对于 REML 的向上偏差。ML-REML 的影响在偏差校正三明治估计中没有那么明显，但依旧很明显。与 REML 结合使用的偏差校正三明治估计过于保守。这些结果更多是为了说明目的而非作为全面比较的基础。应避免一刀切的建议。以下是一些一般性的指导原则。","code":""},{"path":"chap6.html","id":"sec6-7","chapter":"第 6 章 推断（一）","heading":"6.7 主要思想和总体实施指南的总结","text":"对位置度量的推断——处理均值、差异、对比——需要定义并使用可估和可预测的函数。可估函数涉及第 3 章定义的广义推断；可预测函数涉及狭义推断。总体平均是广义推断的一个重要特例；特定个体是狭义推断的一个重要特例。对可估和可预测函数的所有推断都发生在模型尺度上。本章中考虑的推断包括假设检验和置信区间估计。假设检验有两种一般方法：似然比和基于 Wald 的方法。后者包括真实 Wald 统计量以及通过将 Wald 统计量除以可估或可预测函数的秩而获得的近似 \\(F\\) 统计量。我们在本章中强调基于 Wald 的统计量。对于高斯模型，Wald 和似然比 (LR) 统计量在许多重要情况下可证明是等价的，这使得计算要求更高的 LR 方法没有吸引力，也没有必要。对于 GLMMs，LR 统计量在伪似然估计的背景下是未定义的，并且在积分近似法的背景下通常不切实际。LR 统计量最常与仅固定效应 GLMs 一起使用。对于多因素设计，LR 计算的常用方法使用旨在进行序贯检验的方案。对于不均衡的多因素设计，这可能会产生无意义的结果——免责声明。LR 的计算可修改以计算类似于部分平方和的统计量，但不像 Wald 型统计量那么容易。第 8 章更详细地讨论了部分/序贯检验问题。在协方差分量的推断方面，LR 检验确实比基于 Wald 的检验具有重要优势。我们将在第 7 章探讨这个主题。Wald 统计量需要假定已知尺度参数（包括高斯 LMMs 中的协方差分量）并具有近似 \\(\\chi^2\\) 分布。基于 Wald 的近似 \\(F\\) 统计量使用尺度参数估计并具有近似 \\(F\\) 分布。对于仅固定效应高斯 LM，即 \\(N\\left(\\symbf{X\\beta},\\boldsymbol{\\Sigma}{\\sigma}^2\\right)\\)，其中 \\(\\boldsymbol\\Sigma\\) 已知，以及特定的 LMM（例如均衡裂区），可获得精确结果。我们利用矩阵分布理论中关于二次型的结果来建立精确的结果。对于大多数高斯混合模型和所有非高斯广义模型，分布结果是渐近的。对于近似 \\(F\\) 统计量，分母自由度通常必须近似，例如通过 Satterthwaite 程序。Satterthwaite 程序适用于所有高斯线性模型（LM 和 LMM）。使用伪似然估计的 GLMM 使用了 Satterthwaite 程序的特别修改。对于积分近似，Satterthwaite 近似未定义，因此不适用。第 2 章中提出的“Fisher会怎么做？”框架 ANOVA 法提供了分母自由度的一个大致合理的近似。除了具有已知尺度参数的 GLMs、具有单个未知方差分量的 LMs 或具有均衡数据的仅方差分量的 LMMs 之外，所有的线性模型都需要某种形式的偏差调整。对于基于模型的标准误，这意味着 Kenward-Roger 调整。对于三明治估计，这意味着 Morel et al. 的调整。对于高斯模型，应使用通过 REML 获得的协方差估计。Kenward-Roger 调整在 REML 估计的语境下是良定的，并且对于包括重复测量和空间数据的相关误差模型在内的标准应用已经过充分检验。对于非高斯模型，当 PL 所基于的线性化提供合理的近似值时，使用类 REML 方差估计的基于 Kenward-Roger 模型的 PL 法的特别形式效果很好。Stroup Claassen (2020) 比较了 PL 和求积法的 类错误控制和置信区间精度。他们的工作表明，若 \\(\\symbf y\\mid\\symbf b\\) 的分布至少在某种程度上是对称的，PL 提供了合理的近似，并且对于小数据集，PL 的性能优于求积。对于非高斯模型，当 PL 明显不适用时——比如接近极限的单参数模型和具有极端参数值的双参数指数族模型——积分近似通常更为可取。在使用积分近似时，Kenward-Roger 近似是未定义的。因此，对于 PL 表现不佳的 GLMMs，偏差校正的三明治估计是唯一的替代方案。对于我们在第 ?? 章中详细讨论的边际模型，我们必须使用广义估计方程，或者在 GLIMMIX 程序中使用 PL. 回想，积分近似法只能用于真实的似然——根据定义，边际模型是拟似然模型。这意味着如果我们使用边际模型，我们可以在特别的 Kenward-Roger 调整和偏差校正三明治估计之间进行选择。Morel et al. 在广义估计方程的背景下开发了他们的偏差校正。边际模型的经验表明，对于边际模型的推断，偏差校正三明治估计是首选方法。","code":""},{"path":"chap6.html","id":"exe6","chapter":"第 6 章 推断（一）","heading":"练习","text":"本习题涉及文件 Ch_6_Problem1.sas 显示的数据。数据来自一项比较三种处理的研究（在数据集中表示为 Trt）。数据是从七个地点收集的（在数据集中表示为 Location）。对于每个地点的每种处理，从两个区 (plots) 收集了数据。假定数据是高斯分布的，模型指定如下：\n线性预测器：\\(\\eta_{ij}=\\eta+\\tau_{}+L_{j}+\\left(tL\\right)_{ij}\\)，其中 \\(\\tau_i\\) 表示第 \\(\\) 个处理效应，\\(L_j\\) 表示第 \\(j\\) 个位置效应，\\((tL)_{ij}\\) 表示第 \\(ij\\) 个处理 × 地点交互效应。\n分布：\n\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)\n\\(\\left(tL\\right)_{ij}\\text{ iid }N\\left(0,\\sigma_{TL}^2\\right)\\)\n\\(y_{ijk}\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)\n\n恒等连接\n定义处理均值为 \\(\\bar{y}_{\\cdot\\cdot}=\\frac{1}{7\\times2}\\sum_{j,k}y_{ijk}\\)\n推导出与上述模型一致的处理均值的期望。\n推导出与上述模型一致的处理均值的方差。展示相关步骤。\n\nSAS 文件中的 “Run 1” 是上述模型的 GLIMMIX 程序。验证处理均值的标准误与 (.-ii.) 部分的推导一致，通过：\n给出模型方差分量的估计。\n演示这些估计的使用、演示遵循 (.-ii.) 的推导可产生 SAS listing 所示的标准误。\n\n“Run 2” 展示了人们实现该模型的“前-PROC MIXED”方式。预期均方用于确定处理的 “error term”，而这又作为确定标准误的基础。\n根据 “Run 2” 的输出，有关处理的 “error term” 是什么？\n验证 “Run 2” 中的 LSMEANS 语句是否与 (c.-.) 中的回答一致。\n根据 (b.-.) 的方差分量估计，显示如何获得 “Run 2” listing 中显示的处理均值的标准误。\n比较两次运行的处理平均标准误。这对使用“前-PROC MIXED”软件进行混合模型分析有什么启示？\n\n本习题涉及文件 Ch_6_Problem1.sas 显示的数据。数据来自一项比较三种处理的研究（在数据集中表示为 Trt）。数据是从七个地点收集的（在数据集中表示为 Location）。对于每个地点的每种处理，从两个区 (plots) 收集了数据。假定数据是高斯分布的，模型指定如下：线性预测器：\\(\\eta_{ij}=\\eta+\\tau_{}+L_{j}+\\left(tL\\right)_{ij}\\)，其中 \\(\\tau_i\\) 表示第 \\(\\) 个处理效应，\\(L_j\\) 表示第 \\(j\\) 个位置效应，\\((tL)_{ij}\\) 表示第 \\(ij\\) 个处理 × 地点交互效应。分布：\n\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)\n\\(\\left(tL\\right)_{ij}\\text{ iid }N\\left(0,\\sigma_{TL}^2\\right)\\)\n\\(y_{ijk}\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)\n\\(L_j\\mathrm{~iid~}N\\left(0,\\sigma_L^2\\right)\\)\\(\\left(tL\\right)_{ij}\\text{ iid }N\\left(0,\\sigma_{TL}^2\\right)\\)\\(y_{ijk}\\sim N\\left(\\mu_{ij},\\sigma^2\\right)\\)恒等连接定义处理均值为 \\(\\bar{y}_{\\cdot\\cdot}=\\frac{1}{7\\times2}\\sum_{j,k}y_{ijk}\\)\n推导出与上述模型一致的处理均值的期望。\n推导出与上述模型一致的处理均值的方差。展示相关步骤。\n推导出与上述模型一致的处理均值的期望。推导出与上述模型一致的处理均值的方差。展示相关步骤。SAS 文件中的 “Run 1” 是上述模型的 GLIMMIX 程序。验证处理均值的标准误与 (.-ii.) 部分的推导一致，通过：\n给出模型方差分量的估计。\n演示这些估计的使用、演示遵循 (.-ii.) 的推导可产生 SAS listing 所示的标准误。\n给出模型方差分量的估计。演示这些估计的使用、演示遵循 (.-ii.) 的推导可产生 SAS listing 所示的标准误。“Run 2” 展示了人们实现该模型的“前-PROC MIXED”方式。预期均方用于确定处理的 “error term”，而这又作为确定标准误的基础。\n根据 “Run 2” 的输出，有关处理的 “error term” 是什么？\n验证 “Run 2” 中的 LSMEANS 语句是否与 (c.-.) 中的回答一致。\n根据 (b.-.) 的方差分量估计，显示如何获得 “Run 2” listing 中显示的处理均值的标准误。\n比较两次运行的处理平均标准误。这对使用“前-PROC MIXED”软件进行混合模型分析有什么启示？\n根据 “Run 2” 的输出，有关处理的 “error term” 是什么？验证 “Run 2” 中的 LSMEANS 语句是否与 (c.-.) 中的回答一致。根据 (b.-.) 的方差分量估计，显示如何获得 “Run 2” listing 中显示的处理均值的标准误。比较两次运行的处理平均标准误。这对使用“前-PROC MIXED”软件进行混合模型分析有什么启示？本习题以关于均衡不完全区组设计的 3 次 GLIMMIX 运行开始，具有 7 个处理和 7 个区组，每个区组尺寸为 3. 数据和 GLIMMIX 语句位于文件 Ch6_problem2.sas 中。\nRun 1 使用 GLIMMIX 默认算法。以“GLMM 适当形式”说明本运行拟合的统计模型。\n在 Run 1 中用于获得方差分量估计的统计方法是什么？\n入门教科书指出，处理均值的标准误为 \\(\\sqrt{\\hat\\sigma^2/r}\\)，其中 \\(\\hat\\sigma^2\\) 是残差的估计，\\(r\\) 是每种处理的重复次数。将其与 Run 1 的 listing 中处理均值的标准误进行比较。是否存在差异？如果是，请解释。也就是说，Run 1 如何获得处理均值的标准误？哪个是正确的——入门统计方法传统观点还是 Run 1 的 listing？\n现在转向 Run 2.\n统计模型与 Run 1 相同还是不同？\nRun 2 的方差分量估计与 Run 1 相比如何？\nRun 2 使用什么统计方法来获得方差分量估计?\n\n现在转向 Run 3.\n统计模型与 Run 1 运行相同还是不同？\nRun 3 的方差分量估计与 Run 1 和 Run 2相比如何？\nRun 3 使用什么统计方法来获得方差分量估计？\n\n哪个运行最适合用于报告目的？简单解释一下。\n对于你在 f. 中选择的运行，在分析准备用于报告之前，是否应该对给出的语句进行任何调整？如果是这样，需要调整什么以及为什么要调整？\n本习题以关于均衡不完全区组设计的 3 次 GLIMMIX 运行开始，具有 7 个处理和 7 个区组，每个区组尺寸为 3. 数据和 GLIMMIX 语句位于文件 Ch6_problem2.sas 中。Run 1 使用 GLIMMIX 默认算法。以“GLMM 适当形式”说明本运行拟合的统计模型。在 Run 1 中用于获得方差分量估计的统计方法是什么？入门教科书指出，处理均值的标准误为 \\(\\sqrt{\\hat\\sigma^2/r}\\)，其中 \\(\\hat\\sigma^2\\) 是残差的估计，\\(r\\) 是每种处理的重复次数。将其与 Run 1 的 listing 中处理均值的标准误进行比较。是否存在差异？如果是，请解释。也就是说，Run 1 如何获得处理均值的标准误？哪个是正确的——入门统计方法传统观点还是 Run 1 的 listing？现在转向 Run 2.\n统计模型与 Run 1 相同还是不同？\nRun 2 的方差分量估计与 Run 1 相比如何？\nRun 2 使用什么统计方法来获得方差分量估计?\n统计模型与 Run 1 相同还是不同？Run 2 的方差分量估计与 Run 1 相比如何？Run 2 使用什么统计方法来获得方差分量估计?现在转向 Run 3.\n统计模型与 Run 1 运行相同还是不同？\nRun 3 的方差分量估计与 Run 1 和 Run 2相比如何？\nRun 3 使用什么统计方法来获得方差分量估计？\n统计模型与 Run 1 运行相同还是不同？Run 3 的方差分量估计与 Run 1 和 Run 2相比如何？Run 3 使用什么统计方法来获得方差分量估计？哪个运行最适合用于报告目的？简单解释一下。对于你在 f. 中选择的运行，在分析准备用于报告之前，是否应该对给出的语句进行任何调整？如果是这样，需要调整什么以及为什么要调整？本习题使用与习题 2 相同的数据集。参考文件 Ch_6_problem3.sas 中的运行。\nRun 1 使用 GLIMMIX 默认算法。以“GLMM 适当的形式”说明本运行拟合的统计模型。\n研究人员使用 Run 1 中的广义卡方作为过度离散的证据（即，广义卡方/DF = 5.66 >>1 因此过度分散）。根据 20 世纪 80 年代 GLM 教科书的推荐，研究人员使用尺度参数来调整过度分散。Run 2 显示了该调整。研究人员计算了 Run 1 和 2 中的 “-2 Res Log Pseudo-Likelihood” 之差，以获得他们所描述的有关 \\(H_{0}\\colon\\phi=0\\) 的似然比检验，其中 \\(\\phi\\) 是尺度参数的标准符号。\n研究人员是否使用了适当的统计量作为过度分散的证据？如果没有，他们应该做什么？\n验证研究人员为检验 \\(H_{0}\\colon\\phi=0\\) 获得的似然比统计量为具有 1 个自由度的 \\(\\chi^2= 49.92\\) 。\n研究人员将检验 \\(H_{0}\\colon\\phi=0\\) 表征为为似然比检验是否合理？请解释。\n如果你尝试使用 COVTEST 获取 \\(H_{0}\\colon\\phi=0\\) 的检验统计量，会发生什么？\n\n展示一种替代方法（而不是这里的 Run 1 和 2），研究人员可以采用积分近似法（使用 Method=Laplace ——在计算关键部分时，求积法在我的笔记本电脑上大约需要 90 分钟来运行；而拉普拉斯法大约只需 3 秒。对于这个习题，拉普拉斯法和求积法所得估计几乎相同）。\n在你的替代 Run 1 中，你使用什么来代替广义卡方/DF？你得到了什么？该统计量是否是过度分散的证据？\n在你的替代 Run 2 中，你如何解释过度分散？该项的似然比检验的结果是什么？你在此获得的处理的 \\(F\\) 值与 Run 2 相比如何？\n\n本习题使用与习题 2 相同的数据集。参考文件 Ch_6_problem3.sas 中的运行。Run 1 使用 GLIMMIX 默认算法。以“GLMM 适当的形式”说明本运行拟合的统计模型。研究人员使用 Run 1 中的广义卡方作为过度离散的证据（即，广义卡方/DF = 5.66 >>1 因此过度分散）。根据 20 世纪 80 年代 GLM 教科书的推荐，研究人员使用尺度参数来调整过度分散。Run 2 显示了该调整。研究人员计算了 Run 1 和 2 中的 “-2 Res Log Pseudo-Likelihood” 之差，以获得他们所描述的有关 \\(H_{0}\\colon\\phi=0\\) 的似然比检验，其中 \\(\\phi\\) 是尺度参数的标准符号。\n研究人员是否使用了适当的统计量作为过度分散的证据？如果没有，他们应该做什么？\n验证研究人员为检验 \\(H_{0}\\colon\\phi=0\\) 获得的似然比统计量为具有 1 个自由度的 \\(\\chi^2= 49.92\\) 。\n研究人员将检验 \\(H_{0}\\colon\\phi=0\\) 表征为为似然比检验是否合理？请解释。\n如果你尝试使用 COVTEST 获取 \\(H_{0}\\colon\\phi=0\\) 的检验统计量，会发生什么？\n研究人员是否使用了适当的统计量作为过度分散的证据？如果没有，他们应该做什么？验证研究人员为检验 \\(H_{0}\\colon\\phi=0\\) 获得的似然比统计量为具有 1 个自由度的 \\(\\chi^2= 49.92\\) 。研究人员将检验 \\(H_{0}\\colon\\phi=0\\) 表征为为似然比检验是否合理？请解释。如果你尝试使用 COVTEST 获取 \\(H_{0}\\colon\\phi=0\\) 的检验统计量，会发生什么？展示一种替代方法（而不是这里的 Run 1 和 2），研究人员可以采用积分近似法（使用 Method=Laplace ——在计算关键部分时，求积法在我的笔记本电脑上大约需要 90 分钟来运行；而拉普拉斯法大约只需 3 秒。对于这个习题，拉普拉斯法和求积法所得估计几乎相同）。\n在你的替代 Run 1 中，你使用什么来代替广义卡方/DF？你得到了什么？该统计量是否是过度分散的证据？\n在你的替代 Run 2 中，你如何解释过度分散？该项的似然比检验的结果是什么？你在此获得的处理的 \\(F\\) 值与 Run 2 相比如何？\n在你的替代 Run 1 中，你使用什么来代替广义卡方/DF？你得到了什么？该统计量是否是过度分散的证据？在你的替代 Run 2 中，你如何解释过度分散？该项的似然比检验的结果是什么？你在此获得的处理的 \\(F\\) 值与 Run 2 相比如何？本习题也基于与习题 2 相同的数据集。习题参考文件 Ch_6_problem4.sas 中的运行。共有 3 次运行使用与习题 4 中相同的 model 以及 method= 选项。有 3 次运行使用与习题 2 中相同的 MODEL 语句和 method= 选项。\nRun 1 使用 GLIMMIX 默认算法。以“GLMM-适当形式”说明本运行拟合的统计模型。\n在 Run 1 中，用于获得方差分量和模型估计的统计方法是什么？\n现在转向 Run 2.\n统计模型与 Run 1 相同还是不同？\nRun 2 的处理的方差分量估计和 \\(F\\) 值与 Run 1 相比如何？\nRun 2 用于获得方差分量和模型估计的统计方法是什么？\n\n现在转向 Run 3.\n统计模型与 Run 1 相同还是不同？\nRun 3 的处理的方差分量估计和 \\(F\\) 值与 Run 1 和 2 相比如何？\nRun 3 用于获得方差分量和模型估计的统计方法是什么？\n\n这里 Run 2 和 3 之间的差异与习题 4 中观察到的 Run 2 和 3 之间的差异相比如何？解释任何差异。\n本习题也基于与习题 2 相同的数据集。习题参考文件 Ch_6_problem4.sas 中的运行。共有 3 次运行使用与习题 4 中相同的 model 以及 method= 选项。有 3 次运行使用与习题 2 中相同的 MODEL 语句和 method= 选项。Run 1 使用 GLIMMIX 默认算法。以“GLMM-适当形式”说明本运行拟合的统计模型。在 Run 1 中，用于获得方差分量和模型估计的统计方法是什么？现在转向 Run 2.\n统计模型与 Run 1 相同还是不同？\nRun 2 的处理的方差分量估计和 \\(F\\) 值与 Run 1 相比如何？\nRun 2 用于获得方差分量和模型估计的统计方法是什么？\n统计模型与 Run 1 相同还是不同？Run 2 的处理的方差分量估计和 \\(F\\) 值与 Run 1 相比如何？Run 2 用于获得方差分量和模型估计的统计方法是什么？现在转向 Run 3.\n统计模型与 Run 1 相同还是不同？\nRun 3 的处理的方差分量估计和 \\(F\\) 值与 Run 1 和 2 相比如何？\nRun 3 用于获得方差分量和模型估计的统计方法是什么？\n统计模型与 Run 1 相同还是不同？Run 3 的处理的方差分量估计和 \\(F\\) 值与 Run 1 和 2 相比如何？Run 3 用于获得方差分量和模型估计的统计方法是什么？这里 Run 2 和 3 之间的差异与习题 4 中观察到的 Run 2 和 3 之间的差异相比如何？解释任何差异。本习题也基于与习题 2 相同的数据集。习题参考文件 Ch_6_problem5.sas 中的运行。该文件展示了五个运行。前两个是高斯数据。后三个假定二项数据。\n前两个运行产生相同的结果。为什么？\nRun 3 使用与 Run 1 相同的线性预测器和 RANDOM 语句。Run 4 使用与 Run 2 相同的线性预测器和 RANDOM 语句。然而，与 Run 1 和 2 不同，Run 3 和 4 不会产生相同的结果。为什么？\n一些 GLM 书籍表明 Run 3 和 Run 4 不同，因为与高斯分布不同，二项分布没有尺度参数。这些书籍表明，如果在 Run 3 中加入一个尺度参数，其结果将与 Run 4 相同。Run 5 在 Run 3 的基础上加入了尺度参数。其结果是否证实了这些书籍的说法？\n要理解发生了什么，请回忆 \\(Var\\left(\\symbf{y}^*\\right)=\\symbf{V}^*=\\symbf{Z}\\symbf{G}\\symbf{Z}^\\prime+\\symbf{R}^*\\)，其中 \\(\\symbf{R}^*=\\Delta\\symbf{V}_\\mu^{\\frac12}\\symbf{P}\\symbf{V}_\\mu^{\\frac12}\\Delta\\)。对于给区组中的三个观测，\n\\(\\symbf{y}_j=\\begin{bmatrix}y_{ij}\\\\y_{'j}\\\\y_{''j}\\end{bmatrix}\\)\n对于：\nRun 1\nRun 2\nRun 3\nRun 4\nRun 5\n\n给出 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)。\n对于 Run 4，展示构成得分向量和 Hessian（或信息）矩阵以获得协方差分量的 PL 估计所需的每个协方差项的 \\(\\frac{\\partial\\symbf{V}^*}{\\partial\\sigma_i}\\) 形式。\n最后，考虑 Run 6 和 7. 请注意，它们产生的结果是相同的。为什么？请回答：\n推导出 Run 6 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)\n推导出 Run 7 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)\n验证 Run 6 和 7 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\) 是等价的\n\n本习题也基于与习题 2 相同的数据集。习题参考文件 Ch_6_problem5.sas 中的运行。该文件展示了五个运行。前两个是高斯数据。后三个假定二项数据。前两个运行产生相同的结果。为什么？Run 3 使用与 Run 1 相同的线性预测器和 RANDOM 语句。Run 4 使用与 Run 2 相同的线性预测器和 RANDOM 语句。然而，与 Run 1 和 2 不同，Run 3 和 4 不会产生相同的结果。为什么？一些 GLM 书籍表明 Run 3 和 Run 4 不同，因为与高斯分布不同，二项分布没有尺度参数。这些书籍表明，如果在 Run 3 中加入一个尺度参数，其结果将与 Run 4 相同。Run 5 在 Run 3 的基础上加入了尺度参数。其结果是否证实了这些书籍的说法？要理解发生了什么，请回忆 \\(Var\\left(\\symbf{y}^*\\right)=\\symbf{V}^*=\\symbf{Z}\\symbf{G}\\symbf{Z}^\\prime+\\symbf{R}^*\\)，其中 \\(\\symbf{R}^*=\\Delta\\symbf{V}_\\mu^{\\frac12}\\symbf{P}\\symbf{V}_\\mu^{\\frac12}\\Delta\\)。对于给区组中的三个观测，\n\\(\\symbf{y}_j=\\begin{bmatrix}y_{ij}\\\\y_{'j}\\\\y_{''j}\\end{bmatrix}\\)\n对于：\nRun 1\nRun 2\nRun 3\nRun 4\nRun 5\nRun 1Run 2Run 3Run 4Run 5给出 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)。对于 Run 4，展示构成得分向量和 Hessian（或信息）矩阵以获得协方差分量的 PL 估计所需的每个协方差项的 \\(\\frac{\\partial\\symbf{V}^*}{\\partial\\sigma_i}\\) 形式。最后，考虑 Run 6 和 7. 请注意，它们产生的结果是相同的。为什么？请回答：\n推导出 Run 6 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)\n推导出 Run 7 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)\n验证 Run 6 和 7 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\) 是等价的\n推导出 Run 6 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)推导出 Run 7 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\)验证 Run 6 和 7 隐含的 \\(\\symbf{V}_j^*=Var\\left(\\symbf{y}_j^*\\right)\\) 是等价的请参考为第 4 章习题 1 编写的用于两处理设计的 IML 程序。扩写每个程序，使其获得处理 1 和处理 2 的估计和标准误以及处理 1 和处理 2 之间的平均差。对于非高斯数据情况，为模型以及数据尺度编写语句。请参考为第 4 章习题 1 编写的用于两处理设计的 IML 程序。扩写每个程序，使其获得处理 1 和处理 2 的估计和标准误以及处理 1 和处理 2 之间的平均差。对于非高斯数据情况，为模型以及数据尺度编写语句。与习题 6 类似，但改为为第 5 章习题 2 - 5 编写 IML 程序。修改每个程序以包含由 LSMEANS 语句产生的输出（模型和数据的尺度估计、标准误和置信限）。此外，编写 IML 语句来计算近似 \\(F\\) 统计量。与习题 6 类似，但改为为第 5 章习题 2 - 5 编写 IML 程序。修改每个程序以包含由 LSMEANS 语句产生的输出（模型和数据的尺度估计、标准误和置信限）。此外，编写 IML 语句来计算近似 \\(F\\) 统计量。","code":""},{"path":"chap7.html","id":"chap7","chapter":"第 7 章 推断（二）","heading":"第 7 章 推断（二）","text":"","code":""},{"path":"chap7.html","id":"sec7-1","chapter":"第 7 章 推断（二）","heading":"7.1 介绍","text":"在第 6 章中，我们重点讨论了可估和可预测函数的推断。在本章中，我们将注意力转向 \\(\\symbf\\sigma\\) 的元素，即方差和协方差分量。使用第 4 章开头的约定，当我们提到“协方差分量”时，我们指的是 \\(\\symbf\\sigma\\) 的所有部分：方差、协方差，在某些情况下还包括相关参数。我们可能对协方差分量的推断感兴趣，因为我们的目标针对的是 \\(\\symbf\\sigma\\) 本身的元素，或者我们可能只需要评估相互竞争的协方差模型作为一种手段来达到目的——将“正确的”协方差模型作为对可估或可预测函数进行推断的前奏。直接关注协方差分量的常见场景包括遗传学，其中协方差分量度量关注性状的遗传力，以及质量改进，其中协方差分量确定问题区域。“达到目的的手段”的应用包括检验关于协方差的假定（例如，我们能否证明在单向方差分析中可以假定方差相等？），或者在重复测量或空间数据的相互竞争的相关误差模型中进行选择。我们可将协方差分量的推断分为三大类：1) 正式检验；2) 拟合统计量和 3) 区间估计。当我们有明显的嵌套层次结构时，我们只能对协方差分量进行正式检验。我们在 7.2 节中介绍了正式的协方差分量检验。正式检验并不总是可能的，甚至是不可取的。我们经常想要比较非嵌套的协方差模型。在这种情况下，我们必须使用拟合统计量。我们考虑非嵌套比较的例子，并在 7.3 节中介绍拟合统计量。正式检验和拟合统计量要求我们有良定的似然。当我们对非高斯 GLMM 使用伪似然估计时，情况并非如此。因此，非高斯 GLMM 为协方差分量的推断带来了其他线性模型前所未有的挑战。我们在 7.2.3 节中考虑了用于检验的这些问题，并在 7.3 节中回顾了用于拟合统计量的这些问题。最后，7.4 节重点讨论协方差分量的置信区间构建。","code":""},{"path":"chap7.html","id":"sec7-2","chapter":"第 7 章 推断（二）","heading":"7.2 协方差分量的正式检验","text":"虽然检验协方差分量的方法有很多，但这里我们将考虑三种方法，并主要关注一种方法——似然比检验。似然比检验是协方差分量的一种正式检验，足够灵活，可以与 GLMM 及其所有特殊情况（LMM, GLM 和 LM）一起使用。我们还简单地考虑基于 ANOVA 的检验，因为它们在线性模型历史中很重要。然而，与基于 ANOVA 的方差分量估计一样，基于 ANOVA 的检验只能用于仅方差分量的 LMMs，因此它们在当代线性建模中的价值有限。最后，我们考虑基于 Wald 的检验。我们这样做主要是因为他们的小样本表现非常糟糕，从而向学习线性模型的学生发出警告。","code":""},{"path":"chap7.html","id":"sec7-2-1","chapter":"第 7 章 推断（二）","heading":"7.2.1 仅方差分量 LMMs 的基于 ANOVA 的检验","text":"顾名思义，基于 ANOVA 的方差分量检验涉及将 ANOVA 表中的 F 检验应用于代表随机模型效应的变异源。我们可以通过例子来说明基于 ANOVA 的检验。考虑第 3 章 Data Set 3.2 使用的四处理、八地点数据。这些数据的 ANOVA 为通过检查期望均方，我们发现地点的 \\(F\\) 值可用作 \\(H_0\\colon{\\sigma}_L^2=0\\) 的检验统计量。给定 \\(F = 3.41\\) 且 \\(p = 0.0135\\)，对于任何大于 \\(p\\) 值的 \\(\\alpha\\)，有足够的证据得出 \\(H_0\\colon{\\sigma}_L^2>0\\)。正如 Self Liang (1987) 指出的，我们应该将 ANOVA 的 \\(p\\) 值除以 2，因为根据定义，ANOVA \\(F\\) 检验是双侧的，而方差分量的检验必须是单侧检验。即，\\(H_0\\colon{\\sigma}_L^2\\ne 0\\) 不能作为备择假设，因为 \\(\\sigma_L^2\\) 不能为负。此检验的正确 \\(p\\) 值为 \\(0.0135/2\\cong0.0068\\)。所有 ANOVA 检验都以这种方式进行。我们通过检验期望均方来构造 \\(F\\) 检验。对于更复杂的仅方差分量模型，分母可以是均方的线性组合。例如，假设我们有一个裂区实验，其中有两个固定效应因子 和 C 以及一个随机效应因子 B. 和 B 的因子组合随机分配给整区单元，C 的水平随机分配给裂区单元。我们可以将所得 LMM 写为线性预测器：\\(\\eta_{ijkl}=\\eta+\\alpha_i+b_j+(ab)_{ij}+\\gamma_k+(\\alpha\\gamma)_{ik}+(bc)_{ij}+(abc)_{ijk}+w_{ijk}\\)，其中 \\(\\alpha\\) 表示 效应，\\(b\\) 表示 B 效应，\\(\\gamma\\) 表示 C 效应，\\(w\\) 表示整区效应分布：\\(y_{ijkl}\\mid b_j,(ab)_{ij},(ac)_{jk},(abc)_{ijk},w_{ijl}\\mathrm{~iid~}N\\left(\\mu_{ijkl},\\sigma_S^2\\right)\\)，其中 \\(\\sigma^2_S\\) 表示裂区单元之间的方差。\\(b_j\\text{ iid }N\\left(0,\\sigma_B^2\\right)\\text{,}\\left(ab\\right)_{ij}\\text{ iid }N\\left(0,\\sigma_{AB}^2\\right)\\text{,}\\left(bc\\right)_{jk}\\text{ iid }N\\left(0,\\sigma_{BC}^2\\right)\\text{,}\\left(abc\\right)_{ijk}\\text{ iid }N\\left(0,\\sigma_{ABC}^2\\right)\\) 以及 \\(w_{ijk}\\mathrm{~iid~}N\\big(0,\\sigma_W^2\\big)\\)。连接：\\(\\eta_{ijkl}=\\mu_{ijkl}\\)ANOVA 表为代表随机处理效应的变异源以粗体显示。用于检验 \\(\\sigma_{BC}^2\\) 和 \\(\\sigma_{ABC}^2\\) 的 \\(F\\) 值是简单的 MS 之比。另一方面，分离 \\(\\sigma_{AB}^2\\) 和 \\(\\sigma_{B}^2\\) 所需的比值需要更复杂的项。具体而言，\\(\\sigma_{AB}^2\\) 的分母项必须估计 \\(\\sigma_{S}^{2}+4\\sigma_{ABC}^{2}+2\\sigma_{W}^{2}\\)，即 EMS(AB) 中除 \\(\\sigma_{AB}^2\\) 之外的元素。所得检验统计量为\\[F_{AB}=\\frac{MS(AB)}{MS(ABC)+MSW-MSR}\\]其中 MSW 和 MSR 分别表示全区和残差（裂区）MS。对于 \\(\\sigma^2_B\\)，\\[F_B=\\frac{MS(B)}{MS(AB)+MS(BC)-MS(ABC)}\\]给出所需的检验统计量。这两个检验需要对分母自由度进行 Satterthwaite 近似才能确定 \\(p\\) 值。这是我们使用 ANOVA EMS 法所能达到的极限。对于仅方差分量 LMM 之外的模型，我们需要寻找其他方法来检验协方差分量。","code":""},{"path":"chap7.html","id":"sec7-2-2","chapter":"第 7 章 推断（二）","heading":"7.2.2 用于协方差分量检验的 Wald 统计量以及为什么不应使用","text":"协方差分量检验的 Wald 统计量使用 \\(\\symbf{V}_A\\left(\\hat{\\symbf{\\sigma}}\\right)\\) 的对角元、\\(\\symbf{\\sigma}\\) 的渐近协方差阵、协方差分量估计，在第 5 章 5.4.2 节中给出。对于第 \\(\\) 个协方差分量，渐近方差为\\[\\begin{align}\nV_{,ii}=2\\times \\operatorname{trace}\\Bigg[\\symbf{P}\\Bigg(\\frac{\\partial \\symbf V(\\sigma)}{\\partial\\sigma_i}\\Bigg)\\symbf{P}\\Bigg(\\frac{\\partial \\symbf V(\\sigma)}{\\partial\\sigma_i}\\Bigg)\\Bigg]^{-1}\n\\tag{7.1}\n\\end{align}\\]其中 \\(\\symbf{P}=\\left[\\symbf{V}\\left(\\hat{\\symbf{\\sigma}}\\right)\\right]^{-1}-\\left[\\symbf{V}\\left(\\hat{\\symbf{\\sigma}}\\right)\\right]^{-1}\\symbf{X}\\left(\\symbf{X}^{\\prime}\\left[\\symbf{V}\\left(\\hat{\\symbf{\\sigma}}\\right)\\right]^{-1}\\symbf{X}\\right)^{-}\\symbf{X}^{\\prime}\\left[\\symbf{V}\\left(\\hat{\\symbf{\\sigma}}\\right)\\right]^{-1}\\)。用于检验 \\(H_0\\colon\\sigma_i=0\\) 的 Wald 统计量为\\[Wald_{\\sigma_i}=\\frac{\\hat{\\sigma}_i}{\\sqrt{V_{,ii}}}\\]\\(Wald_{\\sigma_i}\\) 使用标准高斯 \\(N (0, 1)\\) 分布进行计算。例如，使用 4 处理、8 地点的 Data Set 3.2，我们有以下结果方差估计：\\(\\hat{\\symbf{\\sigma}}=\\begin{bmatrix}\\hat{{\\sigma}}_L^2\\\\\\hat{{\\sigma}}^2\\end{bmatrix}=\\begin{bmatrix}1.735\\\\2.877\\end{bmatrix}\\)渐近方差：\\(\\symbf{V}_A\\left(\\hat{\\symbf{\\sigma}}\\right)=\\begin{bmatrix}1.771&-0.197\\\\-0.197&0.788\\end{bmatrix}\\)用于检验 \\(H_0\\colon\\sigma_L=0\\) 的 Wald 统计量为\\[Wald\\left(\\sigma_L^2\\right)=\\frac{1.735}{\\sqrt{1.771}}=1.30\\]\\(p\\) 值为 0.1922。回想，对于基于 ANOVA 的检验，\\(F = 3.41\\) 且 \\(p = 0.0135\\)。这是典型的协方差分量 Wald 检验：无意义的结果——通常是非常保守的结果。发生这种情况的原因源于方差分量估计的分布。对于仅方差分量的 LMM，\\(\\hat{\\symbf\\sigma}\\) 的元素取决于 \\(\\chi^2\\) 随机变量的线性组合，即强右偏的分布。中心极限定理对于 Wald 统计量确实成立，但对于方差估计，收敛到正态的速度非常慢。因此，除非样本量达到数千（最好是数万或数十万），否则 Wald 统计量一无是处。对于较小的样本量，协方差分量估计的抽样分布明显是非高斯的。底线：任何受过教育的数据分析师不应使用 Wald 协方差分量检验，但具有数万或更多观测的超大型研究可能除外。","code":""},{"path":"chap7.html","id":"sec7-2-3","chapter":"第 7 章 推断（二）","heading":"7.2.3 协方差分量的似然比检验","text":"协方差分量的似然比检验类似于第 6 章中 (6.20) 定义的可估函数的似然比检验。保持线性预测器的固定效应部分为常数，我们在 \\(H_0\\cdot \\symbf \\sigma=\\symbf\\sigma_0\\) 下拟合模型，确定似然，然后拟合全模型。所得似然比统计量为\\[\\begin{align}\nLR\\left(\\symbf\\sigma_0\\right)=2\\ell\\left(\\hat{\\symbf\\sigma}\\right)-2\\ell\\left(\\hat{\\symbf\\sigma}_0\\right)\n\\tag{7.2}\n\\end{align}\\]注意到\\[LR\\left(\\symbf\\sigma_0\\right)=-2\\log\\left[\\frac{f\\left(\\hat{\\symbf\\sigma}_0\\right)}{f\\left(\\hat{\\symbf\\sigma}\\right)}\\right]\\]其中 \\(f(\\hat{\\symbf \\sigma})\\) 表示在协方差向量估计 \\(\\hat{\\symbf \\sigma}\\) 处计算的似然。如果我们用高斯数据拟合 LMM 模型，我们应使用 REML 估计 \\(\\symbf\\sigma\\)，从而使用 REML 似然产生似然比统计量。\\[-2\\log\\left[\\frac{f_R\\left(\\hat{\\symbf\\sigma}_0\\right)}{f_R\\left(\\hat{\\symbf\\sigma}\\right)}\\right]\\]其中 \\(f_R(\\hat{\\symbf \\sigma})\\) 表示 (5.27) 所示的 REML 似然。无论我们根据全似然还是 REML 似然计算似然比，\\(LR(\\sigma_0)\\) 具有近似 \\(\\chi^2_\\nu\\) 分布的这一结果都成立。自由度 \\(\\nu\\) 等于 \\(\\symbf\\sigma\\) 中协方差参数的数量与 \\(\\symbf\\sigma_0\\) 中协方差参数的数量之差。由于是卡方近似，似然比统计量的另一种符号是 \\(\\chi^2_{LR}\\)。举例说明，让我们回到 4 处理、8 地点的 Data Set 3.2 并检验 \\(H_0\\colon\\sigma^2_L=0\\)。在全模型下\\[\\symbf\\sigma=\\begin{bmatrix}\\sigma_L^2\\\\\\sigma^2\\end{bmatrix}\\]在 \\(H_0\\) 下，\\(\\symbf\\sigma_0 = \\begin{bmatrix}\\sigma^2 \\end{bmatrix}\\) ，即在原假设下，通过移除随机地点效应来简化模型。在全模型下，REML 似然为 \\(4.4477× 10^{-28}\\)，因此 −2 倍 REML 对数似然为 125.95。在简化模型下，REML 似然为 \\(4.4148×10^{-29}\\)，−2 倍 REML 对数似然为 130.58. 所得似然比统计量为 \\(LR(\\sigma_L^2=0)= 4.62\\)。使用 \\(\\chi^2_1\\) 评估 \\(LR\\) ——因为全模型有两个方差分量，而简化模型有一个，因此 \\(\\nu = 1\\) ——我们得到 \\(p = 0.0158\\)。这与基于 ANOVA 的检验一致。","code":""},{"path":"chap7.html","id":"sec7-2-3-1","chapter":"第 7 章 推断（二）","heading":"7.2.3.1 单向方差分析：方差齐性检验","text":"让我们考虑另一个例子。Data Set 7.1 包含来自四种处理的完全随机设计数据。假定数据是高斯分布的，我们可以考虑以下模型线性预测器：\\(\\eta_i=\\eta+\\tau_i\\)分布：通常的等方差假定：\\(y_{ij}\\mathrm{~iid~}N\\left(\\mu_i,\\sigma^2\\right)\\)；不等方差假定：\\(y_{ij}\\sim NI\\left(\\mu_i,\\sigma^2_i\\right)\\)连接：\\(\\eta_i=\\mu_i\\)在继续推断处理效应 \\(\\tau_i\\) 之前，我们需要解决方差问题：所有处理的方差相等还是不相等？我们不能通过基于 ANOVA 的检验来解决这个问题，但我们可以使用似然比检验。这里，检验具有三个自由度，因为不等方差模型具有四个方差参数，\\(\\symbf\\sigma'=\\begin{bmatrix}\\sigma_1^2&\\sigma_2^2&\\sigma_3^2&\\sigma_4^2\\end{bmatrix}\\)，而等方差模型具有一个方差参数，\\(\\symbf\\sigma=\\sigma_0^2\\)。由此，\\(\\nu=4-1=3\\)。在原假设 \\(H_0\\colon\\) 所有 \\(\\sigma_i^2=\\sigma^2,=1,2,3,4\\) 下，共同方差估计为 \\(\\hat\\sigma^2=0.90\\) 以及 \\(-2\\ell\\left(\\hat{\\symbf\\sigma}_0\\right)=38.25\\)。在全模型下，\\(-2\\ell\\left(\\hat{\\symbf\\sigma}\\right)=35.49\\)，这给出了似然比统计量 \\(\\chi_{LR}^2=2.86,p=0.4133\\)。似然比检验没有提供拒绝等方差模型的证据。旁注：在“一般”线性模型时代，当强调 ..d. 误差范式的优越性时，如果我们确实拒绝了 \\(H_0\\)，它将引发一场小型危机，以寻找方差稳定变换的形式。而在当代的建模中，我们则直接利用不等方差模型继续进行可估函数的推断工作。","code":""},{"path":"chap7.html","id":"sec7-2-3-2","chapter":"第 7 章 推断（二）","heading":"7.2.3.2 重复测量示例：选择简约协方差模型","text":"我们对重复测量数据（更一般地，相关误差模型）的主要讨论在第 ?? 章和第 ?? 章中。然而，协方差分量检验最重要的应用之一，就是在完整的协方差模型中进行选择。考虑到这一点，让我们看一个例子，这个例子是第 ?? 章和第 ?? 章示例的预告，并说明如何使用似然比检验来选择协方差模型。Data Set 7.2 包含对总共 12 名个体进行六次测量的重复测量数据，其中 6 名个体随机分配到两种处理之一，另 6 名个体随机分配到另一种处理。“重复测量数据”是指来自任何研究的数据，其中在研究过程中按计划时间观察到与处理相关的重复单元。这些观察可能是每小时、每天、每月进行一次，也可能是不等间隔的，有策略地计划在研究背景下被认为重要的时间进行。因此，描述 Data Set 7.2 的另一种方式是完全随机设计，其中有两个处理和随机分配给每个处理的六个重复。对每个个体进行六次预先计划的观察；例如在处理开始后的 1, 2, 4, 8, 16 和 24 周。假定高斯数据，我们可以将模型的一般形式写为：线性预测器：\\(\\eta_{ijk}=\\alpha_{ij}+s_{ik}\\)，其中 \\(\\alpha_{ij}\\) 是处理 \\(\\) 和时间 \\(j\\) 对线性预测变量的组合固定效应，\\(s_{ik}\\) 是分配给第 \\(\\) 个处理的第 \\(k\\) 个个体的随机效应。我们可以根据研究目标以多种方式划分 \\(\\alpha_{ij}\\)。最常见的两种是因子效应划分，\\(\\alpha_{ij}=\\alpha+\\tau_i+\\gamma_j+(\\tau\\gamma)_{ij}\\)，其中 \\(\\alpha\\) 表示截距，\\(\\tau_i\\) 是处理主效应，\\(\\gamma_j\\) 是时间主效应，\\((\\tau\\gamma)_{ij}\\) 是时间与治疗的交互作用；以及多项式回归划分\\[\\alpha_{ij}=\\beta_{0i}+\\sum_{m=1}^5\\beta_{mi}T_j^m\\]其中，\\(\\beta_{0i}\\) 表示第 \\(\\) 个处理的截距，\\(\\beta_{mi}\\) 表示第 \\(\\) 个处理的 \\(m\\) 阶回归系数，\\(T_{jm}\\) 表示第 \\(j\\) 次观测的 \\(m\\) 次方。因此，\\(m = 1\\) 表示线性回归项，\\(m = 2\\) 表示二次项，等等。分布：\\(s_{ik}\\text{ iid }N\\left(0,\\sigma_S^2\\right)\\)。令 \\(\\symbf{y'}_{ik}\\mid s_{ik}=\\begin{pmatrix}\\begin{bmatrix}y_{i1k}&y_{i2k}&y_{i3k}&y_{i4k}&y_{i5k}&y_{i6k}\\end{bmatrix}|s_{ik}\\end{pmatrix}\\)。\\(\\left(\\symbf{y}\\mid\\symbf{s}\\right)\\sim N\\left(\\symbf{\\mu}\\mid\\symbf{s},\\boldsymbol\\Sigma\\right)\\) 其中 \\(\\symbf{\\mu}=\\symbf{1}_{ts}\\otimes E\\Big(\\symbf{y}_{ik}\\mid s_{ik}\\Big)\\)，\\(\\boldsymbol{\\Sigma}=\\symbf{}_{ts}\\otimes Var\\left(\\symbf{y}_{ik}\\mid s_{ik}\\right)\\) 以及 \\(ts\\) 等于处理数 \\(t\\) × 个体数 \\(s\\)。分布：\\(s_{ik}\\text{ iid }N\\left(0,\\sigma_S^2\\right)\\)。令 \\(\\symbf{y'}_{ik}\\mid s_{ik}=\\begin{pmatrix}\\begin{bmatrix}y_{i1k}&y_{i2k}&y_{i3k}&y_{i4k}&y_{i5k}&y_{i6k}\\end{bmatrix}|s_{ik}\\end{pmatrix}\\)。\\(\\left(\\symbf{y}\\mid\\symbf{s}\\right)\\sim N\\left(\\symbf{\\mu}\\mid\\symbf{s},\\boldsymbol\\Sigma\\right)\\) 其中 \\(\\symbf{\\mu}=\\symbf{1}_{ts}\\otimes E\\Big(\\symbf{y}_{ik}\\mid s_{ik}\\Big)\\)，\\(\\boldsymbol{\\Sigma}=\\symbf{}_{ts}\\otimes Var\\left(\\symbf{y}_{ik}\\mid s_{ik}\\right)\\) 以及 \\(ts\\) 等于处理数 \\(t\\) × 个体数 \\(s\\)。连接：恒等，\\(\\symbf\\eta=\\symbf\\mu\\)连接：恒等，\\(\\symbf\\eta=\\symbf\\mu\\)方便起见，将第 \\(ik\\) 个个体对应的协方差阵 \\(\\boldsymbol\\Sigma\\) 的分块对角元表示为 \\(\\boldsymbol\\Sigma_{ik}\\)。为什么要对 \\(y_{ijk}\\mid s_{ik}\\) 的方差进行所有阐述？为什么它们不像前面例子中的其他高斯线性模型那样 \\(\\operatorname{iid}N\\left(\\mu_{ijk},\\sigma^2\\right)\\)？因为当我们对同一个体进行一段时间的观测时，我们必须考虑到这些观测具有相关性的可能，也就是说，在给定的个体内，\\(Cov\\left(y_{ijk},y_{ij'k}\\mid s_{ik}\\right)\\) 对于时间 \\(j \\ne j\\)，不是必然为零。我们将协方差阵 \\(\\boldsymbol\\Sigma_{ik}\\) 的一般形式写为\\[\\begin{align}\n\\boldsymbol\\Sigma_{ik}=\\begin{bmatrix}\\sigma_1^2&\\sigma_{12}&\\sigma_{13}&\\sigma_{14}&\\sigma_{15}&\\sigma_{16}\\\\&\\sigma_2^2&\\sigma_{23}&\\sigma_{24}&\\sigma_{25}&\\sigma_{26}\\\\&&\\sigma_3^2&\\sigma_{34}&\\sigma_{35}&\\sigma_{36}\\\\&&&\\sigma_4^2&\\sigma_{45}&\\sigma_{46}\\\\&&&&\\sigma_5^2&\\sigma_{56}\\\\&&&&&\\sigma_6^2\\end{bmatrix}\n\\tag{7.3}\n\\end{align}\\]其中在时间 \\(j\\) 时的观测之间的方差 \\(\\sigma_j^2=Var\\left(y_{ijk}\\mid s_{ik}\\right)\\)，\\(\\sigma_{ij}=Cov\\left(y_{ijk},y_{ij'k}\\mid S_{ik}\\right)\\)。这称为非结构化协方差阵 (unstructured covariance matrix). 潜在地，每次观测都具有独特的方差，并且同一个体每两次的观测都具有独特的协方差。对于 \\(J\\) 次观测，非结构化协方差阵中有 \\(J+[J(J-1)/2]\\) 个协方差参数组成 \\(\\symbf\\sigma\\)。不管怎么想，这都不是一个简约的模型。在另一个极端，如果我们假定所有 \\(y_{ijk}\\mid s_{ik}\\sim N\\left(\\mu_{ijk},\\sigma^2\\right)\\)，因此 \\(\\boldsymbol\\Sigma_{ik} = \\symbf \\sigma^2\\)，那么我们就有了一个时间裂区协方差模型 (split-plot--time covariance model)，之所以这么称呼是因为它与裂区实验是相同的模型（我们将在第 9 章中更详细地讨论裂区和其他多水平混合模型）。时间裂区模型也称为独立模型 (independence model). 在第 3 章中，我们看到时间裂区或独立模型也可表示为复合对称模型。非结构化模型和独立/复合对称模型都不适合大多数重复测量分析。就检验假设的统计功效而言，使用非结构化模型通常是浪费且成本高昂的。另一方面，独立/复合对称模型无法解释重复测量之间的重要相关性。当确实存在不可忽略的相关性时，这会导致 类错误率过高。我们通常可以找到折衷 (middle ground)，即一种充分考虑相关性的协方差模型，但比非结构化模型更简约。这样做可以让我们完全控制 类错误率，而不会不必要地牺牲功效。我们将在第 ?? 章和第 ?? 章更详细地讨论协方差模型。目前，两个常见的折衷模型是一阶事前相关模型 (first-order ante-dependence model) ——通常简称为 ANTE(1) ——以及一阶自回归模型 (first-order auto-regressive model) ——简写为 AR(1). ANTE(1) 的协方差阵为\\[\\begin{align}\n\\boldsymbol{\\Sigma}_{{_{tk}}}=\\begin{bmatrix}\\sigma_{1}^{2}&\\sigma_{1}\\sigma_{2}\\rho_{1}&\\sigma_{1}\\sigma_{3}\\rho_{1}\\rho_{2}&\\sigma_{1}\\sigma_{4}\\rho_{1}\\rho_{2}\\rho_{3}&\\sigma_{1}\\sigma_{5}\\rho_{1}\\rho_{2}\\rho_{3}\\rho_{4}&\\sigma_{1}\\sigma_{6}\\rho_{1}\\rho_{2}\\rho_{3}\\rho_{4}\\rho_{5}\\\\&\\sigma_{2}^{2}&\\sigma_{2}\\sigma_{3}\\rho_{2}&\\sigma_{2}\\sigma_{4}\\rho_{2}\\rho_{3}&\\sigma_{2}\\sigma_{5}\\rho_{2}\\rho_{3}\\rho_{4}&\\sigma_{2}\\sigma_{6}\\rho_{2}\\rho_{3}\\rho_{4}\\rho_{5}\\\\&&\\sigma_{3}^{2}&\\sigma_{3}\\sigma_{4}\\rho_{3}&\\sigma_{3}\\sigma_{5}\\rho_{3}\\rho_{4}&\\sigma_{3}\\sigma_{6}\\rho_{3}\\rho_{4}\\rho_{5}\\\\&&&\\sigma_{4}^{2}&\\sigma_{4}\\sigma_{5}\\rho_{4}&\\sigma_{4}\\sigma_{6}\\rho_{4}\\rho_{5}\\\\&&&&\\sigma_{5}^{2}&\\sigma_{5}\\sigma_{6}\\rho_{5}\\\\&&&&&\\sigma_{6}^{2}\\end{bmatrix}\n\\tag{7.4}\n\\end{align}\\]其中，\\(\\rho_j\\) 表示 \\(y_{ijk}\\mid s_{ik}\\) 和 \\(y_{,j+1,k}\\mid s_{ik}\\) 之间的相关性，\\(j=1,2,\\ldots,5\\)。同一个体非相邻观测之间的相关性由间隔中包含的 \\(\\rho_j\\) 的乘积来建模。在这个例子中，对于六次重复测量，ANTE(1) 允许我们将相对于非结构化模型的协方差参数的数量从 21 个减少到 11 个。如果我们进一步假定所有方差和所有相邻观测的相关性都相等，即所有 \\(\\sigma_j^2 =\\sigma^2\\) 且所有 \\(\\rho_j=\\rho\\)，那么我们就有一个 AR(1) 协方差模型。AR(1) 模型只有 2 个协方差参数。在许多应用中，AR(1) 提供了个体内相关性的适当模型，从而在不牺牲 类错误控制的情况下将功效最大化。对于 Data Set 7.2，竞争模型的 REML -2 倍对数似然值为似然比检验用于比较当前模型与下一行更简单的模型。例如，似然比 12.17 检验原假设，即除了 ANTE(1) 模型之外，需要指定非结构化模型所需的额外 10 个协方差参数是否均为零。\\(p\\) 值为 0.2765 意味着我们无法拒绝这一假设——相对于 ANTE(1) 模型，非结构化模型并未增加任何价值，而是浪费且降低了功效。从表中，我们看到我们未能拒绝所有相邻观测相关性均相等且所有方差均相等的原假设——也就是说，ANTE(1) 模型的额外复杂性不会为这些数据增加任何价值——但我们确实拒绝了 AR(1) 相关系数为零的假设。因此，对于这些数据，我们可以得出结论，个体内存在非平凡的相关性，并且，如果仅限于此处所示的协方差模型选择，则 AR(1) 模型提供了对该相关性进行充分建模的最简约的方法。后续分析应使用 AR(1) 模型。只要我们满足两个条件，似然比方法就可以很好地工作。首先，我们必须有一个合理的似然：REML 或全似然。我们不能使用伪似然进行似然比检验。其次，比较的模型必须是嵌套的。此处，独立模型是 AR(1) + 随机个体效应模型（\\(\\rho=0\\)）的子集，而 AR(1) 模型又是 ANTE(1) 模型（具有如上所示的方差和相关性公式）的子集，ANTE(1) 模型又是非结构化模型的子集。如果模型不是嵌套的，我们就不能使用似然比方法。例如，在重复测量分析中经常使用的无额外 \\(s_{ik}\\) 效应的 AR(1) 模型不能与使用似然比检验的复合对称模型进行比较，因为这两个模型都有 2 个协方差参数。在下一节中，我们讨论伪似然问题。在 7.3 节中，我们讨论非嵌套模型的比较问题。","code":""},{"path":"chap7.html","id":"sec7-2-4","chapter":"第 7 章 推断（二）","heading":"7.2.4 GLMMs 的 PL 与积分近似的结果","text":"当我们将相关误差模型拟合到高斯数据时，个体内相关性自然成为 \\(\\symbf R\\) 矩阵的一部分，回想 \\(\\symbf R = Var(\\symbf y\\mid \\symbf b)\\)，给定随机效应的观测的条件方差。对于上一节描述的所有重复测量模型，\\(\\symbf R=\\symbf \\otimes \\boldsymbol \\Sigma_{ik}\\)。当我们将重复测量模型拟合到非高斯数据时，这是如何工作的？对于指数族的非高斯成员和所有拟似然，\\(Var(\\symbf y\\mid \\symbf b)\\) 至少部分依赖于 \\(E(\\symbf y\\mid\\symbf b )\\)。这意味着我们不能像处理高斯数据那样只编写具有相关结构的 \\(\\symbf R\\) 矩阵，而忽略方差函数。解决这个问题有两种方法：“LMM 类比”和“费舍尔会怎么做？”前者得到工作相关矩阵和边际模型，也称为 R 侧建模 (R-side modeling) 或 GEE 型建模 (GEE-type modeling). 后者导致通过线性预测器而不是 \\(Var(\\symbf y\\mid \\symbf b)\\) 来对相关性建模。这称为 G 侧建模 (G-side modeling). 上一节中的 AR(1) + 随机个体效应模型能最好地说明这种差异。","code":""},{"path":"chap7.html","id":"sec7-2-4-1","chapter":"第 7 章 推断（二）","heading":"7.2.4.1 R 侧或工作相关模型","text":"对于非高斯 GLMMs，回想，在矩阵形式中，\\(Var\\left(\\symbf{y}\\mid\\symbf{b}\\right)=\\symbf{V}_\\mu^{1/2}\\symbf{}\\symbf{V}_\\mu^{1/2}\\)，其中 \\(\\symbf V_\\mu\\) 表示方差函数矩阵，\\(\\symbf \\) 表示尺度参数矩阵。我们在第 5 章中看到，对 \\(\\symbf y\\mid\\symbf b\\) 条件分布中的协方差建模的一种方法是，用工作相关性 (working correlation)，又称工作协方差阵 (working covariance matrix)，替换尺度矩阵。对于 AR(1) 重复测量模型，我们将工作协方差阵写为\\[\\begin{align}\n\\symbf{}_W=\\phi\\begin{bmatrix}1&\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5\\\\&1&\\rho&\\rho^2&\\rho^3&\\rho^4\\\\&&1&\\rho&\\rho^2&\\rho^3\\\\&&&1&\\rho&\\rho^2\\\\&&&&1&\\rho\\\\&&&&&1\\end{bmatrix}\n\\tag{7.5}\n\\end{align}\\]例如，假设我们有二项数据。Data Set 7.3 给出了这样一个数据集，其设计结构与 Data Set 7.2 相同，但具有二项而不是高斯响应变量。用于该数据的 R 侧 AR(1) GLMM 为：线性预测器：与高斯模型相同，即 \\(\\eta_{ijk}=\\alpha_{ij}+s_{ik}\\)，其中 \\(\\alpha_{ij}\\) 表示处理-时间分量，可按照高斯模型的描述进行划分。分布：\n\\(s_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_S^2\\right)\\)。这称为 G 侧，因为它描述了线性预测器中随机效应的分布，并且一般地，\\(Var\\left(\\symbf{b}\\right)=\\symbf{G}\\)。\n\\(y_{ijk}\\mid s_{ik}\\sim\\mathrm{Binomial}\\left(n_{ijk},\\pi_{ijk}\\right)\\)，但对方差进行拟似然校正：\\(Var\\left(\\symbf{y}_{ik}\\mid s_{ik}\\right)=\\symbf{V}_\\mu^{1/2}\\symbf{}_W\\symbf{V}_\\mu^{1/2}\\)，其中 \\(\\symbf y_{ik}\\) 表示第 \\(ik\\) 个个体的 6 次观测的向量（如上面针对高斯模型所述），以及 \\(\\symbf A_W\\) 为 (7.5) 中给出的工作协方差。因此：\n\\[Var\\left(\\symbf{y}\\mid\\symbf{s}\\right)=\\symbf{}_{ts}\\otimes\\left(\\symbf{V}_\\mu^{1/2}\\symbf{}_W\\symbf{V}_\\mu^{1/2}\\right)\\]\n\\(s_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_S^2\\right)\\)。这称为 G 侧，因为它描述了线性预测器中随机效应的分布，并且一般地，\\(Var\\left(\\symbf{b}\\right)=\\symbf{G}\\)。\\(y_{ijk}\\mid s_{ik}\\sim\\mathrm{Binomial}\\left(n_{ijk},\\pi_{ijk}\\right)\\)，但对方差进行拟似然校正：\\(Var\\left(\\symbf{y}_{ik}\\mid s_{ik}\\right)=\\symbf{V}_\\mu^{1/2}\\symbf{}_W\\symbf{V}_\\mu^{1/2}\\)，其中 \\(\\symbf y_{ik}\\) 表示第 \\(ik\\) 个个体的 6 次观测的向量（如上面针对高斯模型所述），以及 \\(\\symbf A_W\\) 为 (7.5) 中给出的工作协方差。因此：\n\\[Var\\left(\\symbf{y}\\mid\\symbf{s}\\right)=\\symbf{}_{ts}\\otimes\\left(\\symbf{V}_\\mu^{1/2}\\symbf{}_W\\symbf{V}_\\mu^{1/2}\\right)\\]连接：\\(\\mathrm{logit}\\left(\\pi_{ijk}\\right)\\)GLMM 从业者通常将此称为“GEE 型”模型。由于该模型在线性预测器中包含随机个体效应，因此它不是真正的 GEE 模型。真正的 GEE 模型是完全边际的：线性预测器仅包含固定效应，即 \\(\\symbf\\eta=\\symbf{X\\beta}\\) 并且所有的方差结构都包含于 \\(\\symbf{V}_\\mu^{1/2}\\symbf{}_W\\symbf{V}_\\mu^{1/2}\\)。GEE 型模型具有工作协方差阵，但线性预测器中可能包含其他随机效应。GEE 型模型的估计必须使用某种形式的线性化：第 5 章描述的伪似然、Breslow Clayton (1993) 描述的惩罚拟似然或明确地使用广义估计方程，如 Liang Zeger (1986) 所述。在本次讨论中，我们使用伪似然法。我们现在描述 G 侧的替代方法，然后比较并对比这两种方法的模型选择问题。","code":""},{"path":"chap7.html","id":"sec7-2-4-2","chapter":"第 7 章 推断（二）","heading":"7.2.4.2 Fisher 会怎么做？G 侧方法","text":"回想第 2 章，模型构建的“费舍尔会怎么做？”过程，包括编写“地形”（或设计）方面和处理方面的框架 ANOVA，然后将它们组合起来。对这些数据执行此操作会得到：正如我们在第 3 章中看到的，当我们使用组合框架 ANOVA 来构建高斯数据的模型时，我们必须将最后一行解释为“残差”，以解释高斯分布的尺度参数 \\(\\sigma^2\\)，或者在更复杂的情况下，如重复测量，\\(\\symbf y\\mid\\symbf b\\) 的协方差阵。然而，我们不需要对指数族的单参数成员进行此操作，例如本例中的二项。这就提出了一个问题：二项数据重复测量之间的相关性出现在何处？从概念上讲，在二项 GLMM 中，我们假定 \\(\\pi_{ijk}\\) 因处理和时间效应而系统地变异，并且它会受到个体间和个体内变异的随机扰动。同一个体重复测量之间的相关性必须是后者的一部分：个体内 \\(\\pi_{ijk}\\) 的随机扰动。换言之：在应用连接之前对随机扰动 \\(\\pi_{ijk}\\) 中的相关性进行建模更有意义，还是在应用连接之后通过工作相关性进行建模更有意义。前者要求个体内变异是线性预测器的一部分，而且给予我们一个真实的似然。后者给了我们一个拟似然和一个不对应于任何可行的概率机制的过程。“Fisher 会怎么做”给了我们如下的 GLMM：线性预测器：\\(\\eta_{ijk}=\\alpha_{ij}+s_{ik}+w_{ijk}\\)线性预测器：\\(\\eta_{ijk}=\\alpha_{ij}+s_{ik}+w_{ijk}\\)分布：\\(y_{ijk}\\mid s_{ik},w_{ijk}\\sim\\mathrm{Binomial}\\left(n_{ijk},\\pi_{ijk}\\right)\\)，\\(s_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_s^2\\right)\\)，\\(\\mathbf{w}_{ik}\\sim N\\left(\\mathbf{0},\\boldsymbol\\Sigma_{ik}\\right)\\)，其中 \\(\\symbf{w}'_{ik}=\\begin{bmatrix}w_{i1k}&w_{i2k}&w_{i3k}&w_{i4k}&w_{i5k}&w_{i6k}\\end{bmatrix}\\) 以及\n\\[\\begin{align}\n\\boldsymbol\\Sigma_{ik}=\\sigma_W^2\\begin{bmatrix}1&\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5\\\\&1&\\rho&\\rho^2&\\rho^3&\\rho^4\\\\&&1&\\rho&\\rho^2&\\rho^3\\\\&&&1&\\rho&\\rho^2\\\\&&&&1&\\rho\\\\&&&&&1\\end{bmatrix}\n\\tag{7.6}\n\\end{align}\\]分布：\\(y_{ijk}\\mid s_{ik},w_{ijk}\\sim\\mathrm{Binomial}\\left(n_{ijk},\\pi_{ijk}\\right)\\)，\\(s_{ik}\\mathrm{~iid~}N\\left(0,\\sigma_s^2\\right)\\)，\\(\\mathbf{w}_{ik}\\sim N\\left(\\mathbf{0},\\boldsymbol\\Sigma_{ik}\\right)\\)，其中 \\(\\symbf{w}'_{ik}=\\begin{bmatrix}w_{i1k}&w_{i2k}&w_{i3k}&w_{i4k}&w_{i5k}&w_{i6k}\\end{bmatrix}\\) 以及\n\\[\\begin{align}\n\\boldsymbol\\Sigma_{ik}=\\sigma_W^2\\begin{bmatrix}1&\\rho&\\rho^2&\\rho^3&\\rho^4&\\rho^5\\\\&1&\\rho&\\rho^2&\\rho^3&\\rho^4\\\\&&1&\\rho&\\rho^2&\\rho^3\\\\&&&1&\\rho&\\rho^2\\\\&&&&1&\\rho\\\\&&&&&1\\end{bmatrix}\n\\tag{7.6}\n\\end{align}\\]连接：\\(\\eta_{ijk}=\\operatorname{logit}\\left(\\pi_{ijk}\\right)\\)连接：\\(\\eta_{ijk}=\\operatorname{logit}\\left(\\pi_{ijk}\\right)\\)这是一个 G 侧模型，因为除了 \\(y_{ijk}\\mid s_{ik},w_{ijk}\\) 的条件分布之外，所有的随机变异性都出现在线性预测器中。请注意，该模型中的所有分布都是真实的概率分布。这意味着可以使用伪似然或积分近似来进行估计和推断。这对于 GLMMs 的协方差模型选择具有重要意义。","code":""},{"path":"chap7.html","id":"sec7-2-4-3","chapter":"第 7 章 推断（二）","heading":"7.2.4.3 R 侧与 G 侧：协方差模型选择的后果","text":"对于具有工作相关结构的非高斯模型，我们必须使用伪似然估计。对于仅 G 侧的非高斯模型，我们可以使用伪似然或积分近似。为什么这很重要？我们可以通过检查似然比统计量来回答这个问题。我们知道，在实践中，我们将检验协方差分量的似然比统计量计算为 \\(\\chi_{LR}^2=2\\ell\\left(\\hat{\\symbf\\sigma}\\right)-2\\ell\\left(\\hat{\\symbf\\sigma}_0\\right)\\)，其中 \\(\\ell\\left(\\hat{\\symbf\\sigma}\\right)\\) 表示在 \\(\\hat{\\symbf\\sigma}\\) 处计算的对数似然。使用积分近似，作为估计过程的一部分我们可获得 \\(\\ell\\left(\\hat{\\symbf\\sigma}\\right)\\) 。但是使用伪似然时，我们从不计算 \\(\\ell\\left(\\hat{\\symbf\\sigma}\\right)\\)。我们构建了一个伪似然，因此我们最接近 \\(\\ell\\left(\\hat{\\symbf\\sigma}\\right)\\) 的是 \\(p\\ell\\left(\\hat{\\symbf\\sigma}\\right)\\)，即在 \\(\\hat{\\symbf\\sigma}\\) 处计算的伪似然。使用此方法进行似然比检验的问题在于，全模型伪似然所基于的伪变量 \\(\\symbf y^*\\) 与简化模型所基于的伪变量不同，因此伪似然也不同。虽然我们可以计算一个伪似然比 \\(2p\\ell\\left(\\hat{\\sigma}\\right)-2p\\ell\\left(\\hat{\\sigma}_0\\right)\\)，但由于全模型下的伪变量 \\(\\symbf y^*_0\\) 和原假设（即简化模型）下的伪变量 \\(\\symbf y_0\\) 不具有可比性，因此它的解释存疑\n。对于 Data Set 7.3，下表显示了 R 侧模型的伪对数似然以及使用拉普拉斯法计算的、具有我们在高斯示例（Data Set 7.2）中考虑的协方差结构的 G 侧模型的对数似然。请注意，R 侧复合对称 (CS) 和独立模型并不等价（正如我们在第 3 章中看到的），因此每个模型都有不同的伪似然。尽管表中没有明确说明，但所有对数似然和伪对数似然统计量均已乘以 -2.如果我们使用伪似然统计量，检验非结构化协方差模型与 ANTE(1) 协方差模型的 \\(p\\) 值为 0.0008，这（虚假地）表明我们必须使用非结构化工作协方差模型来充分解释个体内相关。G 侧方法给出了更准确的刻画：带有额外随机个体效应 \\(s_{ik}\\) 的 AR(1) 模型是足够的。我们如何知道 R 侧结果是虚假的？其实 Data Set 7.3 是模拟的。该数据是使用 AR(1) + 随机个体模型生成的。这是一个典型的模拟数据集：R 测伪似然比检验未能可靠地选择 AR(1) + 随机个体模型；而 G 侧似然比检验做到了。总之，对于协方差分量的正式检验，似然比检验是我们可使用的最通用 (versatile) 且最普遍适用 (generally applicable) 的工具。对于高斯数据，使用 REML 对数似然来构造似然比统计量。对于非高斯数据，我们必须使用一种允许计算真实似然的估计方法，例如通过拉普拉斯或高斯-厄尔米特求积的积分近似。这将非高斯 GLMM 的正式协方差分量检验限制为仅 G 侧模型，即没有工作相关结构的模型。对于许多 GLMMs 来说，我们可以在 G 侧模型和 R 侧、GEE 型模型之间进行选择。如果优先考虑正式的协方差检验，请尽可能使用 G 侧模型。上一段不应被解读为对 R 侧、GEE 型建模的全面谴责。有许多情况是 GEE 型模型是唯一可行的方法，而在其他情况下，GEE 是首选方法。我们将在第 ??、?? 章（过度分散）和第 ?? 章（非高斯重复测量）中进一步讨论这个问题。请注意，我们讨论的所有检验都要求固定效应 \\(\\symbf{X\\beta}\\) 保持不变。在原假设下，全模型和简化模型之间唯一允许变化的是模型的随机部分，对于 LMM 为 \\(\\symbf {Zb},\\symbf G\\) 和 \\(\\symbf R\\)，对于 GLMMs 为 \\(\\symbf {Zb},\\symbf G\\) 或 \\(\\symbf A_W\\)。现在，我们将注意力转向协方差模型的比较，对于这种比较，正式的检验要么不可能，要么根本不合适。","code":""},{"path":"chap7.html","id":"sec7-3","chapter":"第 7 章 推断（二）","heading":"7.3 用拟和统计量比较协方差模型","text":"假设我们想了解复合对称模型的拟合与没有额外个体随机效应的 AR(1) 模型的拟合相比如何。两个模型都有 2 个协方差参数—— \\(\\sigma^2\\) 和 \\(\\rho\\)，尽管我们在这两个模型中对这些参数的解释不同。两者都不是另一个的子集。我们无法构建正式的检验。即使可能进行正式检验，许多人认为，对于许多应用来说，假设检验并不是识别适当、简约模型的关键。一些人认为检验是不合适的；另一些人只是表示，虽然检验没有错，但它也不能提供信息。有关这些问题的更广泛讨论，见 Burnham Anderson (2002) 和 Konishi Kitagawa( 2008).信息准则提供了正式检验的替代方案。目前应用最广泛的两种信息准则是 AIC-Akaike(1974) 信息准则和 Schwarz (1978) 提出的 BIC-Bayes 信息准则。Burnham Anderson (1998) 建议对 Akaike 准则进行小样本校正——校正的信息准则缩写为 AICC. 本节简要概述了信息准则背后的理由。然后，我们使用 Data Set 7.2 和 7.3 更新示例以说明其应用。","code":""},{"path":"chap7.html","id":"sec7-3-1","chapter":"第 7 章 推断（二）","heading":"7.3.1 AIC 和 AICC","text":"AIC 的根源可追溯到 Kullbach Leibler (1951)，他们提出了两个模型之间差异的度量，他们用函数 \\(f\\) 表示“真相” (“truth”)，用函数 \\(g\\) 表示“近似模型” (“approximating model). 按照他们的符号，“真相”只取决于数据，由此得到函数 \\(f(\\symbf y)\\)，而近似模型取决于模型和参数，因此表示为 \\(g(\\symbf y\\mid\\symbf\\theta)\\)。对于连续数据，Kullback Leiber 首先使用了一个被 Konishi Kitagawa 称为 K-L 信息的量，记为 \\((f,g)\\)，定义为：\\[E_f\\left[\\log\\!\\left(\\frac{f\\left(\\symbf{y}\\right)}{g\\left(\\symbf{y}\\mid\\symbf{\\theta}\\right)}\\right)\\right]\\]请注意，K-L 信息在概念上类似于似然比统计量。此外，完美的近似模型将产生 \\(\\frac{f\\left(\\symbf y\\right)}{g\\left(\\symbf y \\mid \\symbf\\theta\\right)}=1\\) 的比值，因此对数比等于 0。我们可将 \\((f,g)\\) 重写为 \\(E_f\\left\\{\\log\\Bigl[f\\left(\\symbf{y}\\right)\\Bigr]\\right\\}-E_f\\left\\{\\log\\Bigl[g\\bigl(\\symbf{y}\\left|\\symbf{\\theta}\\right)\\Bigr]\\right\\}\\)。令 \\(E_f\\left\\{\\log\\!\\left[f\\left(\\symbf{y}\\right)\\right]\\right\\}=C\\)，其中 \\(C\\) 为满足 \\(-E_f\\left\\{\\log\\Bigl[g\\bigl(\\symbf{y}\\mid\\symbf{\\theta}\\bigr)\\Bigr]\\right\\}=\\bigl(f,g\\bigr)-C\\) 的常数。式 \\((f,g)-C\\) 称为 K-L 距离。Akaike 表明 \\(-2\\ell\\left(\\hat{\\symbf{\\theta}}\\right)+2p\\)（其中 \\(p\\) 表示参数向量 \\(\\symbf\\theta\\) 中的参数数量）提供了 K-L 距离的良好近似。AIC (Akaike information criterion) 定义为\\[\\begin{align}\nAIC=-2\\ell\\left(\\hat{\\symbf{\\theta}}\\right)+2p\n\\tag{7.7}\n\\end{align}\\]注意，我们可将 AIC 视为 -2 乘以对数似然值减去一个等于模型参数数量的惩罚项。我们知道，如果我们向模型中添加额外的参数，我们总是可以增加对数似然值，直到 \\(\\ell(\\symbf y)\\) 给出最大的可能值。这就是我们在第 6 章中讨论的偏差统计量的基本思想。然而，我们会遇到一个边际收益递减的点，并且拟合模型的整体目的是为数据的变异或表现提供一个简约的解释。AIC 对向模型中添加新参数进行惩罚；除非对数似然值的增加超过了添加到模型中的参数数量，否则我们认为添加这些参数是适得其反的。Burnham Anderson (1998) 建议进行小样本校正。令\\[n^*=\\begin{cases}N\\;\\text{ 若 }\\;\\ell(\\mathbf{\\theta})\\;\\text{ 为全似然}\\\\N-\\operatorname{rank}(\\symbf{X})\\;\\text{ 若 }\\ell(\\symbf{\\theta})\\text{ 为}\\;\\text{REML 似然}\\end{cases}\\]其中 \\(N\\) 表示观测总数。小样本校正为\\[\\frac{2p(p+1)}{n^*-p-1}\\]为 AIC 加上校正项得到小样本调整，或“校正” AIC：\\[\\begin{align}\nAICC=AIC+\\frac{2p\\left(p+1\\right)}{n^*-p-1}=-2\\ell\\left(\\symbf \\theta\\right)+\\frac{2pn^*}{n^*-p-1}\n\\tag{7.8}\n\\end{align}\\]","code":""},{"path":"chap7.html","id":"sec7-3-2","chapter":"第 7 章 推断（二）","heading":"7.3.2 BIC","text":"Schwarz (1978) 提出了一种使用贝叶斯观点的替代方法。我们有许多相互竞争的模型，其一被认为是“正确的”模式。对于每个相互竞争的模型，你都有一个它是“正确的”模型的先验概率。假定所有竞争模型是等可能的，Schwarz 推导出了贝叶斯信息准则 (Bayesian information\ncriterion, BIC)：\\[BIC=-2\\ell\\left(\\hat{\\symbf{\\theta}}\\right)+p\\log s\\]其中 \\(s\\) 表示个体数。","code":""},{"path":"chap7.html","id":"sec7-3-3","chapter":"第 7 章 推断（二）","heading":"7.3.3 协方差模型比较的应用","text":"对于使用 REML 估计的 LMMs，由于固定效应参数不出现在 REML 似然中，所以对数似然的参数向量为 \\(\\symbf\\sigma\\)。参数数量等于协方差参数的数量。保持设计尺寸值 \\(n^*\\) 和 \\(s\\) 不变。为了使用 REML 评估 LMM 协方差模型，三个信息准则可写为\\[\\begin{align}\n&\\text{AIC:}-2\\ell_R\\left(\\hat{\\symbf\\sigma}\\right)+2p\\notag\\\\\n\n&\\text{AICC: AIC}+\\frac{2p\\left(p+1\\right)}{n^*-p-1}=-2\\ell_R\\left(\\hat{\\symbf{\\sigma}}\\right)+\\frac{2pn^*}{n^*-p-1}\n\\tag{7.9}\n\\\\\n&\\text{BIC: }-2\\ell_R\\left(\\hat{\\symbf{\\sigma}}\\right)+p\\log\\left(s\\right)\\notag\n\\end{align}\\]其中 \\(\\ell_R\\left(\\hat{\\symbf{\\sigma}}\\right)\\) 表示在 \\(\\hat{\\symbf\\sigma}\\) 处计算的 REML 对数似然。对于使用积分近似的 GLMMs，参数向量包括协方差参数和固定效应。如果我们令 \\(p\\) 等于协方差分量的数量——以保持与 (7.9) 的一致性——并回想固定效应参数的数量等于 \\(\\operatorname{rank} (\\symbf X)\\)，我们可使用积分近似将 GLMMs 的信息准则写为\\[\\begin{align}\n&\\text{AIC: }-2\\ell\\left(\\hat{\\symbf\\sigma},\\hat{\\symbf\\beta}\\right)+2\\left[p+\\operatorname{rank}\\left(\\symbf{X}\\right)\\right]\\notag\\\\\n\n&\\text{AICC: }-2\\ell\\left(\\hat{\\symbf{\\sigma}},\\hat{\\symbf{\\beta}}\\right)+\\frac{2\\left[p+\\operatorname{rank}\\left(\\symbf{X}\\right)\\right]n^*}{n^*-\\left[p+\\operatorname{rank}\\left(\\symbf{X}\\right)\\right]-1}\n\n\\tag{7.10}\n\n\\\\&\\text{BIC: }-2\\ell\\Big(\\hat{\\symbf\\sigma},\\hat{\\symbf{\\beta}}\\Big)+\\Big[p+\\operatorname{rank}(\\symbf{X})\\Big]\\text{log}(s)\\notag\n\\end{align}\\]其中 \\(\\ell\\Big(\\hat{\\symbf\\sigma},\\hat{\\symbf{\\beta}}\\Big)\\) 表示在参数估计处计算的全对数似然。同样，如果我们保持模型的固定效应部分不变，那么竞争模型的信息准则之间的差异仅由协方差参数导致，从而使我们能够比较各种协方差模型。在 7.2 节中，我们讨论了使用似然比检验来比较协方差模型的两个例子：使用 REML 的高斯数据的 Data Set 7.2 和通过拉普拉斯法使用积分近似的非高斯数据 Data Set 7.3. 继续这些示例，让我们看看信息准则的应用。首先，我们考虑高斯数据。基于 REML 的信息准则为：回想 AIC 的起源，它是对 K-L 距离（即近似模型与“真相”之间的差异）的近似—— AIC 值越小越好。我们也以类似的方式解释其他信息准则。根据所有三个准则，“获胜”模型是 AR(1). 虽然 ANTE(1) 和非结构化模型都具有更大的对数似然值，但该改进并未抵消协方差参数数量的增加。我们可以看到信息准则的吸引力——我们可以快速扫描所有竞争模型的信息并做出选择。请注意，随着协方差参数数量的增加，AICC 相对于 AIC 的校正值会大幅增加。Burnham Anderson 建议，AICC 是这三个信息准则中最可靠的，可作为这些数据的协方差模型选择的指南。现在，二项 G 侧重复测量 GLMM 的基于全似然的信息准则为：同样，结果与我们使用似然比检验发现的结果一致。AR(1) 模型提供了最简约的拟合——我们可将非结构化和 ANTE(1) 模型膨胀的信息准则解释为它们对协方差结构“过度建模” (“-model”) 的证据，以及将复合对称/独立模型膨胀的信息准则解释为对协方差结构“建模不足” (“-model”) 的证据。过度建模会损害功效，而建模不足则无法控制 类错误。","code":""},{"path":"chap7.html","id":"sec7-4","chapter":"第 7 章 推断（二）","heading":"7.4 区间估计","text":"当研究的目标针对协方差分量本身时，我们可能对这些分量的区间估计更感兴趣，而不是检验关于它们的假设。我们简要地讨论一下这个问题。有兴趣的读者可以在专门讨论方差分量的出版物中找到对此主题的更深入讨论，例如 Searle et al. (1992). 这里我们考虑三种构建置信区间的方法。第一个基于标准高斯 \\(N(0,1)\\) 分布，我们提到它主要是因为许多人出于无知而使用它（但不应使用），学习线性模型的学生应该知道这一点。第二个是基于 \\(\\chi^2\\) 近似的 Wald 程序。第三个基于轮廓似然 (profile likelihood).","code":""},{"path":"chap7.html","id":"首先不该做什么","chapter":"第 7 章 推断（二）","heading":"首先：不该做什么","text":"之前，我们看到一种永远不应该用于协方差分量的检验统计量是使用正态近似的 Wald 统计量，即对于检验 \\(H_0\\colon\\sigma_i=0\\)，\\[Wald_{\\sigma_i}=\\frac{\\hat{\\sigma}_i}{\\sqrt{V_{A_{_{ii}}}}}\\]其中 \\(A_{_{ii}}\\) 为 \\(\\hat\\sigma_i\\) 的渐近方差。它基于假定 \\[Z=\\frac{\\hat{{\\sigma}}_i-{\\sigma}_i}{\\sqrt{V_{,ii}}}\\] 具有近似 \\(N(0,1)\\) 分布。我们直言不讳：“任何受过教育的数据分析师不应使用 Wald 协方差分量检验。”除非样本量很大，否则正态近似非常差。与此相关的是根据正态近似导出的置信区间\\[\\hat{\\sigma}_i\\pm{Z}_{\\alpha_2}\\sqrt{V_{,ii}}\\]其中 \\((1− \\alpha)\\) 表示置信水平。同样，对于小于数万或数十万的样本量，正态近似非常差。我们强烈反对这种构建置信区间的方法。重复一遍：任何受过教育的数据分析师都不应该以这种方式构建置信区间。","code":""},{"path":"chap7.html","id":"sec7-4-1","chapter":"第 7 章 推断（二）","heading":"7.4.1 基于 \\(\\chi^2\\) 的 Wald 法","text":"这种方法只是对在入门统计课程中教授的关于方差的标准区间估计的一个详细说明。令 \\(\\sigma_i\\) 为感兴趣的协方差分量。其估计 \\(\\hat \\sigma_i\\) 具有近似分布\\[\\frac{\\sigma_i\\chi_\\nu^2}\\nu\\]经计算得到\\[\\frac{\\nu\\times\\hat{\\sigma}_i}{\\chi_{\\nu,1-\\alpha/2}^2}<\\sigma_i<\\frac{\\nu\\times\\hat{\\sigma}_i}{\\chi_{\\nu,\\alpha/2}^2}\\]我们用它来构建置信区间，其中 \\((1− \\alpha)\\) 表示置信水平。","code":""},{"path":"chap7.html","id":"sec7-4-2","chapter":"第 7 章 推断（二）","heading":"7.4.2 基于似然的方法","text":"此程序基于似然比统计量 \\(\\chi_{LR}^2=2\\ell\\left(\\hat{\\symbf\\sigma}\\right)-2\\ell\\left(\\hat{\\symbf\\sigma}_0\\right)\\)。令 \\(\\sigma_{0,}\\) 为 \\(H_0\\) 下 \\(\\sigma_i\\) 置信区间的感兴趣参数。我们可将 \\(\\ell(\\symbf\\sigma_0)\\) 重写为 \\(\\ell(\\symbf\\sigma_{0,},\\hat{\\symbf\\sigma}_{'})\\)，其中 \\(\\symbf\\sigma_{'}\\) 定义为移除 \\(\\sigma_i\\) 的协方差参数向量，并且 \\(\\hat{\\symbf\\sigma}_{'}\\) 是其最大似然或 REML 估计，具体取决于我们用于估计协方差分量的程序。遵循 Patiwan (2001)，我们将此函数称为轮廓似然 (profile likelihood).定义\\[\\ell\\left(\\sigma_{^{\\prime}}\\mid\\tilde{\\symbf\\sigma}\\right)=\\sup_{\\sigma_{'}}\\left[\\ell\\left(\\tilde{\\sigma}\\mid\\symbf\\sigma_{^{\\prime}}\\right)\\right]\\]其中我们将 \\(\\sigma_i\\) 保持为常数 \\(\\tilde\\sigma\\)。\\(\\sigma_i\\) 的轮廓似然置信区间是所有满足 \\(2\\bigg[\\ell\\big(\\hat{\\symbf\\sigma}\\big)-\\ell\\big(\\symbf\\sigma\\big|\\tilde{\\sigma}\\big)\\bigg]<\\chi_{1,1-\\alpha}^2\\) 的 \\(\\sigma_i\\) 的集合，其中 \\((1− \\alpha)\\) 表示置信水平。SAS® PROC GLIMMIX 提供了计算协方差分量的基于 \\(\\chi^2\\) 的 Wald 置信区间和轮廓似然置信区间的选项，如果需要，还提供了 GEE 型模型中工作协方差参数的选项。对于这些模型，轮廓似然使用伪对数似然。有关更多详细信息，请参阅 SAS/STAT/PROC GLIMMIX Version 9.22 -Line Documentation (2023).","code":""},{"path":"chap7.html","id":"sec7-5","chapter":"第 7 章 推断（二）","heading":"7.5 总结","text":"术语“协方差分量”统指 GLMM 的 \\(\\symbf G\\) 和 \\(\\symbf R\\) 矩阵中的方差、协方差和相关参数。出于符号目的，它们包含第 6 章和第 7 章中经常提到的向量 \\(\\symbf\\sigma\\)。对协方差分量的推断有三种基本形式：关于 \\(\\symbf\\sigma\\) 的一个或多个分量的正式推断\nGLMM 主要工具：似然比检验\n可能的原因包括\n验证假定，例如方差齐性\n协方差分量本身是内在兴趣\n在竞争的协方差模型中进行选择，例如重复测量分析\n后者22仅在模型嵌套时才可能进行\n\nGLMM 主要工具：似然比检验可能的原因包括\n验证假定，例如方差齐性\n协方差分量本身是内在兴趣\n在竞争的协方差模型中进行选择，例如重复测量分析\n后者22仅在模型嵌套时才可能进行\n验证假定，例如方差齐性协方差分量本身是内在兴趣在竞争的协方差模型中进行选择，例如重复测量分析后者22仅在模型嵌套时才可能进行非正式推断\n使用拟合统计量，例如 AIC, AICC, BIC\n拟合统计量 = 对数似然 - 惩罚\n用于在竞争的协方差结构中进行选择\n当结构没有嵌套时特别有用\n使用拟合统计量，例如 AIC, AICC, BIC拟合统计量 = 对数似然 - 惩罚用于在竞争的协方差结构中进行选择当结构没有嵌套时特别有用区间估计\nWald（简单但不推荐）\n基于似然的——轮廓似然\nWald（简单但不推荐）基于似然的——轮廓似然对协方差分量的推断需要保持模型的固定效应部分 \\(\\symbf{X\\beta}\\) 不变。否则，检验会混淆 \\(\\symbf\\beta\\) 和 \\(\\symbf\\sigma\\)，因此无效。对于高斯数据：使用受限似然似然比检验使用 REML 似然拟合统计量是惩罚 REML 似然区间估计为轮廓 REML 似然对于非高斯数据：推断需要条件模型和积分近似（拉普拉斯或求积）。基于伪似然的协方差推断是未定义且无效的。根据定义，边际模型是一种拟似然模型。在 GLIMMIX 中，这意味着仅通过伪似然进行估计。对于伪似然，拟合统计量未定义。检验需要计算 \\(H_0\\) 和 \\(H_A\\) 下的伪似然。但 \\(H_0\\) 和 \\(H_A\\) 各自隐含着不同的伪变量，因此产生的伪似然是不可比的。同样的问题也适用于使用条件模型的伪似然估计。这意味着刚接触 GLMM 的学生和数据分析师要小心：伪似然是默认的 GLIMMIX 算法——你必须使用 METHOD=LAPLACE 或 METHOD=QUADRATURE 覆盖它，才能对非高斯数据的协方差分量进行推断。","code":""},{"path":"chap7.html","id":"exe7","chapter":"第 7 章 推断（二）","heading":"练习","text":"本习题涉及 SAS Data Program Library 中的文件 ch7_prob1.sas，其中给出了数据以及两个 GLIMMIX 运行。数据来自一项涉及三个因素的研究：、B 和存储时间。因素 和 B 分别是制造工艺和精加工过程，各有两个水平。由每种 × B 组合制造的产品批次被置于存储单元中——每种 × B 组合有六个存储单元。在存储 1 周、2 周、4 周、8 周和 16 周后，从每批产品中抽取样本以检查产品的强度。由于抽样是破坏性的，所以可以假定在不同周数的测量是相互独立的。同时，也可以假定“强度”具有高斯分布。有两个 GLIMMIX 程序，分别称为 “/*Run1*/” 和 “/*Run2*/”.\n给出这两个运行的模型的完整描述。\nRun 1 用于估计方差分量的默认方法的名称是什么？\n就混合模型理论而言，这两次运行的主要区别是什么？\n首选哪种方法（即你应该报告哪种方法）？简单解释一下。\n正式地，根据你在 . 中给出的模型参数，“storage_time**b” 在这些运行中检验的是什么？\n对于该项目研究团队的非统计学家，你会如何解释 “storage_time**b” 检验及其结果？提示：在 “Run 3” 中提供了一个交互图来帮助你回答这个问题。\n给出这两个运行的模型的完整描述。Run 1 用于估计方差分量的默认方法的名称是什么？就混合模型理论而言，这两次运行的主要区别是什么？首选哪种方法（即你应该报告哪种方法）？简单解释一下。正式地，根据你在 . 中给出的模型参数，“storage_time**b” 在这些运行中检验的是什么？对于该项目研究团队的非统计学家，你会如何解释 “storage_time**b” 检验及其结果？提示：在 “Run 3” 中提供了一个交互图来帮助你回答这个问题。本习题涉及文件 ch7_prob2.sas 中给出的数据和两个 GLIMMIX 运行。数据来自不完全区组设计，具有七种处理和七个区组。假定数据是高斯的。\n使用 GLIMMIX COVTEST 选项来检验在给定区组效应的条件下，观测条件分布的方差齐性假设（即处理间方差相等）。\n根据 . 中的结果，为这些数据编写适当的 LMM.\n使用 b.，你对处理均值响应之间的差异得出了什么结论？假定 trt 5 为“对照”或参考处理。举出适当的证据。\n使用 GLIMMIX COVTEST 选项来检验在给定区组效应的条件下，观测条件分布的方差齐性假设（即处理间方差相等）。根据 . 中的结果，为这些数据编写适当的 LMM.使用 b.，你对处理均值响应之间的差异得出了什么结论？假定 trt 5 为“对照”或参考处理。举出适当的证据。本习题涉及文件 ch7_prob3.sas 中给出的数据和数据步语句。数据来自具有三种处理和六个区组的随机完全区组设计。响应变量 \\(Y\\) 是计数。\n运行标准随机区组效应 LMM 分析。记下方差分量估计、处理的 \\(F\\) 检验以及处理均值的估计（和标准误）。\n使用对数计数（一种标准的 GLMM 之前的方差稳定变换）重新运行分析。请注意，“log” 是指自然对数。对于零计数，你可以在获取对数之前向计数添加一个小增量 \\(\\varepsilon > 0\\) ——例如 SAS 语句 log=0 log_y=log(y+0.1)。获取方差分量估计、处理的 \\(F\\) 值以及数据尺度上的估计值（和标准误）或处理均值。\n假设给定区组的计数具有泊松分布，以 GLMM 形式重新运行分析。使用与 . 和 b. 相同的线性预测器。获取方差分量估计、处理的 \\(F\\) 值以及数据尺度上的估计值（和标准误）或处理均值。\n根据 c. 获取 GLMM 的 Pearson 卡方/DF 拟合统计量。\n重新运行 c. 中的 GLMM，但向线性预测器添加随机 block*trt 效应，并获得 \\(H_0\\colon\\sigma_{BT}^2 = 0\\) 的似然比检验。d. 中的统计量与 e. 的检验具有本质上相同的解释。简单地解释一下。\n描述 c. 和 e. 之间的线性预测器的变化对 \\(F\\) 检验和数据尺度上处理均值（和标准误）的估计的影响。\n选择最适合报告的 GLMM（c. 或 e.），并对 GLMM、对数变换 LMM 和未变换 LMM 的方差分量、\\(F\\) 以及数据尺度的估计结果进行比较并对比。\n运行标准随机区组效应 LMM 分析。记下方差分量估计、处理的 \\(F\\) 检验以及处理均值的估计（和标准误）。使用对数计数（一种标准的 GLMM 之前的方差稳定变换）重新运行分析。请注意，“log” 是指自然对数。对于零计数，你可以在获取对数之前向计数添加一个小增量 \\(\\varepsilon > 0\\) ——例如 SAS 语句 log=0 log_y=log(y+0.1)。获取方差分量估计、处理的 \\(F\\) 值以及数据尺度上的估计值（和标准误）或处理均值。假设给定区组的计数具有泊松分布，以 GLMM 形式重新运行分析。使用与 . 和 b. 相同的线性预测器。获取方差分量估计、处理的 \\(F\\) 值以及数据尺度上的估计值（和标准误）或处理均值。根据 c. 获取 GLMM 的 Pearson 卡方/DF 拟合统计量。重新运行 c. 中的 GLMM，但向线性预测器添加随机 block*trt 效应，并获得 \\(H_0\\colon\\sigma_{BT}^2 = 0\\) 的似然比检验。d. 中的统计量与 e. 的检验具有本质上相同的解释。简单地解释一下。描述 c. 和 e. 之间的线性预测器的变化对 \\(F\\) 检验和数据尺度上处理均值（和标准误）的估计的影响。选择最适合报告的 GLMM（c. 或 e.），并对 GLMM、对数变换 LMM 和未变换 LMM 的方差分量、\\(F\\) 以及数据尺度的估计结果进行比较并对比。本习题涉及文件 ch7_prob4.sas 中给出的数据和数据步语句。这是一组重复测量的数据。两种治疗（在数据集中编码为 “0” 和 “1”）被随机分配给每个个体（在数据集中表示为 ID。注意 trt “0” 的 ID=1 与 trt 的 ID=1 不同 “1” ——它们是嵌套的，而不是交叉的）。从 WEEK=0 开始每周观察每个受试者。WEEK=0 时的观测不是基线测量。假定这是在对个体进行治疗之后进行的第一次测量。还假定响应变量是高斯的。\n完成下表以获取指定的合理的协方差模型。假定 week 是一个 CLASS 变量。根据此表中包含的信息回答 b. 和 c.\n\n协方差模型复杂性的增加对治疗与周的交互作用的检验有何影响？Kenward-Roger 调整对此有何影响？\n验证（在这些模型中）ANTE(1) 协方差模型是否提供最佳拟合。为什么？\n对于 ANTE(1) 模型，使用三明治估计（而非模型基于的统计量）确定治疗与周交互作用检验的 \\(F\\) 值和 \\(p\\) 值。使用经典三明治估计和 Morel 校正的三明治估计分别完成。\n将 ANTE(1) 模型的三明治估计结果与 AR(1) 和 UN 模型的三明治估计结果进行比较。相比于协方差模型对基于模型的估计的影响，协方差模型的选择如何影响 \\(F\\) 值和 \\(p\\) 值？\n完成下表以获取指定的合理的协方差模型。假定 week 是一个 CLASS 变量。根据此表中包含的信息回答 b. 和 c.\n协方差模型复杂性的增加对治疗与周的交互作用的检验有何影响？Kenward-Roger 调整对此有何影响？验证（在这些模型中）ANTE(1) 协方差模型是否提供最佳拟合。为什么？对于 ANTE(1) 模型，使用三明治估计（而非模型基于的统计量）确定治疗与周交互作用检验的 \\(F\\) 值和 \\(p\\) 值。使用经典三明治估计和 Morel 校正的三明治估计分别完成。将 ANTE(1) 模型的三明治估计结果与 AR(1) 和 UN 模型的三明治估计结果进行比较。相比于协方差模型对基于模型的估计的影响，协方差模型的选择如何影响 \\(F\\) 值和 \\(p\\) 值？本习题涉及文件 Ch7_prob5.sas. 数据来自二项响应的重复测量研究。有 12 个个体（通过变量 ID 编码）；随机分配 6 名个体至每种治疗。每种治疗在五个等距时间点上观测（两种治疗观测的时间点相同）。SAS 程序显示了研究人员试图确定用于分析的正确协方差结构。总的来说，研究人员首先假定了一个非结构化的工作协方差模型。然后进行了检验：以查看是否存在序列相关的证据 (“DiagR” test)，若存在，那么是否有理由使用非结构化协方差模型，或者是否可以将结构简化为 Toeplitz 模型。研究人员还从 ANTE(1) 开始，经过一系列简化，最终简化为 AR(1). 假定简化后得到了 Toeplitz 和 AR(1) 工作协方差模型，那么根据提供的拟合统计量（例如，\\(-2\\log \\text{PL}\\) 和广义 \\(\\chi^2/DF\\)）比较这两个模型，以选择最佳模型。\n现在暂时不要怀疑研究人员的方法。根据研究者的逻辑，你会“排除”和“纳入”以下哪种协方差结构？\n\n研究人员的方法存在致命缺陷，导致 . 的回答无法使用。该缺陷是什么（你应该能用最多一两句话来回答）。\n作为一般原则，应该如何重新指定模型才能真正合理地进行协方差模型选择？\n根据你在 c. 中的回答重新进行模型选择。在此基础上你选择什么协方差模型？记录你的步骤和支持证据。\n无需详细说明，根据你在 d. 中选择的模型，你对是否存在治疗和/或时间效应会得出什么结论。证据是什么？\n现在暂时不要怀疑研究人员的方法。根据研究者的逻辑，你会“排除”和“纳入”以下哪种协方差结构？\n研究人员的方法存在致命缺陷，导致 . 的回答无法使用。该缺陷是什么（你应该能用最多一两句话来回答）。作为一般原则，应该如何重新指定模型才能真正合理地进行协方差模型选择？根据你在 c. 中的回答重新进行模型选择。在此基础上你选择什么协方差模型？记录你的步骤和支持证据。无需详细说明，根据你在 d. 中选择的模型，你对是否存在治疗和/或时间效应会得出什么结论。证据是什么？","code":""},{"path":"chap8.html","id":"chap8","chapter":"第 8 章 处理和解释变量结构","heading":"第 8 章 处理和解释变量结构","text":"","code":""},{"path":"chap8.html","id":"sec8-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.1 处理结构类型","text":"本章开始本书的第三篇。在本篇前三章中，我们为线性建模奠定了基础，并介绍了“广义”和“混合”给建模带来的主要问题。在接下来的四章中，我们阐述了广义线性建模的理论和方法基础。在本书的其余部分，我们将把注意力转向 GLMMs 的使用以及实际应用中的细节。在本章中，我们重点关注处理类型，或者更一般地说，产生 \\(\\symbf{X\\beta}\\) （线性预测器的固定效应部分）的解释变量结构，以及我们如何根据处理设计有意义地划分 \\(\\symbf{X\\beta}\\)。\\(\\symbf{X\\beta}\\) 的某些分解在数学上是有意义的，但在数据建模的上下文中却是不连贯的。从收集数据研究者的角度来看，其他分解可能看似合理，但在数学上并不可行。建模者的目标是确定一个既满足数学要求又满足数据背景的 \\(\\symbf{X\\beta}\\)。广义上讲，我们可以用两种不同的方式对处理设计和解释变量进行分类：单因素或多因素结构定量或定性因素对于单因素结构，我们需要做出的关键决定是，我们是否有一个定性因子——在这种情况下，线性预测器将类似于 \\(\\eta + \\alpha_i\\)，通常称为 ANOVA 型模型——或者一个定量因子——在这种情况下，我们通常应考虑使用基于回归的线性预测器，例如 \\(\\beta_0 + \\beta_1 X\\)。当我们有超过两个水平的因素时，我们通常想要检验关于处理差异的多个假设。这对我们需要考虑的 类错误率有影响。对于多因素结构，除了决定如何根据每个因素是定量的还是定性的来建模之外，还需要考虑三个额外的问题。第一个涉及将线性预测变量划分为有意义的效应，以捕获每个因素对响应的影响以及其影响的独立性（或缺乏独立性）。第二个涉及拟合因子效应的顺序——我们首先拟合哪个效应，最后拟合哪个效应？这个顺序重要吗？如果是这样怎么办？有“正确”的顺序吗？如果没有，我们如何在后续推断中消除顺序的影响？最后，对于多因素研究，多重假设检验对 类错误率的影响变得更加复杂。在 8.2 节中，我们从经典 ANOVA 和回归分析中的 GLMMs 根源开始考虑检验顺序问题。在 8.2 节中，我们还简要介绍了多重性 (multiplicity)，这是当我们根据单个模型检验多个假设时出现的一种复杂情况。8.3 节概述了由可能的因子组合产生的多因素模型的各种情况。然后我们将更详细地查看每种情况：全部为定性因素（8.4 节），既有定性因素又有定量因素（8.5 节），以及全部为定量因素（8.6 节）。最后的评论：我们在本章中讨论的线性预测器可以与任何线性模型一起使用—— GLMM 或其任何特殊情况：LMM、GLM 或 LM。本章中的讨论都不是特定于 LMM、GLM 或 LM 的。","code":""},{"path":"chap8.html","id":"sec8-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.2 可估函数的类型","text":"在本节中，我们关注两个问题。前三部分重点介绍两种检验策略——部分检验策略和序贯检验策略，也称为 1 型, 2 型和 3 型 (type 1, 2, 3) ——这些检验策略是线性模型，广义的、混合的（无论是单独还是组合使用）从经典的最小二乘理论中继承而来的。最后一节将讨论在同一分析中检验多个假设时控制 类错误的替代方法。","code":""},{"path":"chap8.html","id":"sec8-2-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.2.1 经典 ANOV 缩减平方和的关系","text":"可估函数理论源于经典线性模型的 ANOVA 和回归根源。这些根源在于使用平方和和均方（更正式的术语是二次型）作为检验统计量的组成部分。评估多因素模型的经典方法使用缩减平方和 (reduction sums squares). Searle (1971) 对这种方法进行了全面的阐述，PROC GLM（SAS 的综合普通最小二乘 LM 程序）深受 Searle 的影响。当“广义”和“混合”出现后，例如 PROC GENMOD, PROC MIXED 和最近的 PROC GLIMMIX，虽然平方和不再具有任何有用的含义，但它们保留了缩减平方和的思维过程来定义 1 型, 2 型 和 3 型可估函数。我们需要了解这些是什么才能继续。按照 Searle 的方法，假设我们有一个三因素模型，可以产生线性预测器划分 \\(\\symbf{X\\beta}=\\symbf{X}_A\\symbf{\\beta}_A+\\symbf{X}_B\\symbf{\\beta}_B+\\symbf{X}_C\\symbf{\\beta}_C\\)，即 \\(\\symbf X=\\begin{bmatrix}\\symbf X_A&\\symbf X_B&\\symbf X_C\\end{bmatrix}\\) 以及 \\(\\symbf\\beta'=\\begin{bmatrix}\\symbf \\beta_A&\\symbf \\beta_B&\\symbf \\beta_C\\end{bmatrix}\\)。如果我们拟合全模型，模型的平方和对应于二次型 \\(\\symbf{y}'\\symbf{X}(\\symbf{X}'\\symbf{X})'\\symbf{X}'\\symbf{y}\\)。Searle 用 \\(R\\left(\\symbf{\\beta}_A,\\symbf{\\beta}_B,\\symbf{\\beta}_C\\right)\\) 表示全模型的平方和。回想，这是第 6 章中讨论的 Wald 统计量的关键组成部分。从因素 开始，我们得到 的缩减平方和，\\(R\\left(\\symbf{\\beta}_{}\\right)=\\symbf{y}^{\\prime}\\symbf{X}_{}\\left(\\symbf{X}_{}^{\\prime}\\symbf{X}_{}\\right)^-{\\symbf{X}}_{}^{\\prime}\\symbf{y}\\)。然后拟合 B，我们定义在拟合 之上或在给定 的情况下拟合 B 的缩减平方和：\\[R\\big(\\symbf{\\beta}_B\\mid\\symbf{\\beta}_A\\big)=R\\big(\\symbf{\\beta}_B,\\symbf{\\beta}_A\\big)-R\\big(\\symbf{\\beta}_A\\big)=\\symbf{y}'\\symbf{X}_{AB}\\big(\\symbf{X}'_{AB}\\symbf{X}_{AB}\\big)^{-}\\symbf{X}'_{AB}\\symbf{y}-\\symbf{y}'\\symbf{X}_A\\big(\\symbf{X}'_A\\symbf{X}_A\\big)^{-}\\symbf{X}'_A\\symbf{y}\\]其中 \\(\\symbf{X}_{AB}=\\begin{bmatrix}\\symbf{X}_A&\\symbf{X}_B\\end{bmatrix}\\)。通过一些运算，我们可将 \\(R\\left(\\symbf{\\beta}_B\\mid\\symbf{\\beta}_A\\right)\\) 写为 \\(\\symbf{y}'\\symbf{X}_B\\left[\\symbf{X}'_B\\left(\\symbf{}-\\symbf{X}_A\\left(\\symbf{X}'_A\\symbf{X}_A\\right)^-{\\symbf{X}}_A\\right)X_B\\right]^-{\\symbf{X}}_B'\\symbf{y}\\)。继续该过程，我们最终会得到两种评估与每个子模型相关参数的策略。它们分别为：序贯检验 (sequential tests)：\\[\\begin{align}\n&\\mathrm{}{:}\\;R\\left(\\symbf{\\beta}_A\\right) \\notag\\\\\n&\\mathrm{B}\\mid\\mathrm{}{:}\\;R\\left(\\symbf{\\beta}_B\\mid\\symbf{\\beta}_A\\right) \\tag{8.1}\\\\\n&\\mathrm C\\mid \\mathrm{,B}{:}\\;R\\left(\\symbf{\\beta}_C\\mid\\symbf{\\beta}_A,\\symbf{\\beta}_B\\right)=R\\left(\\symbf{\\beta}_A,\\symbf{\\beta}_B,\\symbf{\\beta}_C\\right)-R\\left(\\symbf{\\beta}_A,\\symbf{\\beta}_B\\right)\\notag\n\\end{align}\\]部分检验 (partial tests)：\\[\\begin{align}\n\\mathrm \\mid\\mathrm B,\\mathrm C{:}\\;R\\left(\\symbf{\\beta}_A|\\symbf{\\beta}_B,\\symbf{\\beta}_C\\right)\\notag\\\\\n\\mathrm B\\mid \\mathrm ,\\mathrm C{:}\\;R\\left(\\symbf{\\beta}_B|\\symbf{\\beta}_A,\\symbf{\\beta}_C\\right) \\tag{8.2}\\\\\n\\mathrm C\\mid \\mathrm , \\mathrm B{:}\\;R\\left(\\symbf{\\beta}_C|\\symbf{\\beta}_A,\\symbf{\\beta}_B\\right)\\notag\n\\end{align}\\]","code":""},{"path":"chap8.html","id":"sec8-2-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.2.2 我们如何知道我们正在检验什么？","text":"通过获取各缩减平方和的期望值，我们可以确定每个检验策略中隐含的可估函数的形式。这些可以总结如下。定义 \\(\\symbf{M}_A=\\symbf{}-\\symbf{X}_A\\left(\\symbf{X}_A^{\\prime}\\symbf{X}_A\\right)^-\\symbf{{X}}_A\\) 以及 \\(\\symbf{M}_B=\\symbf{M}_A\\left[\\symbf{}-\\symbf{X}_B\\left(\\symbf{X}_B^{\\prime}\\symbf{M}_A\\symbf{X}_B\\right)^-\\symbf{X}_B\\right]\\symbf{M}_A\\)。那么序贯策略产生以下隐式可估函数的生成器 (generator)，用于比较因子内的水平\\[\\begin{align}&\\mathrm{}{:}\\;\\left(\\symbf{X}_A^{\\prime}\\symbf{X}_A\\right)^-\\left(\\symbf{X}_A^{\\prime}\\symbf{X}_A\\right)\\symbf{\\beta}_A+\\left(\\symbf{X}_A^{\\prime}\\symbf{X}_A\\right)^-\\left(\\symbf{X}_A^{\\prime}\\symbf{X}_B\\right)\\symbf{\\beta}_B+\\left(\\symbf{X}_A^{\\prime}\\symbf{X}_A\\right)^-\\left(\\symbf{X}_A^{\\prime}\\symbf{X}_C\\right)\\symbf{\\beta}_C\\\\\n&\\mathrm{B}\\mid\\mathrm{}{:}\\;\\left(\\symbf{X}_B^{\\prime}\\symbf{M}_A\\symbf{X}_B\\right)^{-}\\symbf{X}_B^{\\prime}\\symbf{M}_A\\symbf{X}_B\\symbf{\\beta}_B+\\left(\\symbf{X}_B^{\\prime}\\symbf{M}_A\\symbf{X}_B\\right)^{-}\\symbf{X}_B^{\\prime}\\symbf{M}_A\\symbf{X}_C\\symbf{\\beta}_C \\tag{8.3}\\\\\n&\\mathrm C\\mid\\mathrm{,B}{:}\\;\\left(\\symbf{X}_C^{\\prime}\\symbf{M}_B\\symbf{X}_C\\right)^-\\symbf{X}_C^{\\prime}\\symbf{M}_B\\symbf{X}_C\\symbf{\\beta}_C\n\\end{align}\\]通过适当的替换，我们可得到利用部分检验策略、用于检验 和 B 的隐式可估函数，即 \\(\\mid B,C\\) 和 \\(B \\mid ,C\\)。注意，当我们使用序贯方法时，除了完全均衡数据外，效应之间的可估比较将包含一些我们无法消除的 B 和 C 的分量，因子 B 水平之间的可估比较包含我们无法消除的 C 的分量。例如，回想我们在第 6 章 6.4.2 节中讨论的不均衡主效应设计。如果我们使用序贯方法，用于因子 比较的母函数 (generating function) 为 \\(\\eta+\\alpha_1+\\begin{pmatrix}1/3\\end{pmatrix}\\begin{pmatrix}\\beta_1+2\\beta_2\\end{pmatrix}+\\begin{pmatrix}1/3\\end{pmatrix}\\begin{pmatrix}2\\gamma_1+\\gamma_2\\end{pmatrix}\\) 以及 \\(\\eta+\\alpha_2+\\begin{pmatrix}1/3\\end{pmatrix}\\begin{pmatrix}2\\beta_1+\\beta_2\\end{pmatrix}+\\begin{pmatrix}1/3\\end{pmatrix}\\begin{pmatrix}2\\gamma_1+\\gamma_2\\end{pmatrix}\\)。取差值，效应的序贯检验为 \\(\\alpha_1-\\alpha_2+\\left(1/3\\right)\\left(-\\beta_1+\\beta_2\\right)\\)。效应的序贯检验与 B 效应混淆。对于模型的每次重新排序，我们都会重新定义 (8.3) 中的母方程，这就是为什么根据我们拟合效应的顺序以及我们使用的是序贯检验还是部分检验策略，模型效应的检验结果会千差万别。我们可将 (8.3) 中的母函数应用于 GLM, LMM 和 GLMM 以及普通的最小二乘线性模型。它们的定义特征是线性预测器的固定效应分量。所有线性模型的 1 型可估函数都使用 (8.3) 为可估函数 \\(\\symbf{K\\beta}\\) 生成 \\(\\symbf K\\) 矩阵。2 型和 3 型可估函数使用部分策略中隐含的生成器—— \\(C\\mid ,B\\)，\\(\\mid B,C\\) 和 \\(B \\mid ,C\\) 生成器。在许多模型中，2 型和 3 型策略是相同的。而对于具有交互作用的模型，存在差异。我们将在 8.3 节中讨论这些问题。","code":""},{"path":"chap8.html","id":"sec8-2-3","chapter":"第 8 章 处理和解释变量结构","heading":"8.2.3 如何决定要检验什么，而不是让它为我们决定","text":"请注意，只有部分策略能够产生不与其他因子相混淆的因子效应假设。这对于检验不均衡的定性效应数据尤为重要，例如我们刚刚考虑的主效应示例。但你不应该将其解读为我们应该总是使用部分检验。我们在第 6 章的回归示例（Data Set 6.1）中看到，部分检验得出了无意义的结果。请记住建模的主要操作原则：一刀切的方法 = 不良的统计实践。一个更好的一般规则为：如果存在明显的模型拟合顺序（例如，单因子的多项式回归），则使用该顺序；否则，从有意义的可估函数的角度来考虑问题。这才是更好的统计实践。巧了，对于那些缺乏明显模型拟合顺序的情况，当你从有意义的可估函数的角度来考虑问题时，我们通常会得到 3 型检验。但并非总是如此。通过深思熟虑来得出 3 型检验的结果，比不了解其含义就盲目地选择部分检验或序贯检验要好得多。回顾我们在第 6 章中讨论的主效应设计，我们最后总结说应该定义在上下文中合理的可估函数。具体来说，就是要利用边际均值，这些均值是在其他因素的效应估计上平均得到的，并且所得差异不与其它因素混淆。如此做时，我们实质上是在采用一种部分检验的策略。然而，我们通过一个更透明的思维过程来清楚地阐明“我们为什么要这样做？”当我们将其扩展到具有交互项的多因素模型时，这种方法会变得更加复杂。我们将在 8.4 节到 8.6 节中讨论多因素模型的变体。","code":""},{"path":"chap8.html","id":"sec8-2-4","chapter":"第 8 章 处理和解释变量结构","heading":"8.2.4 多重性","text":"对于任何具有两种以上处理的处理设计，我们可以通过两种不同的方式来理解 类错误率。一个称为比较错误率 (comparison-wise error rate)，重点关注每个单独检验发生 类错误的概率。另一个称为实验错误率 (experiment-wise error rate)，重点关注我们进行的整组检验中至少发生一个 类错误的概率。举例说明，假设我们有三种处理，一种是对照或参考处理，另两种是我们希望评估的实验处理。将对照的均值表示为 \\(\\mu_C\\)，两个实验处理的均值表示为 \\(\\mu_A\\) 和 \\(\\mu_B\\)。假设我们检验两实验处理的相等性，\\(H_0\\colon \\mu_A=\\mu_B\\)，如果存在证据表明它们相等，那么我们检验它们的均值与对照的相等性，\\(H_0\\colon\\left(\\mu_A+\\mu_B\\right)/2=\\mu_C\\)。现在，我们根据对犯 类错误严重性的评估来设定 \\(\\alpha\\) 水平（或者，更典型的情况是，我们根据规定的方案来设定）。使用比较错误率，\\(\\alpha\\) 水平定义了每次检验拒绝 \\(H_0\\) 的准则，一次检验一个假设。如果我们这样做，在 \\(H_0\\) 下，不拒绝原假设的概率等于 \\(1−\\alpha\\)。如果我们做两次独立的检验（正如我们在这里所做的——关于独立检验，也就是正交检验，下文有更多介绍），不犯任何 类错误的概率等于 \\((1-\\alpha)^2\\)，因此，至少犯一个 类错误的概率是 \\(1−(1-\\alpha)^2\\)；这就是本例的实验错误率。一般来说，如果我们的处理设计有 \\(t\\) 种处理，我们就有 \\(t−1\\) 个自由度。这意味着我们可以定义多达 \\(t-1\\) 个个相互独立的比较 (comparisons)，或者，用可估函数术语来说，如果我们根据相互独立的处理差异来定义 \\(\\symbf{K'\\beta}\\)，那么 \\(\\operatorname{rank}({\\symbf K})\\le t-1\\)，并且当我们恰好按照每个自由度定义一个比较时，不等式取等号。如果我们这样做，实验错误率将等于 \\(1-(1-\\alpha)^{t-1}\\)。将比较的数量限制在 \\(t-1\\) 并不保证它们之间的相互独立。无论如何，相互独立性本身并不能作为一组对比 (contrasts) 有价值的标准。正如我们将在下文看到的，可能存在更多科学上连贯但并非相互独立的比较。实验错误率的一个更普遍表述为：若将每次比较的错误率记为 \\(\\alpha_C\\)，则对于 \\(k\\) 次比较，实验错误率 \\(\\alpha_{\\small EW}\\ge 1-(1-\\alpha_{\\small C})^k\\)。我们如何知道比较是否独立？在这里，我们引入正交对比 (orthogonal contrasts). 对于单向处理设计，我们有以下定义：对比：若 \\(\\symbf {k'\\beta}\\) 可写为 \\(\\sum_i k_i\\mu_i=0\\) 且 \\(\\sum_i k_i=0\\)，则可估函数是一个对比 (contrast). 对于 GLMs 和 GLMMs，需将 \\(\\mu_i\\) 替换为 \\(\\mu_i^*\\) （第 \\(\\) 种处理的伪变量均值），定义才成立。请注意，在对比的定义中，\\(\\symbf k\\) 必须为向量。对比：若 \\(\\symbf {k'\\beta}\\) 可写为 \\(\\sum_i k_i\\mu_i=0\\) 且 \\(\\sum_i k_i=0\\)，则可估函数是一个对比 (contrast). 对于 GLMs 和 GLMMs，需将 \\(\\mu_i\\) 替换为 \\(\\mu_i^*\\) （第 \\(\\) 种处理的伪变量均值），定义才成立。请注意，在对比的定义中，\\(\\symbf k\\) 必须为向量。正交对比：令 \\(\\symbf{k}'_1\\symbf\\beta\\) 和 \\(\\symbf{k}'_2\\symbf\\beta\\) 为两个对比。它们是正交的当且仅当 \\(\\symbf k'_1 \\symbf k_2=0\\)。正交对比：令 \\(\\symbf{k}'_1\\symbf\\beta\\) 和 \\(\\symbf{k}'_2\\symbf\\beta\\) 为两个对比。它们是正交的当且仅当 \\(\\symbf k'_1 \\symbf k_2=0\\)。\\[\\begin{align}\n记正交对比的定义为式 (8.4)\\tag{8.4}\n\\end{align}\\]在单向处理设计中，一组完全正交对比 (complete set orthogonal contrasts) 必须恰好具有 \\(t − 1\\) 个相互正交的对比，每个对比对应一个处理自由度。这是实验错误率取等号 \\(\\alpha_{\\small EW}= 1-(1-\\alpha_{\\small C})^k\\) 的唯一情形。否则对于 \\(k\\) 个比较，\\(\\alpha_{\\small EW}\\ge 1-(1-\\alpha_{\\small C})^k\\)。举例说明，让我们考虑两种四处理设计。首先，四种处理分别为：假设目标要求检验以下三个假设：\\(H_0\\colon\\left(\\mu_1+\\mu_2\\right)/2=\\left(\\mu_3+\\mu_4\\right)/2\\)，即，对照药与实验药的平均响应相等\\(H_0\\colon\\mu_1=\\mu_2\\)，即，对照药没有剂量效应\\(H_0\\colon\\mu_3=\\mu_4\\)，即，实验药没有剂量效应这是一组完全正交对比的例子。你可以检查：\\(\\symbf{k'}_1=\\begin{pmatrix}1/2\\end{pmatrix}\\begin{bmatrix}1&1&-1&-1\\end{bmatrix}\\)\\(\\symbf{k'}_2=\\begin{bmatrix}1&-1&0&0\\end{bmatrix}\\)\\(\\symbf{k'}_3=\\begin{bmatrix}0&0&1&-1\\end{bmatrix}\\)对于所有 \\(c=1,2,3\\) 有 \\(\\symbf{k}_c^{\\prime}\\symbf{1}=0\\)，那么这三个都是对比对于所有 \\(c\\ne c'=1,2,3\\) 有 \\(\\symbf{k}_c^{\\prime}\\symbf{k}_{c^{\\prime}}=0\\)，那么这些对比相互正交正交对比虽然在数学上很整洁，但也可能不合适。对于许多实验来说，其目标违背了正交对比所施加的限制。以我们的第二个四处理设计为例：这里，一组明显的均值比较为\\[\\begin{aligned}H_{0}{:}\\mu_{{\\mathrm{control}}}&=\\mu_{{\\mathrm{exp~1}}}\\\\H_{0}{:}\\mu_{{\\mathrm{control}}}&=\\mu_{{\\mathrm{exp~2}}}\\\\H_{0}{:}\\mu_{{\\mathrm{control}}}&=\\mu_{{\\mathrm{exp~3}}}\\end{aligned}\\]每个都是对比，这组比较不符合正交标准 (eq:8-4)。即使在上述药物剂量的例子中，我们也可以合理地提、问：“为什么不比较两种药物在低（或高）剂量下的效应呢？”或者“为什么只比较药物在剂量上的平均值，而不是反过来？”在很多情况下，目标是更广泛的发现——我们只是想比较所有的处理对，并观察它们之间是否有任何不同。这种需求的一个极端情况出现在微阵列测试 (micro-array testing) 中，我们会测试成千上万的基因，以查看是否有任何基因产生了效应。这种检验提出了多重性 (multiplicity) 问题。如果我们在实验中只检验一个假设，我们可以将第一类错误率设为 \\(\\alpha\\)。然而，这种情况很少发生。通常，我们有 \\(t > 2\\) 个处理，这意味着如果有意义的话我们可以定义 \\(t −1 >1\\) 个正交对比，如果处理设计允许（例如我们的第二个四处理设计），我们可以定义 \\(t-1\\) 个有意义但非正交的对比，或者，如果我们想比较所有可能的处理对，则总共有 \\(t(t−1)/2\\) 对需要比较。即使采用正交对比，实验错误率也大于比较错误率。如果我们想要进行所有可能的比较，很容易看出实验错误率会急剧增加。在实际数据分析中，问题是我们应该控制哪种类型的错误率：比较错误率还是实验错误率？没有一刀切的答案。这里有几个策略。当然还有其他的。我们列出这些是因为它们可用于 PROC GLIMMIX，我们将在本书的其余部分使用这些程序来实现大多数示例。Protected LSD。LSD 检验23实际上就是第 6 章定义的 \\(t\\) 检验，用于成对的处理均值，对于 GLMM, LMM, GLM 和 LM 有不同的变体。我们使用比较错误率来实施每个检验，但我们在这些检验之前有一个决策规则：如果我们未能以指定的 \\(\\alpha\\) 水平拒绝总体处理的 \\(F\\) 检验，即 \\(H_{0}\\)：所有 \\(\\mu_i\\) 相等，我们就会停止。只有当我们拒绝总体处理假设时，我们才会进行单个配对 \\(t\\) 检验。Protected LSD。LSD 检验23实际上就是第 6 章定义的 \\(t\\) 检验，用于成对的处理均值，对于 GLMM, LMM, GLM 和 LM 有不同的变体。我们使用比较错误率来实施每个检验，但我们在这些检验之前有一个决策规则：如果我们未能以指定的 \\(\\alpha\\) 水平拒绝总体处理的 \\(F\\) 检验，即 \\(H_{0}\\)：所有 \\(\\mu_i\\) 相等，我们就会停止。只有当我们拒绝总体处理假设时，我们才会进行单个配对 \\(t\\) 检验。Bonferroni。我们将比较拒绝标准设为 \\(\\alpha/k\\)，其中 \\(k\\) 是我们计划进行的比较总数。如果我们只有一项比较，未受保护的 LSD 和经过 Bonferroni 调整的拒绝标准是相同的。如果我们有 10 种处理并进行所有 45 对比较，经 Bonferroni 调整的拒绝标准就是 \\(\\alpha/45\\)。对于 \\(\\alpha = 0.05\\)，LSD 的比较错误率是 0.05，而使用 Bonferroni 的比较错误率是 0.0011.Bonferroni。我们将比较拒绝标准设为 \\(\\alpha/k\\)，其中 \\(k\\) 是我们计划进行的比较总数。如果我们只有一项比较，未受保护的 LSD 和经过 Bonferroni 调整的拒绝标准是相同的。如果我们有 10 种处理并进行所有 45 对比较，经 Bonferroni 调整的拒绝标准就是 \\(\\alpha/45\\)。对于 \\(\\alpha = 0.05\\)，LSD 的比较错误率是 0.05，而使用 Bonferroni 的比较错误率是 0.0011.Tukey’s “Honestly Significant Difference”。使用学生化极差。大多数统计方法入门教科书都涵盖了这些内容，例如 Snedecor Cochran (1989) 以及 Steel et al. (1996).Tukey’s “Honestly Significant Difference”。使用学生化极差。大多数统计方法入门教科书都涵盖了这些内容，例如 Snedecor Cochran (1989) 以及 Steel et al. (1996).Scheffé’s Procedure。它使用基于 \\(F\\) 的整体临界值的检验。详情请参见 Westfall et al. (1999).Scheffé’s Procedure。它使用基于 \\(F\\) 的整体临界值的检验。详情请参见 Westfall et al. (1999).Sidak’s Procedure。他本质上从所需的实验错误率开始，并求解隐含的比较标准。当所有比较都相互正交时，这很容易做到。当比较不正交时，情况就更复杂了。详情请参见 Sidak (1967).Sidak’s Procedure。他本质上从所需的实验错误率开始，并求解隐含的比较标准。当所有比较都相互正交时，这很容易做到。当比较不正交时，情况就更复杂了。详情请参见 Sidak (1967).出于特殊目的，还有另外两项调整：Dunnett’s Procedure。这是一种特定的校正，旨在用于“对照”和每种处理之间的一组比较，例如上面的第二个处理设计。许多应用程序，包括 PROC GLIMMIX，都使用 Dunnett (1955) 和 Hsu (1992) 开发的校正。Dunnett 法仅适用于对照组与每个处理组的比较集。Dunnett’s Procedure。这是一种特定的校正，旨在用于“对照”和每种处理之间的一组比较，例如上面的第二个处理设计。许多应用程序，包括 PROC GLIMMIX，都使用 Dunnett (1955) 和 Hsu (1992) 开发的校正。Dunnett 法仅适用于对照组与每个处理组的比较集。Nelson’s Procedure。它将各处理与给定处理所有水平的均值进行检验。不直接对均值进行相互比较。详情见 Nelson (1982, 1991, 1993).Nelson’s Procedure。它将各处理与给定处理所有水平的均值进行检验。不直接对均值进行相互比较。详情见 Nelson (1982, 1991, 1993).","code":""},{"path":"chap8.html","id":"sec8-3","chapter":"第 8 章 处理和解释变量结构","heading":"8.3 多因素模型：概述","text":"我们现在将讨论扩展到多因素处理（或解释变量）设计。这些是涉及两个或多个因素的研究，我们通常将以字母命名，即因素 、因素 B 等。这些设计包括具有交叉分类、嵌套或两者兼有的解释变量结构，特别是当我们有三个或更多因素时。在本节中，我们将回顾基本概念、符号表示和术语，并概述了根据上下文对线性预测器进行有意义划分的方法。这些划分可以归类为全定性、定性-定量和全定量三类。8.4 节、8.5 节和 8.6 节将依次讨论。为讨论析因结构，我们使用以下常用术语。我们按字母顺序用大写字母表示因素：双因素结构的因素 和因素 B，如果添加第三个因素则有因素 C，等等。斜体的小写字母表示因素的水平数：因素 有 \\(\\) 个水平、因素 B 有 \\(b\\) 个水平等。拉丁字母 \\(\\alpha,\\beta,\\gamma\\) 分别指因素 , B, C 对应的模型效应。当我们从 ANOVA 型效应转换为回归型效应时（从 8.5 节开始），我们用回归符号替换 ANOVA 型符号，例如用 \\(\\beta_0 +\\beta_1X\\) 代替 \\(\\eta+\\alpha_i\\)。“多因素” 效应 (“multi-factor” effect) 是指涉及多个因素的任何效应。多因素效应可以是交互作用，例如 \\((\\alpha\\beta)_{ij}\\) 或嵌套效应，例如 \\(\\beta(\\alpha)_{ij}\\)。我们将大部分讨论集中在双因素模型上。在大多数情况下，使用三因素模型涉及对我们用于双因素模型的方法的直接扩展。当扩展不直接时，会有一些讨论。我们使用两种策略（或三种策略，具体取决于我们的观点）来划分多因素模型。尽管存在广泛的误解，但并没有包罗万象的“正确”方法来划分多因素模型中的线性预测器。两种主要策略是交叉分类 (cross-classification) 和嵌套。我们在前几章中已经定义了这两种策略并给出了实例。如果从另一个角度看，还有第三种策略——即根本不进行划分，而是直接使用因素 与因素 B 第 \\(ij\\) 个组合的线性预测器 \\(\\eta_{ij}\\)，并将所有感兴趣的推断定义为可估函数的形式。采用交叉分类策略时，我们将 \\(\\eta_{ij}\\) 分解为主效应和交互效应，例如 \\(\\eta = \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij}\\)。对于嵌套策略，划分的一般形式是 \\(\\eta = \\alpha_i + \\beta(\\alpha)_{ij}\\)。请注意，这意味着 \\(\\beta(\\alpha)_{ij} = \\beta_j + (\\alpha\\beta)_{ij}\\)。当我们想要通过询问两个因素是否独立地影响响应，或者因素之间是否存在不可忽略的交互作用以进行推断时，交叉分类模型更有意义。嵌套模型在以下两种情况下有意义：1) 处理结构不是交叉分类的；2) 处理结构是交叉分类的，但我们知道存在交互作用，并且我们想要关注简单效应。对于后者，我们利用这样一个事实，即我们可以将嵌套效应解释为简单效应——例如，\\(\\beta(\\alpha)_{ij}\\) 是给定 时 B 的简单效应的模型参数化。要点：对于交叉分类和嵌套线性预测器，对主效应 \\(\\alpha_i\\)（以及 \\(\\beta_j\\)，如果存在的话）的推断都取决于多因素效应 \\((\\alpha\\beta)_{ij}\\) 或 \\(\\beta(\\alpha)_{ij}\\) 是否可忽略。如果多因素效应不可忽略——例如如果我们拒绝 \\(H_0\\colon (\\alpha\\beta)_{ij}=0\\) 或 \\(H_0\\colon \\beta(\\alpha)_{ij}=0\\) ——那么对主效应的推断将包括，用 Nelder (1968) 的优雅措辞来说，一组“无趣的假设” (“uninteresting hypotheses”)。更直白地说，在存在不可忽略的多因素效应的情况下，对主效应的推断是无意义的。根据给定因素是定性的还是定量的，以及特定研究的目标，交叉分类和嵌套的划分有几种变体。对于交叉分类和嵌套，根据某些特定背景，不对截距显式建模可能是合理的，如此，交叉分类的线性预测器就成为 \\(\\alpha_i+\\beta_j+\\left(\\alpha\\beta\\right)_{ij}\\)，而嵌套线性预测器则成为 \\(\\alpha_i+\\beta(\\alpha)_{ij}\\)。对于定量因素，进一步的调整是合理的。对于定性因素 和定量因素 B，线性预测器 \\(\\eta+\\alpha_i+\\left(\\beta_1+\\delta_i\\right)X_j\\) 或 \\(\\beta_{0i}+\\beta_{1i}X_j\\) 可能更为合适。注意到这些是 \\(\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\) 和 \\(\\alpha_i+\\beta(\\alpha)_{ij}\\) 的变体，其中涉及 \\(X_j\\)（因素 B 的定量水平）的回归系数取代了 ANOVA 型模型效应。如果两个因素都是定量的，那么我们可能会选择一个完全多元回归模型，例如 \\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_1X_1^2+\\beta_{22}X_2^2+\\beta_{12}X_1X_2\\)，其中 \\(X_1\\) 和 \\(X_2\\) 分别代表因素 和 B 的水平。注意到 \\(\\beta_1X_1+\\beta_{11}X_1^2\\) 取代了 \\(\\alpha_i\\)，\\(\\beta_2X_2+\\beta_{22}X_2^2\\) 取代了 \\(\\beta_j\\)，而 \\({\\beta}_{12}X_1X_2\\) 取代了 \\((\\alpha\\beta)_{ij}\\)。最后，谈谈 2 型和 3 型检验。我们之前看到，1 型检验对应于序贯检验。对于仅主效应模型，2 型和 3 型检验是等价的——都指的是部分检验。但对于析因效应有一个细微的差异。就缩减平方和而言，它们为：2 型检验：\\[\\begin{align}\n&\\mathrm {:}\\;R\\big(\\mid B\\big) \\\\\n&\\mathrm B{:}\\;R\\left(B\\mid \\right) \\\\\n&\\mathrm{*B}{:}\\;R\\left(*B\\mid ,B\\right)\n\\end{align}\\]对于 2 型检验，在拟合其他主效应后检验感兴趣的主效应；交互效应是在所有涉及交互效应的主效应都已被拟合的情况下进行检验的。这是析因 ANOVA 模型中部分平方和的经典定义。3 型检验：\\[\\begin{align}\n&\\mathrm {:}\\;R\\big(\\mid B,*B\\big) \\\\\n&\\mathrm B{:}\\;R\\left(B\\mid ,*B\\right) \\\\\n&\\mathrm{*B}{:}\\;R\\left(*B\\mid ,B\\right)\n\\end{align}\\]当 PROC GLM 推出时，这引起了极大的混乱，更不用说在某些方面引起了极大的恐慌。根据我们之前对仅主效应设计所描述的可估函数，可以更好地理解 3 型检验以及 3 型可估函数的方法。首先，写出调整主效应后边际均值的可估函数——即 SAS 所称的最小二乘均值——也就是说，要对模型中的所有其他效应进行平均，以获得一个理想化的描述：如果数据完美均衡，均值会是多少。假设缺失数据要么是偶然发生的，要么是由于故意设计的不完全设计（如正交主效应设计）造成的，3 型可估函数实质上代表了处理缺失数据的首选方法，这也是它们成为 GLMM 模型默认检验的原因。请注意，对于双因素模型，2 型和 3 型检验仅在主效应方面有所不同，并且仅在数据不均衡时有所不同。你可以使用 8.2 节中讨论的方法来准确地确定 2 型方法检验的内容，并决定它或 3 型检验是否最能满足你的目标。同样，解决此类问题的最佳方式不是问哪种缩减方法最有意义，而是你最终得到了什么可估函数。","code":""},{"path":"chap8.html","id":"sec8-4","chapter":"第 8 章 处理和解释变量结构","heading":"8.4 全定性因素多因素模型","text":"对于全定性因素模型，我们只使用效应模型（回归模型在这里没有意义——为什么？）。如果我们有两个因素，表示为因素 和因素 B，我们可能有一个析因或交叉分类结构，表示为 \\(×B\\)，或一个嵌套结构，记为 \\(B ()\\)。回想第 2 章的结构性差异。如果所有 AB 组合要么有观测、要么本可以有观测时，我们就有了析因设计；而如果在给定因素 的某个水平下观测到因素 B 的某个水平，并排除了在 的其他任何水平下观测到该相同 B 水平的可能性，这时的结构就是嵌套结构。视觉上，下图描述了析因或交叉分类结构而若假定 B1 与 A1 被同时观测到时，B1 与 A2 就不能被同时观测到，则下图描述了嵌套结构例如，在育种试验中，如果因素 是公畜，因素 B 是母畜，那么在特定的繁殖季节，一只母畜只能与一只公畜交配。我们说 B 嵌套在 中，因为给定的 B 水平只能分配给 的一个水平，但反之则不然。当有三个因素时，可能所有三个因素是交叉分类的，我们将其表示为 \\(×B×C\\)，或者一个因素可能嵌套在另一个因素中但与第三个因素交叉，例如 \\(×B(C)\\)，或者我们可能有一个因素嵌套在另外两个因素的因子组合中，例如 \\(C (× B)\\)，或者我们可能有一个严格的嵌套层次结构，例如 \\(C (B( ))\\)。我们可以此方式继续考虑四个或更多因素。超过三个因素时，可能性太多，无法一一列出。这些结构需要关注一些细节，但它们是我们用于两因素和三因素结构的原理的直接扩展。交叉分类结构使用因子效应模型——对于双因素为 \\(\\eta_{ij}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\) ——或此形式的某种变体。在某些设计中，我们可能需要删除交互项。例如，在本章前面讨论的主效应设计（Data Set 6.2）中，设计的构建不允许估计交互效应。嵌套结构使用嵌套效应模型 \\(\\eta_{ij}=\\eta+\\alpha_i+\\beta(\\alpha)_{ij}\\) 或此形式的某种变体。我们将在本节的其余部分中更详细地研究我们如何使用这些模型。我们用高斯数据的双因素 \\(2 ×3\\) 析因结构进行说明。当我们讨论这个示例的过程中，我们将清楚地了解到嵌套模型的使用。此外，虽然该示例使用高斯数据，但所示过程适用于所有线性模型—— GLMs, LMMs, GLMMs 以及 LMs.","code":""},{"path":"chap8.html","id":"sec8-4-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.4.1 选项的回顾","text":"我们的基础构建模块是定义在交互效应、简单效应和主效应上的对比。推断过程是有序进行的。首先，我们确定是否存在不可忽略的交互作用。接下来的步骤取决于第一步的结果。如果交互作用可以忽略不计，则推断集中在主效应上。在没有交互作用的情况下，对简单效应的推断构成了 Nelder 所描述的“无趣的假设”。如果交互作用是不可忽略的，则推断重点转向简单效应。在存在不可忽略交互作用的情况下，对主效应的推断则构成了“无趣的假设”，并且在许多情况下可能导致完全无意义的结论。三因素设计的推断遵循相同路径的扩展。我们从三向交互开始。如果可以忽略不计，我们将进行双向交互。如果它们都可以忽略不计，那么我们才开始研究主效应。对于任何不可忽略的交互作用，我们将注意力集中在该交互作用中包含的简单效应上，选择那些最能针对促使我们进行研究的目标提供信息的简单效应。对于具有多个水平的因素，我们需要使用对比来划分变异，或者如 8.2 节所述，使用成对或多重比较检验。这必须包括对交互作用的划分。为什么？考虑一个双因素模型，其中因素 有 \\(\\) 个水平，因素 B 有 \\(b\\) 个水平。因此，\\(×B\\) 交互作用拥有 \\((-1)(b-1)\\) 个自由度——也就是说，它由 \\((-1)(b-1)\\) 个独立的单自由度对比组成。随着 \\(\\) 和 \\(b\\) 的增加，交互作用的自由度呈乘性增长。如此，即使是非常显著的交互作用也可能集中在一两个或少数几个交互对比中，如果构成整体交互效应的其余对比本质上都接近于零，那么这些重要的交互作用很可能被掩盖。除非我们明确地查看这些对比，否则我们可能会错过一个重要的交互，并导致后续推断走上错误的道路。最后，在多个因素的情况下，多重性问题可以以不同的方式产生。在存在不可忽略的交互作用时，在每组简单效应比较中应用多重性调整可能是合适的。在其他情况下，将多重性调整应用于所有处理组合中可能更为合理。在没有交互作用的情况下，当使用多重性调整时，通常是对每个因素内的主效应进行比较。接下来的两小节说明了这些思想。8.4.2 节介绍了各种推断工具。8.4.3 节讨论了适用于本例的多重性调整。","code":""},{"path":"chap8.html","id":"sec8-4-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.4.2 定性因素推断工具：“SLICE”, “SLICEDIFF” 等工具","text":"本节的工作示例数据显示为 Data Set 8.1 中。处理结构为 \\(2×3\\) 析因。数据是高斯的，设计结构是完全随机的。我们使用标准的析因线性模型：线性预测器：\\(\\eta_{ij}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\)分布：\\(y_{ij}\\sim NI\\left(\\mu_{ij},\\sigma^2\\right)\\)连接：恒等，\\(\\eta_{ij}=\\mu_{ij}\\)我们从总体模型效应的近似 \\(F\\) 检验开始，首先关注 \\(× B\\) 交互作用。使用 GLIMMIX 进行计算，所需的语句为：相关输出为对于 \\(×B\\) 交互作用的检验，\\(p = 0.0099\\)。这使得 和 B 主效应的结果毫无意义。同学们请注意！帮自己一个忙。当有证据表明存在不可忽略的交互作用时，你就已经完成了近似 \\(F\\) 检验了。许多学生似乎无法在家庭作业和考试中约束自己，并继续详细总结主效应结果，即便这些结果是无意义且“无趣的”。为自己节省一些写作上的困扰，也为读者节省一些时间和麻烦：如果交互作用不可忽略——请停止！给定交互的结果，我们需要对其进行描述。这些数据的均值和交互图 (interaction plot) 为：我们将 B 水平的均值与 的每个水平的联系起来，即使因素 B 不是定量的，以可视化交互。对于 B2 似乎没有平均 效应，但对于 B1 有很大的 效应，而对于 B0 有更大的 效应。从不同的角度来看，A1 内 B 的水平似乎没有平均差异，但 A0 内 B 的水平有显著差异。两种观点都是正确的，但通常选择其一来说明研究目标。有两种工具可用于分解与定性因素的交互作用：总体简单效应 \\(F\\) 检验和成对简单效应差异。SAS 线性模型程序分别将其称为 SLICE 和 SLICEDIFF。 SLICEDIFF 仅在 GLIMMIX 中可用。假设我们想要得到给定 时 B 各个水平之间的简单效应统计量。所需的 GLIMMIX 语句为（我们将其包含在上述给出的 MODEL 语句之后）：相关的 SLICE 输出为：SCLICEDIFF 的输出为：正式地，两个 SLICE 检验由原假设 \\(H_0\\colon \\mu_{i0}=\\mu_{i1}=\\mu_{i2}(=0,1)\\)。我们可以使用任意这样的两个对比——这样两个对比放在一起意味着 \\(H_0\\) ——来构造可估函数。例如：\\[\\symbf{T'\\mu}=\\begin{bmatrix}1&0&-1&0&0&0\\\\0&1&-1&0&0&0\\end{bmatrix}\\begin{bmatrix}\\mu_{00}\\\\\\mu_{01}\\\\\\mu_{02}\\\\\\symbf\\mu_{1}\\end{bmatrix}\\]其中 \\(\\symbf\\mu\\) 表示包含所有 \\(\\mu_{ij}\\) 的向量，\\(\\symbf\\mu_1\\) 表示所有包含 \\(\\symbf\\mu_{1j}\\) 的向量。SLICEDIFF 的假设和可估函数由所有可能的成对差值 \\(\\mu_{ij}-\\mu_{ij'}\\) 产生，其中 \\(j\\ne j\\)。输出证实了我们在交互图中看到的内容。实质性的 B 效应出现在 A0 内，但不出现在 A1 内。给定交互作用，关注简单效应的另一种方式是重新将线性预测器表达为嵌套模型。如果我们想关注给定 时 B 的简单效应，我们使用 \\(\\eta_{ij}=\\eta+\\alpha_{}+\\beta(\\alpha)_{ij}\\)（其中 \\(\\) 和 \\(j\\) 是索引）。使用 GLIMMIX，我们通过以下语句来实现这个模型：关注的输出：B() 的总体检验平均了 B|A0 和 B|A1 的简单效应切片。这是一个过度聚合统计量 (overly aggregated statistic) 的例子：通过平均两个感兴趣的检验，得到的统计量没有提供任何我们可以使用的信息。另一方面，请注意，在线性预测器的析因参数化中，对比结果与 SLICE 检验是相同的。这是达到同一目的的两种不同方法。嵌套参数化通常有助于数据分析师更清楚地看到他们正在检验的确切内容。当存在缺失的 AB 组合时，使用嵌套模型可以避免可估计问题。读者练习：如果你使用析因参数化并尝试计算 GLIMMIX 语句定义的对比：会发生什么？为什么会发生这种情况？如果你继续使用析因参数化，那么需要如何修改对比语句以使它们可估？","code":"proc glimmix;\n class a b;\n model y=a b a*b;lsmeans a*b / slice=a slicediff=a;proc glimmix;\n class a b;\n model y=a b(a);\n contrast ‘slice: B | A0’ b(a) 1 0 -1,\n                          b(a) 0 1 -1; \n contrast ‘slice: B | A1’ b(a) 0 0 0 1 0 -1,\n                          b(a) 0 0 0 0 1 -1;\n lsmeans b(a) / slicediff=a;contrast ‘slice: B | A0’ a*b 1 0 -1,\n                         a*b 0 1 -1;\ncontrast ‘slice: B | A1’ a*b 0 0 0 1 0 -1,\n                         a*b 0 0 0 0 1 -1;"},{"path":"chap8.html","id":"sec8-4-3","chapter":"第 8 章 处理和解释变量结构","heading":"8.4.3 多重性调整","text":"我们在上一节中看到，工作示例中的推断针对的是简单效应。如果我们关注给定 的 B 的简单效应，则对于 的每个水平，我们在 B 的水平之间有 3 种可能的成对比较。我们在上一节中看到给定 的 B “SLICEDIFF”结果，使用比较错误率。事实上，它们是经过选择的 LSD 检验。如果我们想控制实验错误率，我们需要决定如何定义它。我们可以将其定义为适用于所有可能的处理组合，但这似乎有点矫枉过正。有 15 种可能的成对比较，但我们只打算进行 6 个，即两组，每组三个成对比较。一种适当但不那么严厉的方法是在每组简单效应检验中应用多重性校正。例如，我们可以使用以下 GLIMMIX 语句结合我们使用的析因参数化来实现 Bonferroni 校正。所得结果：此输出与我们之前看到的唯一区别是最后一列。这些是 Bonferroni 调整的 \\(p\\) 值。在这种情况下，它们不影响 A1 内的简单效应检验——它们本来就不显著——但它确实缓和了我们关于给定 A0 的情况下 B0 和 B1 之间的平均差异的结论。在这种情况下，GLIMMIX 算法将 Bonferroni 调整分别应用于 水平内的每组三个简单效应。使用 LSMESTIMATE 语句定义相同简单效应的另一种方法可与之进行比较：这得到了如下输出：这里，GLIMMIX 算法将多重性调整应用于整个六均值比较集。请注意，这会导致更保守的调整 \\(p\\) 值，例如，A1 条件下 B1 与 B2 的简单效应比较得到的调整 \\(p\\) 值为 0.1196，相比使用 SLICEDIFF 选项获得的 0.0598 更为保守。无论你使用的是 GLIMMIX 还是其他软件，都需要了解应用多重性调整的规则。如果在析因组合中有一个是“对照”或参考处理怎么办？到目前为止，在我们讨论的例子中，因素 和 B 所代表的身份是通用的。我们假设 A0 和 A1 实际上是一种植物的品种或一种动物的品系。假设 A0 是普通品种，容易感染某种疾病或害虫。假设 A1 是新开发的具有抗病性的品种。假设 B1 和 B2 是用于控制害虫或疾病的两种化学物质，而 B0 表示“未处理” (“untreated) ——即未施用任何化学物质。最后，响应 Y 是植物健康状况的一些指标——或者更可能是形成复制单元的区中植物的平均健康状况。现在，想象一下，按照目前的做法，生产者种植品种 A0，并使用 B2 来控制疾病或害虫。然而，越来越多的证据表明使用 B2 可能存在问题——例如，对环境或健康造成危害。假设 B1 据称更安全，尽管其有效性尚未得到证实。那么，这项研究的目标就是确定我们是否可以使用 B1 从易感品种 A0 中获得可接受的性能，或者更好的是，我们是否可以从抗性品种 A1 中获得同等的性能，而无需使用任何化学品（B0）。在这种情况下，完全忽略处理的析因结构是有意义的。相反，我们可以将处理组合 \\(A0× B2\\) 定义为“对照”或参考处理，并实施 Dunnett 检验，即每个处理组合与 \\(A0× B2\\) 进行比较，并使用 Dunnett-Hsu 多重性校正。与析因参数化一起使用的所需 GLIMMIX 语句为所得结果：最后一列显示 Dunnett-Hsu 调整的 \\(p\\) 值。倒数第二列显示未调整的 \\(p\\) 值。在这里，它们的结果基本相同：无论使用哪种化学物质（B 的水平），都没有统计上显著的证据表明抗病品种与对照处理之间存在平均差异，但易感品种（A0）在没有 B2 的情况下表现不佳。","code":"lsmeans a*b / slicediff=a adjust=bonferroni;lsmestimate a*b ‘b1 v b2 | a1’ 1 -1 0 0 0 0,\n                ‘b1 v b3 | a1’ 1 0 -1 0 0 0, \n                ‘b2 v b3 | a1’ 0 1 -1 0 0 0,\n                ‘b1 v b2 | a2’ 0 0 0 1 -1 0,\n                ‘b1 v b3 | a2’ 0 0 0 1 0 -1, \n                ‘b2 v b3 | a2’ 0 0 0 0 1 -1 / adjust=bonferroni;lsmeans a*b / diff=control(‘0’ ‘2’) adjust=dunnett;"},{"path":"chap8.html","id":"sec8-5","chapter":"第 8 章 处理和解释变量结构","heading":"8.5 多因素：部分定性，部分定量","text":"定量因素水平通常允许我们利用回归关系来简化线性预测器。我们在第 1 章中看到了这种方法的简单形式，并在第 3 章中基于 Data Set 3.3 的多批次示例中进行了研究。在本节中，我们将以这些示例为基础。许多有关设计的书籍相当重视使用正交多项式对比来划分定量因素变异。这在前计算机时代是有意义的（正交多项式是在 20 世纪 40 年代开发的），但在一个很容易获得当代建模技术的时代，这是徒劳的。Littell et al. (2002) 和 Stroup et al. (2018) 举例说明了更可取的替代办法。在本节中，我们将定性-定量线性预测器的三种应用结合在一起，这些应用通常以单独和分开的形式呈现。学生和线性建模从业者经常忽略的一点是，这些都是同一主题的变体。","code":""},{"path":"chap8.html","id":"sec8-5-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.5.1 线性预测器的通用形式","text":"假设我们有一个双因素结构，其中因素 是定性的，因素 B 是定量的。这种结构的例子比比皆是。通常，因素 的水平取决于不同的处理（例如对照或实验）或条件（例如暴露或未暴露于环境危害）。因素 B 的水平可以是时间、年龄、剂量、温度、降雨量、实验单元初始条件的某种度量等。通常，响应与 B 的定量水平之间的关系可以通过回归来描述。在这种情况下，我们可以用一个更简洁的模型来捕捉回归关系，从而取代因素 B 的效应模型。如果因素 B 与响应之间的关系是线性的，我们可以使用类似 Data Set 3.3 中多批次示例的模型。我们已经看过线性预测器的两种有用形式：\\[\\begin{align}\n\\eta_{ij}&=\\eta+\\alpha_i+\\left(\\beta_1+\\delta_i\\right)X_j \\tag{8.5}\\\\\n\\eta_{ij}&=\\beta_{0i}+\\left(\\beta_{1i}\\right)X_j \\tag{8.6}\n\\end{align}\\]请注意，(8.5) 是 \\(\\eta_{ij}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\) 的特例，其中 \\({\\beta}_j={\\beta}_1X_j\\) 且 \\(\\left(\\alpha\\beta\\right)_{ij}=\\delta_iX_j\\)，(8.6) 是 \\(\\eta_{ij}=\\alpha_i+\\beta(\\alpha)_{ij}\\) 的特例，其中 \\(\\alpha_i=\\beta_{0i}\\) 且 \\(\\beta(\\alpha)_{ij}=\\beta_{1i}X_j\\)。当因素 B 的定量水平与响应之间的关系比线性回归更复杂时，我们可以简单地用额外的多项式回归项来扩展 (8.5) 或 (8.6)，假设多项式模型合适，或者替换为另一种回归模型——例如样条或非线性模型——如果它们更适合作为拟合模型。我们将在 8.6 节中分别查看每种情况的例子。线性预测器 (8.5) 和 (8.6) 描述了潜在增长曲线模型、协方差模型分析以及定量-定性析因设计分析。虽然这些往往是分开的，但从建模的角度来看，它们只是同一模型的不同解释方式。","code":""},{"path":"chap8.html","id":"sec8-5-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.5.2 通用线性预测器的多种用途","text":"","code":""},{"path":"chap8.html","id":"sec8-5-2-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.5.2.1 潜在增长曲线模型","text":"诸如医学和药物科学、社会学和心理学、教育、工程和农业等各种各样的学科都采用了这种模型的变体。以药物稳定性测试为例，我们想要了解稳定性限制特征如何随时间变化，可能针对不同的药物。让因素 代表不同的药物，因素 B 代表时间，我们显然有一个可以用线性预测器(8.5) 或 (8.6)，或 (8.5) 和 (8.6) 的多项式或非线性扩展对处理结构进行建模。在教育领域，我们可能想要追踪学生的学习进度，并进一步比较不同小组或不同课程下的进度。同样，让小组或课程定义因素 ，年级水平定义因素 B，我们就有了一个可以用 (8.5) 或 (8.6) 建模的结构。潜在增长曲线分析的细节遵循第 3 章基于 Data Set 3.3 的示例，因此这里无需重复。","code":""},{"path":"chap8.html","id":"sec8-5-2-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.5.2.2 协方差分析","text":"我们使用协方差分析——通常称为 ANCOVA 模型——来考虑伴随变量 (concomitant variables). 我们利用它们与响应变量的关系来提高推断的准确性。通常，伴随变量，更常被称为协变量，是我们在研究开始时测量的重复单元的特征。例如初始土壤肥力、能力或成绩测试的预测试得分、临床试验的基线测量等。大多数 ANCOVA 模型都建立在响应与协变量之间线性关系的假设之上。令 \\(y\\) 表示响应，\\(X\\) 表示协变量，我们使用线性预测器 \\(\\eta_j=\\beta_0+\\beta_1X_j\\) 来估计连接 \\(\\eta_j=g\\left({\\mu}_j\\right)\\)，其中 \\(\\mu_j=E\\Big(y_j\\Big)\\)，当我们将 ANCOVA 推广到混合模型时，称为条件均值。如果我们假定 \\(y\\) 的分布是高斯的，那么我们就像在其他例子中一样，使用恒等连接。通过添加处理效应，这一简单线性回归成为 ANCOVA 模型。ANCOVA 区分了发生这种情况的两种方式：1）处理影响截距，但对斜率没有影响； 2) 处理同时影响斜率和截距。前者称为等斜率 ANCOVA (equal slopes ANCOVA) 模型；后者称为不等斜率 ANCOVA (unequal slopes ANCOVA) 模型。请注意，不等斜率 ANCOVA 模型只是 (8.5) 或 (8.6)，具体取决于我们参数化的方式。","code":""},{"path":"chap8.html","id":"sec8-5-2-2-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.5.2.2.1 等斜率示例","text":"本示例使用 Data Set 8.2. 使用完全随机设计观察了四种处理。响应变量是高斯变量。在实验开始时，对研究中的每个重复单元测量了一个协变量（表示为 \\(X\\)）。此时，我们不命名处理、响应和协变量——读者可以随意发挥你的想象力并提供你自己的场景。所有 ANCOVA 程序都首先使用不等斜率模型检验斜率的相等性。正式地，我们从线性预测器 \\(\\eta_{ij}=\\eta+\\alpha_i+\\left(\\beta_1+\\delta_i\\right)X_{ij}\\) 开始，其中 \\(X_{ij}\\) 是在第 \\(ij\\) 个重复单元测量的协变量。我们可以使用以下 GLIMMIX 语句来实现此检验。其中 TRT 标识处理，X 指协变量。模型项 X*TRT 对应模型中的 \\(\\delta_i X_{ij}\\)；相应的 \\(F\\) 值检验 \\(H_0\\colon\\) 所有 \\(\\delta_i = 0\\)。输出关键输出 \\(p\\) 值为 0.2889；我们无法拒绝 \\(H_0\\colon\\) 所有 \\(\\delta_i =0\\)，这意味着我们可以假定斜率相等。我们使用等斜率模型 \\(\\eta_{ij}=\\eta+\\alpha_i+\\beta_1X_{ij}\\) 继续对处理进行推断。所需的 GLIMMIX 语句为我们添加 LSMEANS 语句是因为，在解决了等/不等斜率问题后，我们想要继续推断处理效应。相关输出：我们可以看到 \\(H_0\\colon\\) 所有 \\(alpha_i\\) 都相等的 \\(p<0.0001\\)；我们拒绝在任何合理的 \\(\\alpha\\) 水平上等处理均值的假设。“最小二乘均值”在某种意义上进行了调整，即如果在协变量完全相等的单元上观察到每个处理，则它们预测处理的均值。正式地，\\(\\mathrm{LSMean}_i=\\hat{\\eta}+\\hat{\\alpha}_i+\\hat{\\beta}_1\\bar{X}_{\\cdot\\cdot}\\)，其中 \\(\\bar X_{\\cdot\\cdot}\\) 是协变量的总体均值。如果我们不考虑协变量，会发生什么？换句话说，如果我们对完全随机设计简单地使用通常的线性预测器 \\(\\eta+\\alpha_i\\)，将如何影响处理效应的检验和处理均值估计？以下是从模型中删除协变量的结果：请注意，处理效应的 \\(p\\) 值现在为 0.0518，而不是 <0.0001，这可能会改变有关处理效应的结论。ANCOVA 模型处理均值的标准误在 1.17 到 1.26 之间，现在为 2.83，精度损失很大。均值的相对值变化很大：例如，使用 ANCOVA 模型，处理 1 和 2 的均值分别为 47.7 和 54.6，而在未调整的模型中它们基本上交换了位置。这是什么原因造成的？未调整均值实际上估计 \\(\\text{Unadjusted LSMean}_i=\\hat{\\eta}+\\hat{\\alpha}_i+\\hat{\\beta}_1\\bar{X}_{\\cdot}\\) 其中 \\(\\bar{X}_{\\cdot}\\) 表示仅在接受第 \\(\\) 种处理的单元上观察到的协变量均值。除非随机化导致所有 \\(\\bar{X}_{\\cdot}\\) 完全相等（其概率基本上为零），否则未调整的均值会与协变量混淆，因此处理差异的估计也会混淆。这是观测协变量的图形：我们可以很容易地看到，处理 1 和 4 的协变量均值明显高于处理 2 和 3。估计的协变量回归系数为 \\(\\hat\\beta_1=-2.67\\)，这意味着如果没有随机化导致协变量值的均匀分布，那么第 1 种和第 4 种处理的未调整均值将远低于第 2 种和第 3 种处理。重要的是，我们要强调，我们在上图中看到的协变量并不意味着随机化过程出现了错误。随机化必然发生在收集协变量数据之前。研究人员和统计学家需要了解这一点并保持现实：我们在这里看到的随机化是普遍存在的现象：它往往是常态而不是例外。因此，ANCOVA 调整的价值就体现出来了。","code":"proc glimmix;\n class trt;\n model y=trt x x*trt;proc glimmix;\n class trt;\n model y=trt x;\n lsmeans trt / diff;"},{"path":"chap8.html","id":"sec8-5-2-2-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.5.2.2.2 不等斜率 ANCOVA","text":"让我们考虑一个类似的设定，但数据集不同。该示例作为 Data Set 8.3 出现在附录中。我们从与上一个示例相同的模型和相同的检验开始：\\(\\eta_{ij}=\\eta+\\alpha_i+\\left(\\beta_1+\\delta_i\\right)X_{ij}\\)，检验等斜率假设 \\(H_0\\colon\\) 所有 \\(\\delta_i=0\\)。结果为：回想 x*trt 的 \\(F\\) 值检验 \\(H_0\\colon\\) 所有 \\(\\delta_i=0\\)，\\(p\\) 值为 0.0028，这有力地证明了斜率不相等。对处理的推断必须使用不等斜率模型进行。这意味着，对于任何处理效应的检验，可估函数必须是 \\({\\alpha}_{}-{\\alpha}_{^{\\prime}}+\\left({\\delta}_{}-{\\delta}_{^{\\prime}}\\right)X\\)：没有其他方法可以产生可估函数。所有处理差异的估计和检验都与协变量混淆。这意味着有关处理的任何结论都是特定于协变量的。这也提出了一个问题：有关处理的 \\(F=6.62,p = 0.0069\\) 是在 \\(X\\) 的什么值下计算出来的？两个明显的值是 \\(X = 0\\)（如果 \\(F\\) 仅基于 \\(\\alpha_i\\) 之间的差异计算）和协变量的总体均值 \\(\\bar X_{\\cdot\\cdot}\\)。让我们从 \\(X = 0\\) 开始。检验所有 \\(\\alpha_i\\) 是否相等的对比为该对比的 \\(F\\) 值为 6.62，与上面给出的 \\(F\\) 值相同。另一方面，对于这些数据，\\(\\bar X_{\\cdot\\cdot} = 9 65\\)。在 \\(X\\) 的总体均值上对处理进行检验的可估函数需要以下对比语句得到的 \\(F\\) 值为 7.53，\\(p\\) 值为 0.0043. 我们应该使用哪个？对于这些数据，根据处理绘制的协变量图形为：我们可看到协变量的范围大约在 6 到 13 之间。如果我们在 \\(X=0\\) 处进行检验，我们的检验将在该数据集中从未出现过的协变量值上进行。而在 \\(\\bar X_{\\cdot\\cdot}=9.65\\) 处进行检验看起来要合理得多。线性建模的学生请注意：当你使用任何软件时，你必须训练自己提出此类问题。一些额外信息。这些数据（在盲化数据后）来自对母猪产后应激治疗方法的比较。协变量是窝仔数。产后应激会随着窝仔数的增加而增加，这意味着我们必须调整窝仔数以获得治疗之间的公平比较。斜率的不等性告诉我们，窝仔数增加的影响在治疗方法之间有所不同，或者更具体地说，从动物健康研究人员的角度来看，一些治疗方法在缓解较大窝仔数的负面影响方面比其他治疗方法更有效——这本身就是重要的信息。这也意味着在 \\(X=0\\) 时检验处理的相等性，即检验产后应激药物对“母亲”（窝仔数为 0 的母猪，即还不是真正的母亲）的有效性是毫无意义的。建模不仅仅是数学或统计理论——你还必须关注所建模的主题！我们可以使用参数化 (8.6) 来获得每种治疗的协变量回归的估计。在 GLIMMIX 语言中，(8.6) 为：选项 NOINT 抑制截距 \\(\\eta\\)，使得 TRT 对应于 \\(\\beta_{0i}\\) 且 X(TRT) 对应于 \\(\\beta_{1i}X_{ij}\\)。解为：例如，治疗 1 的回归方程估计为 \\(52.3-0.675X\\)，治疗 2 为 \\(74.3 − 3.75X\\)，依此类推。治疗 1 下产仔数增加的影响似乎比其他治疗的影响要小得多。我们可以比较不同 \\(X\\) 值的治疗。实际上，这意味着与主题专家进行对话，以查明是否有特别感兴趣的 \\(X\\) 值。为了便于说明，我们确定 \\(X = 7\\) 和 \\(X = 12\\)（接近 \\(X\\) 范围的上下限）处的调整均值。获得这些的 GLIIMIX 语句是：所得输出：我们可以看到，在 \\(X=7\\) 时，治疗 1 到治疗 3 之间的差异相对较小，与我们在 \\(X=12\\) 时看到的差异相比。与斜率估计一致，治疗 1 受 \\(X\\) 变化的影响明显小于其他治疗。这里并未展示这些结果，但在实践中，我们也会在感兴趣的 \\(X\\) 值下获得治疗之间的均值比较检验。","code":"contrast ‘trt at x=0’ trt 1 0 0 -1,\n                      trt 0 1 0 -1,\n                      trt 0 0 1 -1;contrast ‘trt at x mean’ trt 1 0 0 -1 x*trt 9.65 0 0 -9.65,\n                         trt 0 1 0 -1 x*trt 0 9.65 0 -9.65,\n                         trt 0 0 1 -1 x*trt 0 0 9.65 -9.65;proc glimmix;\n class trt;\n model y=trt x(trt) / noint solution;lsmeans trt / at x=7;\nlsmeans trt / at x=12;"},{"path":"chap8.html","id":"sec8-5-2-2-3","chapter":"第 8 章 处理和解释变量结构","heading":"8.5.2.2.3 析因处理设计","text":"定性-定量析因设计是线性预测器的第三种主要应用，其形式源自 (8.5) 和 (8.6)。我们用 Data Set 8.4 来说明该方法。这些数据来自 \\(3 ×6\\) 析因处理结构。与本章中的其他示例一样，响应变量具有高斯分布，并且数据是使用完全随机设计收集的。定性因素（）具有标记为 C, E1 和 E2 的三个水平（表示对照和实验处理 1 和 2）。定量因素（B）的六个水平简单地标记为 1 到 6. 根据过去的经验，已知对照处理对 B 的水平的响应呈近似线性关系，斜率为正。研究人员有理由相信，实验处理在 B 的水平上的增长更快（即它们在 B 的较低水平上就能达到目标响应）——他们想查明这种情况是否确实发生，以及两种实验处理中的哪一种看起来更好（在给定的 B 水平上具有更高的平均响应，则“更好”）。这些数据的交互图为：正如预期的那样，实线（因子 的水平 C）近似线性。虚线是 E1 和 E2 在 B 水平上的均值轮廓，看起来稍微呈曲线状。E2 的轮廓似乎始终大于 E1.对于对这些数据有用的线性预测器，它必须能够解释对 B 水平的曲线响应。可能的线性预测器为：\\[\\eta_{ij}=\\eta+\\alpha_i+\\left(\\beta_1+\\delta_i\\right)X_j+\\left(\\beta_2+\\gamma_i\\right)X_j^2+\\beta_j+\\left(\\alpha\\beta\\right)_{ij}\\]其中 \\(\\beta_2\\) 是二次回归系数，\\(\\gamma_i\\) 表示二次回归的第 \\(\\) 个处理效应，\\(\\beta_j\\) 是除了线性和二次回归之外，因子 B 主效应的总和 (catch-)，\\((\\alpha\\beta)_{ij}\\) 是除了 \\(\\delta_{ij}X\\) 和 \\(\\gamma_{ij}\\) 之外，AB 交互效应的总和。你可以将总和项视为二次回归欠拟合的部分。实现该模型的 GLIMMIX 语句为：语句 |X|X 是 SAS 建模中 X *X X*X *X*X 的简写，是 \\(\\alpha_i+\\begin{pmatrix}\\beta_1+\\delta_i\\end{pmatrix}X_j+\\begin{pmatrix}\\beta_2+\\gamma_i\\end{pmatrix}X_j^2\\) 的 SAS 建模语言。而项 |B 则定义了线性预测器的 \\(\\beta_j+\\left(\\alpha\\beta\\right)_{ij}\\) 分量。选项 HTYPE=1 要求进行本章前面定义的“ 1 型”检验，即序贯检验。在这里，它们显然是必要的，因为我们要首先检验截距和 主效应项，然后是线性回归，然后是二次等等。如果你愿意，你可以尝试默认的 3 型检验，你会立即明白为什么它对我们没有用。1 型检验的结果为：我们看到两个欠拟合项 B 和 *B 的 \\(p\\) 值远未达到统计显著性，因此我们可以放心地得出结论，不等系数二次回归模型提供了足够的拟合。X*X*的 \\(F\\) 值检验 \\(H_0\\colon\\) 所有 \\(\\gamma_i = 0\\)，即 B 水平回归的二次分量对于所有三种处理都是相同的。\\(p\\) 值为 0.1264 表明我们缺乏证据来拒绝这一假设。如果我们接受它，我们将从模型中移除 \\(\\gamma_i\\)，从而使用一个具有不等线性回归分量、相等二次分量的模型进行推断。然而，这似乎存在问题，原因有二。首先，这与关于处理在 B 的不同水平上的响应的研究动机不一致。其次，它与我们在交互图中看到的视觉证据相矛盾。可能是 X*X*的两个自由度分解为一个基本上为零的对比和一个相当显著的对比。考虑到交互图和预先研究的理论，一个合理的猜测是 E1 和 E2 的二次分量几乎相同，但它们的平均值与 C 的二次分量相差很大。为评估这一点，我们需要定义分别检验 \\(H_0\\colon\\gamma_{\\small E1}=\\gamma_{\\small E2}\\) 和 \\(H_0\\colon\\gamma_C=\\frac{\\gamma_{\\small E1}+\\gamma_{\\small E2}}{2}\\) 的对比。或者，我们可以按照 (8.6) 的方式重新参数化模型：\\(\\eta_{ij}=\\beta_{0i}+\\beta_{1i}X_j+\\beta_{2i}X_j^2\\) 并检验 \\(H_0\\colon\\beta_{2,E1}=\\beta_{2,E2}\\) 和 \\(H_0\\colon\\beta_{2,C}=\\left(\\frac{\\beta_{2,E1}+\\beta_{2,E2}}{2}\\right)\\)。后者的优点是它还允许我们直接估计二次回归方程，而不是以过度参数化、难以解释的形式获得估计，相应的 GLIMMIX 语句是：术语 X2 是 X*X ——你必须在数据步骤中定义该项。项 和 X() 与我们之前在不等斜率 ANCOVA 模型中看到的含义相同；X2() 对应于 \\(\\beta_{2i}X_j^2\\)。输出：参数估计给出了每种处理的截距、线性和二次回归系数的估计。请注意，对于处理 C 的二次分量检验，\\(p = 0.7114\\)。对于处理 E1 和 E2，\\(p\\) 值均 \\(<0.01\\)。\n这提供了支持最初信念的证据，即处理 C 的响应呈线性增加，而处理 E1 和 E2 的响应呈曲线增加。对比结果提供了这样的证据：E1 和 E2 的二次系数彼此相似，但与 C 不同。推断将沿着与不等斜率 ANCOVA 模型中的处理比较相同的线路继续进行。我们可能想要通过回归估计来确定在 B 的哪个水平上，E2 的平均响应大于等于 C 的平均响应。我们也想知道 E1 和 E2 是否有差异，如果有，差异有多大。我们首先通过比较它们的线性分量来做到这一点，如果它们没有显著差异，我们再比较它们的截距。如果回归分量没有差异，那么回归线是平行的；如果再加上截距不同，那么这些线不仅平行，并且一种处理在所有 B 的观测水平上始终高于另一种处理。","code":"proc glimmix data=a;\n class a b;\n model y=a|x|x a|b/htype=1;proc glimmix data=a;\n class a b;\n model y=a x(a) x2(a)/noint solution;\n contrast ‘quad c vs quad e1+e2’ x2(a) 2 -1 -1;\n contrast ‘quad e1 v e2’ x2(a) 0 1 -1;"},{"path":"chap8.html","id":"sec8-6","chapter":"第 8 章 处理和解释变量结构","heading":"8.6 多因素：全定量因素","text":"在所有处理或解释因素都是定量的结构中，我们扩展了上一节所使用的策略。假定我们能够确定解释变量和响应变量之间的函数关系，我们就可以根据多元回归来定义线性预测器。在许多情况下，二阶多项式回归就足够了。二阶多项式包括最高到主效应的二次项和仅包含线性-线性的双向交互项。这是经典的响应曲面模型 (response surface model)，由 Myers, Montgomery Anderson-Cook (2009)，Box, Hunter Hunter (2005)，Khuri Cornell (1996)，以及 Box Draper (1987) 进行了深入的描述。通常，多项式方法是不够的，我们必须使用其他方法。重要的替代方法包括样条函数和非线性均值模型。有完整的课程专门介绍响应曲面方法，还有其他课程完全致力于非线性模型。这不是我们此处的目的。在本节中，我们通过两个因素的例子来介绍这些基本方法。在 8.6.1 节中，我们考虑二阶多项式。在 8.6.2 节中，我们考虑其他多元回归模型。同样，虽然所有的例子都使用了高斯数据，但我们的重点是线性预测器。我们讨论的每一个线性预测器都可以适应具有“广义”和“混合”特征或两者兼有的模型。","code":""},{"path":"chap8.html","id":"sec8-6-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.6.1 二阶多项式，又称经典响应面线性预测器","text":"本示例的数据显示在 Data Set 8.5 中。我们在析因安排中因素 和 B，各有五个水平，总共 25 个处理组合。每个处理有来自完全随机设计的三个观测。以下显示了 25 种处理组合均值的三维图形。此图显示了与二阶多项式回归相关的特征形状——具有视觉上可识别的最大或最小响应的穹顶或倒穹顶形状。二次主效应回归解释了曲线型分布，线性×线性项解释了倾斜度。通常，估计使响应最大化或最小化的因素组合是数据分析的主要目标。在本例中，假定响应值越低越好，我们发现在 轴和 B 轴上，在 3 附近似乎都存在一个最小值。二阶多项式线性预测器使我们能够通过普通微积分确定使响应最小化（或最大化）的处理组合，假设模型提供了适当的拟合，这是一个明显的优势。当我们将二次定性定量模型拟合到 Data Set 8.3 时，我们开始使用与上一节中看到的相同策略。在这里，我们从线性预测器开始：\\[\\begin{align}\n\\eta_{ij}=\\beta_0+\\beta_{1A}X_A+\\beta_{1B}X_B+\\beta_{2A}X_A^2+\\beta_{2B}X_B^2+\\beta_{12}X_AX_B+(\\alpha\\beta)_{ij}\n\\tag{8.7}\n\\end{align}\\]其中 \\(X_k; k = ,B\\) 表示因素 和 B 的定量水平，\\(\\beta_{1,k}\\) 表示线性回归系数，\\(\\beta_{2,k}\\) 表示二次回归系数，\\(\\beta_{12}\\) 表示线性交互系数，\\((\\alpha\\beta)_{ij}\\) 作为所有高阶主效应（此时为三次和四次）和除线性相互作用项以外的所有相互作用项的总和项。我们将 \\((\\alpha\\beta)_{ij}\\) 解释为欠拟合 (lack fit) 项。虽然这里没有显示，但与上一节中的定性定量示例一样，我们可以划分 \\((\\alpha\\beta)_{ij}\\)。(8.7) 所需的 GLIMMIX 语句为语法 xa|xa|xb|xb@2 是 xa xb xa*xa xb*xb xa*xb 的 SAS 建模简写，其中 XA 和 XB 分别表示与因素 和 B 水平相对应的量。选项 @2 将可能的 XA 与 XB 的组合限制为二阶项——否则 SAS 会将命令解释为包括三阶和四阶项，例如二次-二次交互作用 xa*xa*xb*xb。模型中的 CLASS B 和 *b 定义了欠拟合项。与上一节中的定性定量示例一样，我们希望在这里使用 1 型或序贯拟合检验——使用默认的 3 型方法会产生无意义的结果。为什么在这个例子中，3 型检验会给出无意义的结果？3 型检验，或称为部分检验，根据定义，是在拟合了其他所有效应之后检验每个效应。对于回归效应，这意味着首先拟合所有 ×B 效应 \\((\\alpha\\beta)_{ij}\\)，然后再拟合回归效应。但如果你首先拟合了所有 ×B 效应，那么就没有剩余的要拟合的内容了！这与首先拟合所有回归效应，然后再拟合 \\((\\alpha\\beta)_{ij}\\) 的做法非常不同。对于后者，\\((\\alpha\\beta)_{ij}\\) 拟合的是拟合回归方程后剩余的部分。1 型检验结果为：这里最重要的结果是与欠拟合项 *B 相关的 \\(F\\) 值和 \\(p\\) 值；\\(p=0.67\\) 告诉我们，二阶多项式回归没有欠拟合的证据。然后我们从线性预测器中删除 \\((\\alpha\\beta)_{ij}\\) 以估计二阶回归方程，等等。这些留作练习。","code":"proc glimmix;\n class a b;\n model response=xa|xa|xb|xb@2 a*b/htype=1;"},{"path":"chap8.html","id":"sec8-6-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.6.2 其他定量-定量模型","text":"在许多情况下，二阶多项式不能提供足够的拟合。此外，即使我们最终能够确定一个在数学上提供足够拟合的高阶多项式，这样的模型通常也几乎无法提供与研究目标相关的有用见解。上例中所示的一般策略仍然适用：如果可能，我们希望使用某种形式的回归，而不是基于效应的线性预测器。其他回归函数包括正弦和余弦函数、非线性函数和样条函数。当因子水平有规律地、周期性变化时，正弦和余弦函数非常有用，例如在每日、每月、季节和其他时间序列中。本节给出了两个例子；一个涉及非线性形式的响应曲面建模，另一个涉及样条函数。寻求时间序列入门的读者可以参考 Box Jenkins (2008)；非线性模型可以参考 Watts Bates (1988)、Gallant (1987) 或 Davidian et al. (1995)；样条回归参见 Rupert、Carroll Wand (2003) 和 Gu (2002).","code":""},{"path":"chap8.html","id":"sec8-6-2-1","chapter":"第 8 章 处理和解释变量结构","heading":"8.6.2.1 非线性均值模型","text":"从 GLMM 的角度来看，这些模型是线性的，因为我们通过固定效应和随机效应部分的加性函数对链接进行建模。然而，我们有 \\(\\eta=f(\\symbf{X},\\symbf\\beta)+\\symbf{Zb}\\)，而不是线性预测器 \\(\\eta=\\symbf{X\\beta}+\\symbf{Zb}\\)，其中 \\(f(\\symbf{X},\\symbf\\beta)\\) 不一定是线性的。例如，Data Set 8.6 包含来自一项研究的数据，该研究的设计结构与 Data Set 8.5 类似，但这两个因素水平上的响应轮廓似乎并不适合多项式回归。这些是模拟数据，但响应轮廓受到了 Landes et al. (1999), Paparozzi et al. (2005), Stroup et al. (2006) 和 Frenzel et al. (2010) 讨论的非线性植物营养剂量响应轮廓的驱动。散点图（左）显示了实际数据点；纵轴是响应；下方的两个轴表示因素 和 B 的水平。右图显示了连接这些点的线，从而更容易看到数据的响应轮廓。右下角对应于 和 B 的最低水平。沿着 和 B 轴，我们看到响应突然上升，随后是一个较长的平稳期。如果我们拟合一个二阶多项式，我们得到一个响应面估计，如下图所示。二次回归无法捕捉到突然上升和随后的长期平稳期。相反，它模糊了这两个特征，并产生了对曲面的不良近似。显然，我们预计使用二阶近似会导致不准确的结论和不恰当的建议。Olson et al. (2001) 提出了一种使用 Hoerl 函数线性化形式的替代二阶响应面模型。在 GLMM 术语中，线性预测器为：\\[\\begin{align}\n\\eta_{ij}=\\beta_{0}+\\beta_{1,}X_{}+\\beta_{1,B}X_{B}+\\beta_{11}X_{}X_{B}+\\beta_{2,}L_{}+\\beta_{2,B}L_{B}+\\beta_{22}L_{}L_{B}\n\\tag{8.8}\n\\end{align}\\]其中 \\(L_k=\\log\\bigl(X_k\\bigr);k=,B\\)。在 (8.8) 中，我们通过对数而不是二次项对响应面的曲线分量进行建模，并使用对数和线性项的乘积对交互作用进行建模。所得的响应面估计为拟合有明显的改进。Guo et al. (2006) 表明，这种方法允许我们使用有效的不完全析因设计，特别适合在资源高度受限时估计 Nelson Anderson (1975) 所说的“线性平台”响应轮廓。我们也可以使用显式非线性 \\(\\eta_{ij}\\)。同样来自 Guo 等人的一个例子是 Gompertz 函数的二元扩展：\\[\\begin{align}\n\\eta_{ij}=\\alpha+\\exp\\left\\{-\\beta\\times\\exp\\left[-\\gamma_1X_A-\\gamma_2X_B-\\gamma_{12}X_AX_B\\right]\\right\\}\n\\tag{8.9}\n\\end{align}\\]Gompertz 得到了如下的响应面估计：线性化的 Hoerl 和 Gompertz 函数为这些数据产生了相似的拟合。Hoerl 方法的主要优势在于易于识别合适的有效设计和能够使用线性模型软件进行估计。Gompertz 等非线性模型的优势在于建模非标准响应轮廓的灵活性更大，以及非线性模型的参数本身更易于进行有意义的解释。例如，在 Gompertz 模型中，\\(\\alpha,\\beta\\) 和 \\(\\gamma\\) 分别定义了截距、响应的渐近线或平台值以及因素 和 B 水平上的增加率。","code":""},{"path":"chap8.html","id":"sec8-6-2-2","chapter":"第 8 章 处理和解释变量结构","heading":"8.6.2.2 样条或分段回归","text":"一些响应轮廓无法通过单一函数形式来表征。分段或样条 (segmented spline) 回归将响应轮廓划分为可修改为更简单的回归模型的部分。第 ?? 章详细介绍了平滑样条，它为我们提供了一种处理不规则回归曲线的有用方法。这种方法特别适合 GLMM. Ruppert et al. (2003) 给出线性样条函数为 \\(\\beta_0+\\beta_1X_i+\\sum_{j=1}^K\\gamma_jI\\Big(X_i>t_j\\Big)\\Big(X_i-t_j\\Big)\\)，其中 \\((X_i>t_j)\\) 为定义在 \\(X_i>t_j\\) 上的指示函数。平滑样条程序通过最小化下式获得参数估计\\[\\begin{align}\n\\left(\\symbf{y}^{*}{-}\\symbf{X}\\symbf{\\beta}{-}\\symbf{Z\\gamma}\\right)^{\\prime}\\left(\\symbf{y}^{*}{-}\\symbf{X}\\symbf{\\beta}{-}\\symbf{Z\\gamma}\\right)+\\lambda^{2}\\symbf\\gamma^{\\prime}\\symbf\\gamma\n\\tag{8.10}\n\\end{align}\\]其中 \\(\\symbf y^*\\) 对应于 PL 估计中的伪变量，\\(\\symbf{X\\beta}\\) 表示样条模型的 \\(\\beta_0+\\beta_1 X\\) 分量，\\(\\symbf{Z\\gamma}\\) 表示其余部分，\\(\\lambda\\) 是一个调整常数 (tuning constant). 如果我们将线性样条视为伪模型 (pseudo-model) \\(\\symbf{y}^*=\\symbf{X}\\symbf{\\beta}+\\symbf{Z}\\gamma+\\symbf{e}\\) 的特例，其中 \\(Var\\left(\\symbf{e}\\right)=\\phi\\symbf{}\\) 且 \\(Var\\left(\\gamma\\right)=\\sigma^2\\symbf{}\\)，Schabenberger (2008) 表明调整常数 \\(\\lambda=\\phi/\\sigma\\)，并且可以使用 PL 估计算法来估计线性样条。作为示例，Data Set 8.7 包含了在 X 轴上的多个点进行观测的两种处理的数据。在用于估计模型的 GLIMMIX 语句之后，下面展示了数据的图形以及平滑样条回归的估计。这些 GLIMMIX 语句为：因素 为定性处理因子；X 是定量因素。EFFECT 语句定义 X 上的样条，并使用 RANGEFRACTION 选项定义回归分段的 \\(t_j\\) 值。对于 RANGEFRACTION，这些点是根据 X 值范围的小数形式的百分比来定义的。在实践中，在选择范围分数时需要进行一些试验和错误，以在简约性和响应曲线的充分表征之间取得平衡。在这里，我们展示了检验模型效应的结果以及样条回归关于观测数据的拟合图。从图中我们可以看到，随着 X 水平的变化，响应似乎在 X 轴上呈间断性的激增，并且这些激增的位置和幅度在因素 的两个水平之间有所不同。","code":"proc glimmix data=spline;\n class a;\n effect spline_x=spline(x /knotmethod=rangefractions \n    (0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9));\n model y=a|spline_x / /*noint*/ solution;\n output out=gmxout pred=p;\nrun;\nproc sgplot data=gmxout;\n series y=p x=x / group=a name=“fit”;\n scatter y=y x=x / group=a;\n keylegend “fit” / title=”A”;\nrun;"},{"path":"chap8.html","id":"sec5-7","chapter":"第 8 章 处理和解释变量结构","heading":"8.7 总结","text":"这就结束了我们对处理和解释变量结构的线性预测器和推断策略的初步考虑。你从本章中应该掌握的主要思想是：线性预测器的 \\(\\symbf{X\\beta}\\) 分量的形式需要反映解释变量或处理设计中的因素类型以及研究目标。精心选择的 \\(\\symbf{X\\beta}\\) 可以实现清晰的分析。选择不当的 \\(\\symbf{X\\beta}\\) 往往会带来混乱而不是启发。线性预测器的 \\(\\symbf{X\\beta}\\) 分量的形式需要反映解释变量或处理设计中的因素类型以及研究目标。精心选择的 \\(\\symbf{X\\beta}\\) 可以实现清晰的分析。选择不当的 \\(\\symbf{X\\beta}\\) 往往会带来混乱而不是启发。避免一刀切的规则。食谱（走得太远）是不好的——深思熟虑更难，但效果会好很多。当你发现食谱开始阻碍你的思考，或者当你不再思考而只是“套用公式”时，你就知道你已经走得太远了。避免一刀切的规则。食谱（走得太远）是不好的——深思熟虑更难，但效果会好很多。当你发现食谱开始阻碍你的思考，或者当你不再思考而只是“套用公式”时，你就知道你已经走得太远了。检验顺序会对你检验的内容会产生巨大影响。对于不均衡的数据，你实际检验的内容可能与你认为正在检验的内容大相径庭。检验顺序会对你检验的内容会产生巨大影响。对于不均衡的数据，你实际检验的内容可能与你认为正在检验的内容大相径庭。确保你知道你正在检验什么。如果你无法以可估函数形式表达你正在检验的内容，那就停下来，直到你能表达为止。确保你知道你正在检验什么。如果你无法以可估函数形式表达你正在检验的内容，那就停下来，直到你能表达为止。多因素模型提出了有关 \\(\\symbf{X\\beta}\\) 的最具挑战性的建模问题。多因素模型提出了有关 \\(\\symbf{X\\beta}\\) 的最具挑战性的建模问题。共同主题有：\n首先检验交互作用。\n不可忽略的交互作用要求仅对简单效应进行推断。\n交互作用的缺失要求仅对主效应进行推断。\n当多重性问题出现时，需要予以处理。\n有时，研究的具体背景会凌驾于上述所有内容之上。\n共同主题有：首先检验交互作用。不可忽略的交互作用要求仅对简单效应进行推断。交互作用的缺失要求仅对主效应进行推断。当多重性问题出现时，需要予以处理。有时，研究的具体背景会凌驾于上述所有内容之上。出发点：\n一般来说，定性效应需要效应模型。\n只要有可能，尝试用回归模型来表达定量因素的效应。\n回归不仅仅意味着多项式回归。\n出发点：一般来说，定性效应需要效应模型。只要有可能，尝试用回归模型来表达定量因素的效应。回归不仅仅意味着多项式回归。在本书的其余部分中，我们将再次讨论本章中的大多数模型形式。请参考本章以了解线性预测器中处理设计部分的基本模板和动机。其余章节主要关注线性预测器中随机部分 \\(\\symbf{Zb}\\) 的各种形式、\\(\\symbf R\\)（对于 LMMs）或工作相关结构（对于 GLMMs 的边际推断）、观测的分布以及连接的各种形式。","code":""},{"path":"chap9.html","id":"chap9","chapter":"第 9 章 多水平模型","heading":"第 9 章 多水平模型","text":"","code":""},{"path":"chap9.html","id":"sec9-1","chapter":"第 9 章 多水平模型","heading":"9.1 设计结构类型：单水平和多水平模型定义","text":"在第 8 章中，我们研究了构建和使用处理（或更一般地说，解释变量结构）的主要策略。在本章中，我们将注意力转向设计，或者 Fisher (1935) 所说的线性预测器的“地形”方面。简而言之，处理方面主要决定了我们如何指定 \\(\\symbf{X\\beta}\\)，而设计或地形方面则倾向于推动与使用 \\(\\symbf{Zb}\\) 相关的指定和策略。正如我们在第 2 章中所看到的，广义上定义的设计或地形方面涉及重复单元 (units replication) 和观察的单元 (units observation)24 及其结构。这些单元是什么？我们有单元是更大单元的子集或细分吗？我们有没有有意义的单元分组——层、集群或区组 (strata, clusters blocks)？我们如何对这些单元进行随机化？本章我们主要关注的是正确识别重复单元和随后的 \\(\\symbf{Zb}\\) 的指定。我们的重复单元是否只有一种尺寸 (size)，还是有不同尺寸的重复单元？如果是后者，那么我们就有了一个多水平研究 (multi-level study). 多水平研究的名称因所在领域而异。从历史上看，农业统计学家是最先认识和命名多水平研究的——他们称之为裂区实验 (split-plot experiments). 在医疗中心，生物统计学家称它们为嵌套析因/集群设计 (nested factorial clustered designs). 社会科学家则称之为分层设计 (hierarchical designs). 无论不同领域的术语如何，它们指的都是同一设计结构。模型中表征设计的组成部分有时被称为干扰参数 (nuisance parameters). 这既不适当也不利于理解，因为“讨厌”一词常被解读为“不重要”。设计部分可能很复杂，我们如何定义线性预测器来表征它们对后续推断有重大影响。在建模练习中，当事情出现严重错误时，往往是因为在设计方面出现了问题。当存在多水平结构而未能考虑或考虑不当时，是导致事情变糟的最常见原因。关于设计或地形结构的许多术语都来源于实验设计。但这并不意味着本章的讨论仅适用于设计实验。调查、观察性研究和准实验都是有结构的。它们都有观察的单元。它们都有与重复单元至少是不严格对应的实体。聚类、分层和病例对照匹配都是区组的形式。为了构建一个有用且信息丰富的模型，所有这些都需要被适当地理解和描述。在某些方面，本章对于非正式设计的实验或随机试验的研究可能更重要，原因在于参与非实验性研究的人有时可能忽视设计原则的应用。因此，为避免任何误解，每当我们进行比较性的推断时，设计原则都适用。本章使用设计术语是因为它是多水平建模的通用语言，但读者需要理解本章中的语言是广义的。综上所述，让我们为多水平研究是什么以及不是什么建立一个精确的工作定义。我们将一项研究定义为多水平的，当且仅当它具有至少两个随机变异源，以及构成研究架构的至少两种不同尺寸的单元。或者，我们也可以根据不同的随机化方案数量来定义多水平研究。关键在于：不止一种尺寸的单元，或等价地，多个随机化方案。如果我们只有一种尺寸的单元，意味着重复单元和观察的单元是相同的，或等价地，所有处理水平或因素水平组合都使用单一的随机化方案，那么我们就不是在进行多水平研究。一种尺寸的单元：单水平。两种或更多种尺寸的单元：多水平。单一的随机化方案：单水平。两步或更多步的随机化：多水平研究。最简单的多水平研究是具有多个抽样单元的完全随机设计。在这类研究中，重复单元由几个个体组成，如围栏中的动物、诊所中的患者、区 (plot) 中的植物。本章重点关注更复杂的多水平研究，因为这些研究通常会给建模师和数据分析师带来最大的困难。在 9.2 节中，我们将更详细地了解多水平研究是如何产生的。我们还回顾了第 2 章中介绍的框架 ANOVA（“Fisher会做什么？”或 WWFD ?）过程，这是一种帮助我们可视化单水平和多水平研究之间区别的方法，也是一种帮助人们正确指定多水平设计组成部分的方法。如果不考虑分区 (blocking)，我们就无法讨论多水平研究。在多水平研究中使用分区时，会出现单水平设计中不会出现的问题。对于单级设计，决定是将区组 (blocks) 效应视为固定效应还是随机效应通常对模型估计或推断的影响最多只是轻微的。但多水平研究则不然。具体来说，将区组效应定义为固定效应可能会让你陷入无法轻易逃脱的困境。主要的两个问题是：错误的推断统计量和虚假的不可估性。9.3 节将重点讨论分区。在 9.4 节中，我们考虑一些示例，说明多因素处理结构和多水平设计结构如何组合在一起形成整个线性预测器。根据定义，多因素模型必须是混合模型。本章到 9.4 节的内容同样适用于非高斯 GLMMs 和 LMMs.在前面的章节中，特别是第 3、5 和 6 章，我们讨论了边际模型和条件模型、G 侧和 R 侧协方差建模以及 GEE 型模型。在 9.5 节中，我们将研究这些问题如何具体应用于多水平模型。","code":""},{"path":"chap9.html","id":"sec9-2","chapter":"第 9 章 多水平模型","heading":"9.2 多水平模型的类型及其产生方式","text":"重复单元和随机化方案提供了我们构建 \\(\\symbf{Zb}\\) 所需的关键信息。在本节中，我们回顾第 2 章中的一些关键概念以及它们如何在多水平场景中体现。","code":""},{"path":"chap9.html","id":"sec9-2-1","chapter":"第 9 章 多水平模型","heading":"9.2.1 重复单元——不仅在设计的实验中","text":"对于设计的实验，重复单元（设计语言中的实验单元）从规划阶段到数据分析阶段都占据着重要地位。它正式定义为独立分配处理的最小实体。“独立”这个词至关重要。如果我们将动物圈养，并通过一个公共的围栏喂食器分配并应用处理，则实验单元就是围栏。动物版对着分配；我们可以单独测量它们，但它们到处理水平的分配不是独立的。我们不能在围栏内将一只动物分配给一个处理，将另一只动物分配给另一个不同的处理。实验单元思维方式深深地嵌入在实验设计的思考过程中。这大大有助于从实验设计中构造线性预测器。我们已经有了所有必要的信息。对于调查、观察性研究和准实验，我们必须更加努力，但我们仍然需要识别这些要素，以便构建能够产生准确分析的线性预测器。每项调查、观察性研究等都具有类似于重复单元的概念。让我们看一个随机试验的例子，它几乎是但又不完全是一个设计的实验，来加深我们对术语的理解，然后考虑这些思想如何转化为调查和观察性研究。为此，我们使用 Fisher 的术语“地形”，就像我们在第 2 章中所做的那样。“地形”一词很有帮助，因为它不受可能妨碍讨论的设计或勘测术语的影响，但我们需要将“地形”理解为广义上的研究结构，而不是狭义上的地理或地质概念。最简单的地形结构有一种单元。我们对这些单元进行观察，并将处理水平分配给这些单元。这种结构定义了一种完全随机设计 (completely randomized design, CRD)。CRD 显然是一个单水平结构。就调查而言，简单的随机样本与 CRD 具有相同的结构，因此需要考虑类似的建模因素。CRD 是唯一仅具有一种单元尺寸的设计，但 CRD 并不是唯一的单水平设计。如果我们有两种尺寸的单元，比如教室和教室内的学生，并且教室接受处理分配（例如课程 A1 分配到一个教室，课程 A2 分配到另一个教室等），但我们对单个学生进行测试以查看处理的效果，那么我们就有了一个带有子抽样 (sub-sampling) 的 CRD，这是最简单的多水平结构。假设教室被分区——学校是一个明显的分区标准。现在我们的研究中有三种尺寸的单元。如果我们仍然在教室水平上接受处理（比如在任何给定的学校，一间教室接受课程 A1，另一间教室接受课程 A2），那么我们就会采用随机区组设计——如果我们仍然对单个学生进行测量，那么我们就有了子抽样。带子抽样的 CRD 和随机区组设计（带或不带子抽样）都是单因素设计，但它们仍然需要多水平模型。为什么？因为，尽管它们具有单一的处理因素\n，并且需要一个将因素水平分配给重复单元的随机化方案，但它们仍然需要多个随机变量——除了观测的（基于随机效应的条件）分布之外，线性预测器中至少有一个随机效应——以充分考虑相关概率过程的变异源。当我们添加第二个因素时，会出现更复杂的多水平研究。假设除了课程（A1 或 A2）作为一个因素外，我们还有一个因素 B。如果像之前一样将因素的组合分配给教室，如果我们在教室水平进行观察，那么这仍然是一个单水平实验；如果我们在单个学生水平进行观察，那么这是一个简单的多水平子抽样模型。我们有一个由因素组合定义的 × b 个处理，但重复单元仍然是教室。然而，假设我们像以前一样将课程分配给教室，但将因素 B 的水平分配给整个学校或教室内的单个学生。现在，我们有两个不同尺寸的重复单元，并且我们需要两个随机化方案，一个用于将不同的课程（因素 的水平）分配给教室，另一个用于分配因素 B 的水平。这些是更复杂的多水平研究的定义标准。这在观察性研究中是如何运作的？假设我们想知道暴露在环境危害中是否会增加某种健康问题的风险。一个可能的设计会涉及识别暴露于这种危害的人群和一组未暴露但在其他方面相似的人群。我们可能会进一步尝试创建匹配的配对——一个暴露和一个未暴露但具有相似特征的人。匹配在观察性研究中产生线性预测效应的方式与在设计性实验中的分区是一样的。个人显然是观察的单元。那么重复的单元是什么呢？这里的“处理”因素是危害，存在或不存在。危害是“分配”给地点的。如果我们想要将这种推断扩展到所有这类危害，重复的单元就是地点，而不是个体。如果我们只查看一个地点，我们就会相应地限制推断范围。在一个调查样本中，假设我们想要找出学生的政治取向是否因高等教育机构的类型而异。例如，公立大学是否拥有更高比例的自我认同的保守派？私立文理学院是否拥有更高比例的自我认同的自由派？在这种情况下，因素是机构的类型，分配给类型的最小实体是整个学院或大学。被调查的单个学生构成子抽样。如果我们加入第二个因素，比如专业，那么特定机构中特定专业的整个项目就是关于这个因素的重复单元。现在我们有一个多水平研究：两个因素，机构类型和专业；两个重复单元，学院或大学和学院内部的项目。我们在第 2 章中介绍的框架 ANOVA (WWFD?) 过程可以是识别多水平研究并将其转化为线性预测器的特别有效的方法。","code":""},{"path":"chap9.html","id":"sec9-2-2","chapter":"第 9 章 多水平模型","heading":"9.2.2 重温“Fisher 会怎么做？”——地形和处理部分","text":"让我们将上述示例转换为框架 ANOVA 术语。这将有助于我们直观地了解单水平和多水平。首先，最简单的完全随机形式成为：在这里，单水平结构是显而易见的—— “trt” 必须是因子，而 “unit(trt)” 是重复单元的唯一可能候选者。回想一下我们在第 3 章中对条件和边际模型的讨论，必须将 “unit(trt)” 所在行视为“残差”，因此在高斯模型的线性预测器中不会出现，但它可能会并且通常应该作为非高斯模型线性预测器中的随机效应出现。这种残差与线性预测器效应是过度离散的一个方面，我们将在第 ?? 章和第 ?? 章中更深入地探讨它。区组设计有一个额外特征：现在在地形方面有两个元素。在这个有限的意义上，我们有一个多水平模型，但我们仍然只有一个因素和唯一一个可能的重复单元：区组-处理的组合。因此，从 GLMM 的角度来看，我们有一个单水平模型。同样，区组-处理效应必须视为残差，因此在高斯模型的线性预测器中不会出现，但在某些非高斯模型中可能会出现。如果我们还包括子抽样，例如我们的学校/教室示例中的学生，我们就有“unit(block)” 行下方的实线将框架 ANOVA 分为上下两部分。上部分是处理水平随机化发生的地方；而下部分是无法独立于区组-处理组合进行随机化的抽样单元。但是，它们除了区组和 unit(block) 的变异外，还有自己的变异。地形方面有三个水平：区组、单元和子单元。模型必须以某种方式考虑这三个水平。对分层线性模型 (hierarchical linear model, HLM) 用户特别重要的一点评论是：在 HLM 术语中，subunit(block × trt) 被称为水平 1 (level 1)，即研究中的最小单元。在之前的框架 ANOVA 中，对于没有子抽样的区组设计，区组-处理组合是代表水平 1 的项。这就是 HLM 术语可能有些不够清楚甚至令人困惑的地方。HLM 水平并不区分观察的单元和重复单元。在有子抽样的情况下，重复单元是水平 2，没有子抽样的情况下，重复单元是水平 1。这不一定是个问题，但可能会令人困惑。HLM 用户确实需要在这方面多加努力，因为 HLM 术语并不强调设计概念。对于环境危害观察性研究，可能的框架 ANOVA 为：关于正在研究的处理因素，我们仍然有一个单水平设计，但现在我们有了三层嵌套结构。这种结构极易导致伪重复 (pseudo-replication)：在这种情况下，错误地将配对或个体作为重复。在这个设定中，它们显然不是。请注意，在 HLM 术语中，真正的重复，site，是模型水平 3 的组件。再次强调，HLM 水平和重复单元之间没有明确的对应关系。最后，大学类型-专业的例子也是一个多水平模型。这里我们有两个不同的重复单元：类型/机构，以及专业/项目。请注意，我们将框架 ANOVA 分为三个层次：两个水平与处理设计相关，即类型/机构和专业/项目，以及一个水平用于子抽样，即单个学生。通常，对于高斯模型，我们必须将学生效应视为残差，但对于一些非高斯模型，学生效应应在线性预测器中表示。此外，我们在处理结构方面有一些灵活性。如果不同类型的机构有不同的专业，我们可以用一个嵌套效应—— major(institution) ——来代替单独的主效应（major）和交互效应（major × institution）。\n图 2.6 中的不连通不完全设计—— 10 个区组，每个尺寸为 3，有 6 种处理，其中一组（处理 0, 1, 2）始终一起位于 5 个区组中，另一组（处理 3, 4 和 5）始终位于另五个区组中——为我们提供了多水平结构的另一种视角。框架 ANOVA揭示了多水平结构的面貌。重复单元是组 (set) 的区组以及组内处理的区组内单元 (units within block treatments within set)。如果处理设计具有额外的结构——例如，析因设计中 “set” 对应于因素 ，每个组内的处理对应于因素 B 的水平——那么我们可以在区组水平用 替换 “set”，在单元水平用 B 和 × B 替换 “trt(set)”。我们可以根据特定研究的要求和限制，以无数种方式改变这些结构。Federer King (2007) 一书中列举了裂区设计的各种变体。任何多水平研究——无论是设计的研究、观察性研究还是调查——的关键特征都是至少存在两个不同尺寸的重复单元。这意味着所有多水平模型都必须是混合模型。与最小单元相关的变异可以嵌入 \\(\\symbf y\\mid \\symbf b\\) 的分布中。实际上，对于具有高斯分布的观测，这是必须的。另一方面，与较大单元相关的变异必须在线性预测器中建模——也就是说，在 \\(\\symbf b\\) 中必须至少包含较大的重复单元作为随机模型效应。换句话说，如果我们有一个多水平结构，我们的线性预测器就必须包含一个 \\(\\symbf{Zb}\\) 分量。","code":""},{"path":"chap9.html","id":"sec9-3","chapter":"第 9 章 多水平模型","heading":"9.3 区组在多水平模型中的作用","text":"区组效应是固定的还是随机的？自从 Yates (1940) 开发了区组间信息的恢复（原始的随机区组效应方法）以来，这是持续争议的问题。这一问题对于多水平模型尤其重要，因为固定或随机的决定会影响可估性。在建模中，我们有两种不可估性——真实的和虚假的。真实不可估性的例子包括仅在截距上定义的函数或 ANOVA 型模型中孤立的处理效应：\\(\\eta + \\tau_i\\) 是可估的，但 \\(\\eta\\) 和 \\(\\tau_i\\) 单独是不可估的。在仅含主效应的设计中的交互效应，以及因子设计中缺少因子组合的某些交互对比，都是真实不可估性的其他常见例子。虚假不可估性的最严重的例子出现在多水平设计中。其中大多数可以直接归因于将区组当作固定效应来处理，这要么是有意为之，要么是更常见地，使用了不能识别固定效应与随机效应差异的软件，从而不适当地评估了可估性。在 9.3.1 节中，我们将更详细地讨论固定区组或随机区组的争议的各个方面。在 9.3.2 节中，我们具体考虑了多水平模型对可估性的影响。","code":""},{"path":"chap9.html","id":"sec9-3-1","chapter":"第 9 章 多水平模型","heading":"9.3.1 重温“固定区组效应与随机区组效应”","text":"区组效应是固定的还是随机的？用线性预测器术语来表述这个问题，我们是把模型写成 \\(\\eta+\\tau_i+\\rho_j\\)，其中 \\(\\rho_j\\) 表示区组效应，是 \\(\\symbf{X\\beta}\\) 的一部分；还是写成 \\(\\eta+\\tau_i+r_j\\)，其中 \\(r_j\\) 表示区组效应，有一个假定的分布，是 \\(\\symbf{Zb}\\) 的一部分？争议主要有两个方面，一个是定义性的，一个是实用性的，后者涉及两个模型的估计和推断的小样本行为。让我们从回顾定义开始。根据定义，固定模型效应表示具有有限水平的因素，并且我们的设计提供了对该因素的每个水平的观察。随机模型效应来源于一个因素，其水平构成了一个总体。我们无法观察整个总体；相反，我们通过某种形式的抽样来观察总体的代表。对于随机效应，我们通过模型表示；对于固定效应，我们则是详尽地观察。因此，对于固定效应的推断不会超出实际观察到的水平（使用回归模型在定量水平之间进行插值的情况除外），而对于随机效应的推断则扩展到所观察到的水平所代表的总体中。从定义的角度来看，这场争议的关键在于：我们是将观察到的区组视为整个总体，还是将它们理解为来自一个目标总体的代表？我们是否满足于将推断仅限于我们实际观察到的区组，即狭义或特定区组的推断，还是我们的目标要求进行广义或总体平均的推断？支持随机区组效应观点的人认为，根据纯粹的定义标准，假设固定区组效应是一个缺乏内在一致性的建模决策。固定区组的拥护者通常会回避定义问题并关注实际问题。而实际问题是什么呢？首先，如果我们将讨论限制在均衡数据（即无缺失数据的随机完全区组设计）以及根据处理差异定义的可估函数上，那么争议就没有实际意义。固定和随机区组效应的分析会产生相同的结果。那么不均衡数据？在实际研究中，数据缺失常常发生。此外，在许多情况下，我们使用不完全区组设计而不是完全区组设计来提高效率 (efficiency). 我们将在第 ?? 章中更详细地讨论后一个问题，该章涉及规划设计的精度、功效 (power) 和样本量。这就提出了第二个主要的实际问题。对于不均衡区组设计，Yates (1940) 表明，恢复区组间信息并将其与区组内信息相结合可以提高效率。后来，区组间和区组内信息的综合分析被证明等价于随机区组效应混合模型分析（例如，参见 Bhattacharya, 1998）。随机区组分析更有效——前提是我们有已知的区组方差。但当我们对其未知时又该如何呢？这是一个重要的问题，因为在实际研究中，我们从来都无法确切知道。这就是真正的争议所在，这可以表述为：为了从恢复区组间信息中获益，区组方差的估计需要有多好？得到一个好的估计需要满足什么条件？固定区组的主要论点基于一个假设，即在有较少区组的研究中（例如，在农业中，使用 3 - 5 个区组是常规做法），区组自由度太少，因此无法得到区块方差的良好估计，因此恢复区组间信息是适得其反的。如果该假设是真的，那么支持固定区组的论点就站得住脚。如果不是，则不然。那么情况有多糟糕呢？让我们考虑一个案例，固定区组拥护者可能会称之为“随机区组模型的噩梦场景”。以下是分区安排：我们有 3 个处理和 3 个区组，每个区组的尺寸为 2。该设计仅允许使用 2 个自由度来估计区组方差。固定区组和随机区组分析比较起来如何？为了回答这个问题，我们转向模拟。考虑两种场景：1) 所有三种处理的均值相等（例如，\\(\\mu_1 = \\mu_2 = \\mu_3 = 0\\)）；2) 处理之间相差 15 个单位（例如，\\(\\mu_1 = 0,\\mu_2 = 15,\\mu_3 = 30\\)）。场景 1 允许我们评估 类错误控制；场景 2 允许我们评估功效。假设数据符合高斯分布。不失一般性，设 \\(\\sigma^2=1\\)。在理论上，对于 \\(\\alpha = 0.05\\)，在场景 1 下，对于 \\(H_0\\colon \\mu_1 = \\mu_2 = \\mu_3\\) 的拒绝率应为 5%. 在场景 2 下，随机区组分析的拒绝率取决于区组方差 \\(\\sigma^2_B\\)。对于本模拟中使用的 \\(\\sigma^2_B=0.25\\)，在理论上，固定区组和随机区组分析下的拒绝率应分别为 0.806 和 0.849. 读者可以很容易地验证，改变 \\(\\sigma^2_B\\) 不会改变基本发现。这些发现是什么？以下表格提供了 1000 次模拟实验的摘要。场景 1：所有均值相等场景 2：所有均值不等鉴于该设计中每个处理有两个重复，且只有 1 个自由度用于残差，我们不应该对任何一种分析抱有太大期望。当然，我们不会在实际研究中采用这种设计。也就是说，如果区组太少确实会影响随机区组效应分析，我们应该在这种设计中看到这种影响被最大化。然而事实并非如此。可以肯定的是，\\(\\sigma^2_B\\) 和 \\(\\sigma^2\\) 本身的估计并不准确。然而，关于 类错误率和功效特性的比较显示，随机区组分析比固定区组分析更有利。这是因为处理的检验统计量取决于 \\(Var{\\left[\\symbf{K'}\\left(\\symbf{X'V}^{-1}\\symbf{X}\\right)^-\\symbf{K}\\right]}\\) 的总体估计，其中 \\(\\symbf K\\) 通过可估函数 \\(\\symbf{K'\\beta}\\) 定义了 \\(H_0\\colon\\mu_1=\\mu_2=\\mu_3\\)。重要的是这个估计的质量，而不是单独的 \\(\\sigma^2_B\\) 和 \\(\\sigma^2\\)。底线：这远不是固定区组分析的决定性胜利——如果说有什么的话，随机区组分析似乎在整体上略微优于固定区组模型——尤其是在检验处理效应方面。到目前为止，我们一直在考虑单水平的情况。如果从得分来看，在定义方面，随机区组模型因对方弃权而获胜；在实际应用方面，对于区组较少的设计，两者基本上打了个平手，而对于区组较多的设计，随机区组模型略胜一筹。如果我们只关注单水平设计，这场争议就没有看上去那么严重。现在，我们将注意力转向多水平模型，这才是真正的问题所在。","code":""},{"path":"chap9.html","id":"sec9-3-2","chapter":"第 9 章 多水平模型","heading":"9.3.2 固定区组、多水平设计与虚假的不可估性","text":"让我们从图 2.6 中的不连通设计开始。回想，平面图为我们在第 2 章中看到，对于该设计，假定区组效应 \\(\\rho_j\\) 是固定的，则朴素线性预测器 \\(\\eta+\\tau_i+\\rho_j\\) 会导致边际处理均值的可估性问题，以及涉及不在共同区组中处理差异的可估性问题。例如，考虑定义为 \\(\\eta+\\tau_i+1/10\\sum_{j=1}^{10}\\rho_j\\) 的边际均值，为了使边际均值可估，必须存在系数 \\(a_{ij}\\)，使得 \\(\\sum_{,j}a_{ij}\\left(\\eta+\\tau_i+\\rho_j\\right)=\\eta+\\tau_i+1/10\\sum_j\\rho_j\\)。这意味着 \\(\\sum_{,j}a_{ij}=1\\)（为获取 \\(\\eta\\)），\\(\\sum_ja_{ij}=1\\) 以及对于所有 \\('\\ne \\) 有 \\(\\sum_ja_{'j}=0\\)（为获取 \\(\\tau_i\\)），并且对于 \\(j=1,2,\\ldots\\) 有 \\(\\sum_ia_{ij}=1/10\\)。前两个条件很容易同时满足，但前两个条件排除了了第三个条件的满足。或者，我们可以证明 \\(\\symbf K\\) 不满足 \\(\\symbf{K'}(\\symbf{X'X})^{-}\\symbf{X'X}=\\symbf{K'}\\) 准则，因此 \\(\\mu+\\tau_i+1/10\\sum_j\\rho_j\\) 不可估。从区块效应的定义角度来看，这个例子为我们首次展示了虚假的不可估性。如果我们把区块效应视为随机的，那么恢复区组间信息将使我们能够准确分析这一设计的真正内容：一个嵌套的析因设计，其中析因结构由 set 和 treatment(set) 定义。这就是我们 9.2.2 节末尾看到的框架 ANOVA.利用固定区组模型，我们可以使用不太简单的线性预测器版本，以解决可估性问题：\\(\\eta+\\alpha_i+\\beta(\\alpha)_{ij}+\\rho(\\alpha)_{ik}\\)，其中 \\(\\alpha\\) 表示组效应，\\(\\beta(\\alpha)\\) 表示 set 内的处理，\\(\\rho(\\alpha)\\) 表示区组效应，现在视为嵌套在组内。实际上，大多数固定区组拥护者会承认，在这种设计背景下，\\(\\rho(\\alpha)\\) 是一个整区效应 (whole-plot effect) ——即关于组的重复单元——因此是随机的。然而，我们应该注意到，固定效应拥护者倾向于使用普通最小二乘软件，该软件在求解估计方程、确定标准误以及（对于本次讨论而言最重要的）评估可估性时将 \\(\\rho(\\alpha)\\) 视为固定的。上述修复对于均衡数据“起效”（在一定程度上）。现在让我们考虑一个不均衡的多水平设计。Stroup et al. (2018) 阐释了裂区实验的分析，其中整区是不完全区组设计，其结构与上一节中所示的类似。以下显示了分区安排的基本特征。因素 的水平被随机分配给贯穿每个区组的行。因素 B 的水平随机分配给区组中每行内的两个单元格，即在因素 的每个重复单元内部。用框架 ANOVA 的术语来说，我们有其中 \\(r\\) 表示区组数量：如果每种类型使用超过一个区组的话，则 \\(r\\) 可以是 3，也可以是 6, 9 等。标准的实验设计教科书为这种设计给出的线性预测器为：\\[\\eta_{ijk}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\rho_k+(\\alpha\\rho)_{ik}\\]如果我们把区组——即 \\(\\rho_k\\) 和 \\((\\alpha\\rho)_{ik}\\) ——当作固定效应，那么所有的 和 × B 边际均值都将变得不可估，所有 的差异也将变得不可估，以及所有涉及 的不同水平的 × B 的差异——这包括给定 B 的 的简单效应——也将变得不可估。这提供了一个明显虚假的不可估的例子。没有人会争辩说 \\((\\alpha\\rho)_{ij}\\) 应该视为固定效应，因为它是关于因子 的重复单元效应。问题通常出现在使用为普通最小二乘法设计的软件（例如 SAS® PROC GLM）来计算此类设计的分析时。PROC GLM 无法区分固定效应和随机效应，因此不恰当地评估了边际均值、差异及所有其他可估函数，就好像 \\((\\alpha\\rho)_{ik}\\) 是固定的。这就是为什么混合模型（包括所有多水平模型）的计算只能使用专门为混合模型分析设计的软件来完成。","code":""},{"path":"chap9.html","id":"sec9-4","chapter":"第 9 章 多水平模型","heading":"9.4 使用多水平设计","text":"在前面的章节中，我们定义了多水平结构，回顾了使用框架 ANOVA (WWFD?) 过程来识别多水平结构，并仔细研究了区组效应的适当指定对多水平分析的影响。处理完所有这些预备知识后，我们现在看看使用多水平模型的实际方面。我们首先通过多水平分析的示例开始。","code":""},{"path":"chap9.html","id":"sec9-4-1","chapter":"第 9 章 多水平模型","heading":"9.4.1 多水平结构示例","text":"在本节中，我们将考虑三个多水平分析的示例——指定模型、将模型转换为所需的 GLIMMIX 语句并解释相关输出。在这里，我们只关注高斯数据。在未来的章节中，我们将考虑这些相同的设计，但使用非高斯数据。示例 9.1  (嵌套析因结构) 此示例的数据显示在 Data Set 9.1 中。该设计遵循我们在上一节中考虑的图 2.6 的结构——朴素地称其为不连通设计，但更准确地称为嵌套析因。该模型的完整指定为：线性预测器：\\(\\eta_{ijk}=\\eta+\\alpha_i+\\beta(\\alpha)_{ij}+r(\\alpha)_{ik}\\)，其中 \\(r\\left(\\alpha\\right)\\) 表示组内的区组。线性预测器：\\(\\eta_{ijk}=\\eta+\\alpha_i+\\beta(\\alpha)_{ij}+r(\\alpha)_{ik}\\)，其中 \\(r\\left(\\alpha\\right)\\) 表示组内的区组。分布：\n\\(y_{jk}\\mid r\\left(\\alpha\\right)_{ik}\\sim N\\left(\\mu_{ijk},\\sigma^2\\right)\\)\n\\(r\\left(\\alpha\\right)_{ik}\\text{ iid }N\\left(0,\\sigma_R^2\\right)\\)\n分布：\\(y_{jk}\\mid r\\left(\\alpha\\right)_{ik}\\sim N\\left(\\mu_{ijk},\\sigma^2\\right)\\)\\(r\\left(\\alpha\\right)_{ik}\\text{ iid }N\\left(0,\\sigma_R^2\\right)\\)连接：恒等，\\(\\eta_{ijk}=\\mu_{ijk}\\)连接：恒等，\\(\\eta_{ijk}=\\mu_{ijk}\\)GLIMMIX 语句为相关输出：对于区组，方差分量估计值为 \\(\\hat\\sigma^2_R=60.55\\)，对于以区组效应为条件的观测，方差分量估计为 \\(\\hat\\sigma^2=22.75\\)。“trt(set)” 的 F 值检验 \\(H_0\\colon\\) 所有 \\(\\beta(\\alpha)_{ij} = 0\\)，我们可以更有意义地表示为 \\(H_0\\colon\\eta_{00}=\\eta_{01}=\\eta_{02}\\) 以及 \\(\\eta_{13}=\\eta_{14}=\\eta_{15}\\)。它的 \\(p\\) 值为 0.0089，这告诉我们 1) 检验 “set” 的主效应构成了一个“无趣的假设”，2) 由于 “trt(set)” 是一个综合假设，我们需要将其分成几个组成部分，\\(H_0\\colon\\eta_{00}=\\eta_{01}=\\eta_{02}\\) 以及 \\(\\eta_{13}=\\eta_{14}=\\eta_{15}\\)，使用 SLICE 选项以获得信息丰富的解释。这告诉我们，我们有证据表明在组 1（\\(p=0.0017\\)）内有处理效应，但在组 0（\\(p=0.9709\\)）内没有。查看处理均值就能明白这一结果。最后，SLICEDIFF 列表显示了组内处理的成对简单效应。我们看到，组 1 中的处理效应来源于统计上显著的差异 \\(\\eta_{13}-\\eta_{15}\\) 和 \\(\\eta_{14}-\\eta_{15}\\)（\\(p\\) 值分别为 0.0007 和 0.0048）。换句话说，处理 5 的均值与处理 3 和 4 的均值存在显著差异。示例 9.2  (不完全条区 (strip-plot)) 此示例的数据在 SAS Data Program Library 中显示为 Data Set 9.2。处理设计为 3 × 3 析因。地形设计有九个区组，每个区组由两行两列、总共四个单元组成。因素 的水平随机分配给每个区组的行，但有以下限制：三个区组仅接收水平 A1 和 A2；三个区组仅接收 A1 和 A3；其他三个区组仅接收 A2 和 A3。因素 B 的水平随机分配给每个区组的列，但有以下限制：接收给定一对 水平的区组之一必须接收 B1 和 B2，一个必须接收 B1 和 B3，另一个必须接收 B2 和 B3。\n因此，我们的多水平结构是二维不完全区组设计。框架 ANOVA 过程可以帮助我们直观地看到所需的线性预测器：请注意，虽然有两个随机化过程，但实际上存在三种尺寸的重复单元：用于 水平的行、用于 B 水平的列以及用于 × B 组合的行列交叉。这些数据的模型为：线性预测器：\\(\\eta_{ijk}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+r_k+(ar)_{ik}+(br)_{jk}\\)线性预测器：\\(\\eta_{ijk}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+r_k+(ar)_{ik}+(br)_{jk}\\)分布：\n\\(y_{ijk}\\mid r_k,\\left(ar\\right)_{ik},\\left(br\\right)_{jk}\\thicksim NI\\left(\\mu_{ijk},\\sigma^2\\right)\\)\n\\(r_k\\mathrm{~iid~}N\\left(0,\\sigma_R^2\\right)\\)\n\\(\\left(ar\\right)_k\\mathrm{~iid~}N\\left(0,\\sigma_{AR}^2\\right)\\)\n\\(\\left(br\\right)_k\\mathrm{~iid~}N\\left(0,\\sigma_{BR}^2\\right)\\)\n分布：\\(y_{ijk}\\mid r_k,\\left(ar\\right)_{ik},\\left(br\\right)_{jk}\\thicksim NI\\left(\\mu_{ijk},\\sigma^2\\right)\\)\\(r_k\\mathrm{~iid~}N\\left(0,\\sigma_R^2\\right)\\)\\(\\left(ar\\right)_k\\mathrm{~iid~}N\\left(0,\\sigma_{AR}^2\\right)\\)\\(\\left(br\\right)_k\\mathrm{~iid~}N\\left(0,\\sigma_{BR}^2\\right)\\)连接：恒等，\\(\\eta_{ijk}=\\mu_{ijk}\\)连接：恒等，\\(\\eta_{ijk}=\\mu_{ijk}\\)该模型的 GLIMMIX 语句，包括激活 ODS 图形以使用 LSMEANS 语句 MEANPLOT 选项生成交互图，为：与我们对不均衡混合模型的标准误和检验统计量偏差的讨论一致，我们使用 DDFM=KR 来调用 Satterthwaite 自由度近似和 Kenward-Roger 偏差校正。交互图如下所示：我们没有看到 B 效应与 A3 结合的视觉证据。对于 A1 和 A2，B1 的均值似乎不同于 B2 和 B3 的均值。由 A1, A2, B2 和 B3 组成的 -B 组合在视觉上略有不同。下面的正式统计量可以说明差异是否具有统计学意义，但主题专家需要确定这些差异是否大到足以产生影响，尽管有正式统计量。方差分量估计和总体效应检验为：× B 的 \\(F\\) 统计量提供了交互作用的有力证据（\\(p =0.007\\)），与交互作用图中的视觉证据一致。此时，推断应关注简单效应。因此，我们看一下 LSMEANS 和简单效应的结果：这些结果为上面给出的视觉特征提供了正式支持：在 A3 中，B 的差异均未达到统计显著性；均值范围在 18.47 到 21.13 之间，标准误为 2.19。在 A1 和 A2 中，B2 和 B3 均值与 B1 不同，但彼此没有差异，B1 与 B3 的差异大于 B2 与 B3 的差异。我们可以继续。例如，B1 与 A1 和 A2 组合的均值似乎与 A3 处所有 B 均值的集合相似。我们将此作为练习。我们在这里看到的是分析的基本流程。请注意，对处理设计本身的推断与对任何其他析因处理设计的推断没有区别。唯一的区别在于正确定义模型的随机效应以与设计结构保持一致。我们这样做是为了确保使用正确的标准误和检验统计量。示例 9.3  (不完全区组的响应曲面设计) 在 8.6.1 节中，我们为所有因素的定量处理设计引入了二阶多项式回归。回想一下，该模型的吸引力部分源于使用高效、不完全析因设计的能力。在响应面应用中，一种流行的设计是 Box-Behnken 设计 (Box Behnken, 1960)。 此示例使用三因素 Box-Behnken 设计——请参阅响应曲面教科书，例如 Myers, Montgomery Anderson-Cook (2016) 对此和其他响应曲面设计进行了全面描述。每个因素都有三个水平——正如响应曲面设计和分析的常见做法，我们将三个水平编码为 -1, 0 和 1。我们通过将每个因素的 水平 0 与来自其他两个因素的水平 -1 和 1 组成的 22 析因设计结合形成处理组合来构建设计。由于我们这里有三个因素，这给我们提供了 12 个处理组合，每组包含四个处理组合。除此之外，我们添加了一种额外的处理组合，其中所有因素的水平均为 0，在设计术语中称为“中心点” (“center point”).本例的数据在 SAS Data Program Library 中显示为 Data Set 9.3. 这些数据的多水平性源于使用的不完全区组设计。与特定因素水平 0 相关联的每组四个处理组合看起来都在一起，每组两个区组，每个区组尺寸为 4. 该设计有六个这样的区组，每组两个。第七个区组由四个全部接收中心点的单元组成，完成了整个设计。请注意，存在两个随机化阶段：第一个阶段随机地将组分配给区组；第二个阶段随机地将组内的处理组合分配给区组内的单元。这种两阶段随机化使得这成为了一个多水平设计。同时请注意，这些多水平并不完全对应于设计中的单一因素（就像示例 9.1 和 9.2 那样。此处呈现这个例子正是为了表明多水平结构可以采取多种形式。建模从业者需要提高警惕。这个例子也突出了依赖食谱的弱点。学习思考这些事情比学习一个足够全面的食谱来涵盖所有建模意外情况更容易——假设你能设计出这样的食谱！我们将如何使用 “WWFD?” 框架 ANOVA 过程来帮助构建线性预测器？这是：函数 \\(f\\left(,B,C\\right)=\\beta_0+\\sum_{=,B,C}\\beta_iX_i+\\sum_{=,B,C}\\beta_{ii}X_i^2+\\sum_{\\neq '=,B,C}\\beta_{ii'}X_iX_i'\\)，因素 , B 和 C 的二阶响应曲面模型，其中 \\(\\beta\\)s 表示回归系数，\\(X_i;=,B,C\\) 表示三个因素的定量水平。这些数据的模型可写为：线性预测器：\\(\\eta_{ijkl}=\\eta_{ijk}+r_{k}\\)。其中 \\(r_l\\) 表示区组效应，并且 \\(\\eta_{ijk}=f\\left(,B,C\\right)+\\left(\\alpha\\beta\\gamma\\right)_{ijk}=\\beta_0+\\sum_{=,B,C}\\beta_iX_i+\\sum_{=,B,C}\\beta_{ii}X_i^2+\\sum_{\\neq '=,B,C}\\beta_{ii'}X_iX_{'}+\\left(\\alpha\\beta\\gamma\\right)_{ijk}\\)。其中 \\(\\left(\\alpha\\beta\\gamma\\right)_{ijk}\\) 为总和项（欠拟合项）。线性预测器：\\(\\eta_{ijkl}=\\eta_{ijk}+r_{k}\\)。其中 \\(r_l\\) 表示区组效应，并且 \\(\\eta_{ijk}=f\\left(,B,C\\right)+\\left(\\alpha\\beta\\gamma\\right)_{ijk}=\\beta_0+\\sum_{=,B,C}\\beta_iX_i+\\sum_{=,B,C}\\beta_{ii}X_i^2+\\sum_{\\neq '=,B,C}\\beta_{ii'}X_iX_{'}+\\left(\\alpha\\beta\\gamma\\right)_{ijk}\\)。其中 \\(\\left(\\alpha\\beta\\gamma\\right)_{ijk}\\) 为总和项（欠拟合项）。分布：\n\\(y_{ijkl}\\mid r_l\\sim N\\left({\\mu}_{ijkl},{\\sigma}^2\\right)\\)\n\\(r_l\\mathrm{~iid~}N\\left(0,{\\sigma}_R^2\\right)\\)\n分布：\\(y_{ijkl}\\mid r_l\\sim N\\left({\\mu}_{ijkl},{\\sigma}^2\\right)\\)\\(r_l\\mathrm{~iid~}N\\left(0,{\\sigma}_R^2\\right)\\)连接：恒等，\\(\\eta_{ijkl}=\\mu_{ijkl}\\)连接：恒等，\\(\\eta_{ijkl}=\\mu_{ijkl}\\)GLIMMIX 语句分三个步骤出现。在步骤 1 中，我们评估模型的欠拟合情况。假设没有欠拟合的证据，我们拟合完整的二阶响应曲面，从线性预测器中删除 \\(\\left(\\alpha\\beta\\gamma\\right)_{ijk}\\)。如果有看起来可以忽略不计的二阶项，我们可以删除它们并运行第三步以拟合更简约的模型。步骤 1 的 GLIMMIX 语句为：注意，我们将 AA, BB 和 CC 定义为 CLASS 变量，以形成欠拟合项 AA*BB*CC。我们在 8.6 节介绍定量析因处理设计时使用了这个技巧。此外，我们在这里使用序贯检验策略，因为我们想在拟合 \\(f(,B,C)\\) 后评估欠拟合。因为我们有一个不完全区组设计，我们使用 Kenward-Roger 调整。相关输出：特别是，因为此时我们正在检验欠拟合项，所以此时我们只关心 AA*BB*CC 的结果。\\(F = 0.22\\) 和 \\(p = 0.8776\\) 表明没有欠拟合的证据。我们继续执行步骤 2。步骤 2 仅需要更改 CLASS 和 MODEL 语句。我们从 MODEL 中删除欠拟合项，并在 CLASS 语句中删除相应的效应。新的语句为：相关输出：请注意线性-线性项 *B 和二次项 C*C。它们的 \\(F\\) 值均小于 1。此外，线性-线性项 *C 的 \\(p\\) 值为 0.2869。这让人怀疑这三个项是否应该保留在模型中。C 的线性主效应也有一个我们通常认为不显著的 \\(p\\) 值。然而，线性-线性 B*C 项具有统计显著性：包含 \\({\\beta}_{BC}X_BX_C\\) 但不包含 \\(\\beta_C X_C\\) 的回归模型违反了序贯检验的精神，并且会产生因素 B 线性和二次主效应的解释问题。因此，因此，根据步骤 2 得出的合理步骤 3 简化模型将是：\\[f\\left(,B,C\\right)=\\beta_0+\\sum_{=,B,C}\\beta_iX_i+\\sum_{=,B}\\beta_{ii}X_i^2+\\beta_{BC}X_BX_C\\]针对步骤 3 修改的 GLIMMIX 语句是：相关输出：我们可以使用这些回归系数生成响应曲面估计图，执行此操作的 SAS 程序是：这将创建一个三维图，显示每个 C 水平在 和 B 水平上的预测响应：我们可以看到，因子 C 对响应曲面的形状有轻微影响。整体图像是一个在 和 B 水平上略微倾斜的二维二次曲面，最大响应出现在中心点附近。本示例的要点是，准确拟合曲面需要对欠拟合以及要保留哪些效应做出适当的决定。除非我们正确地指定多水平误差结构，否则我们无法做到这一点。这个例子说明了多水平混合模型和响应曲面方法是兼容的——在这样的例子中，我们必须同时使用这两种方法。此处未显示但值得一提的另一种非标准多水平结构是 Milliken Johnson (2008) 提出的混合裂区 (mixed-split-plot). 一个由多州组成的联合体进行了一项多因素研究。该方案要求采用裂区设计。这一部分方案内容得到了传达，但关于哪个因素被分配给整区（较大的重复单元）以及哪个因素被分配给裂区（较小的重复单元）的部分却没有传达。因此，对于不同州的数据，因素 和 B 的位置被颠倒了。将这种结构视为不完全区组设计——类似于不连通/嵌套析因示例——揭示了其分析实际上是一种直接但不常见的多水平建模应用。","code":"proc glimmix data=ten_blk_six_trt;\n class block set trt;\n model y = set trt(set);\n random block(set);\n lsmeans set trt(set) / slice=set slicediff=set;ods graphics on;\nproc glimmix data=ds8d2;\n class block a b;\n model y=a|b / ddfm=kr;\n random intercept a b / subject=block;\n lsmeans a*b / plot=meanplot(sliceby=a join) slicediff=a;proc glimmix;\n class block aa bb cc;\n model y=a|a|b|b|c|c@2 aa*bb*cc/ htype=1 ddfm=kr;\n random intercept / subject=block;class block;\nmodel y=a|a|b|b|c|c@2 / htype=1 ddfm=kr;proc glimmix;\n class block;\n model y=a|a b|b b|c/ solution htype=1 ddfm=kr;\n random intercept / subject=block;data rs3;\n do a=-1 to 1;\n  do b=-1 to 1;\n   do c=-1 to 1;\n    y_hat=50.085+1.60*a+1.694*b+0.519*c-3.303*a*a-3.515*b*b-1.163*b*c;\n   output;\n  end;\n end;\nend;\nproc sort data=rs3;\n by c;\nproc g3d;\n by c;\n plot a*b=y_hat/xticknum=3 yticknum=3;"},{"path":"chap9.html","id":"sec9-4-2","chapter":"第 9 章 多水平模型","heading":"9.4.2 多因素处理和多水平设计结构——它们如何一起拟合","text":"上一节中的三个示例都使用了特定的处理结构。事实上，我们在将多水平模型的地形方面与处理方面相结合的方式上具有相当大的灵活性。考虑上一节中的示例 9.1。我们使用了线性预测器 \\(\\eta_{ijk}=\\eta+\\alpha_i+\\beta(\\alpha)_{ij}+r(\\alpha)_{ik}\\)。回顾我们在第 8 章中考虑的处理结构的类型，如果处理设计的具体细节需要，我们可以将处理分量改为其他形式。例如，如果每组内的处理水平实际上都是相同的，我们的处理设计实际上是 2 × 3 析因设计，我们可以相应地修改线性预测器，用 \\(\\eta_{ijk}=\\eta+\\alpha_{}+\\beta_{j}+\\left(\\alpha\\beta\\right)_{ij}+r\\left(\\alpha\\right)_{ik}\\) 替换上面的式子。如果每组中的处理水平是定量的，并且可以修正为多项式回归，我们可以使用 \\(\\eta_{ijk}=\\beta_{0i}+\\beta_{1i}X_{j}+\\beta_{2i}X_{j}^{2}+r(\\alpha)_{ik}\\)。相反，如果我们有一个更加非线性的形状，我们可能更喜欢线性化的 Hoerl，\\(\\eta_{ijk}=\\beta_{0i}+\\beta_{1i}X_{j}+\\beta_{2i}\\log\\left(X_{j}\\right)+r\\left(\\alpha\\right)_{ik}\\)。注意，所有这些都是 \\(\\eta_{ijk}=\\eta_{ij}+r\\left(\\alpha\\right)_{ik}\\) 的变体。嵌套、析因和回归模型只是表示编写 \\(\\eta_{ij}\\) 以适应处理设计的不同方式。为多水平设计结构建模的组件 \\(r(\\alpha)_{ik}\\) 从不更改。请注意，它与关于 \\(\\eta_{ij}\\) 的所有变体都兼容。举例说明，让我们将另一种均值模型应用于示例 9.2。这些数据提供了比 Data Set 9.4.1 更有趣的演示，因为增加了复杂性：示例 9.1 是基本的均衡裂区；示例 9.2 是不完全条区，具有关于因素 水平、因素 B 水平和 AB 处理组合的不同重复单元。然而，我们可以改变处理模型，就像我们对示例 9.1 所做的那样。也就是说，线性预测器的一般形式 \\(\\eta_{ijk}=\\eta_{ij}+r_{k}+\\left(ar\\right)_{ik}+\\left(br\\right)_{jk}\\) 划分为处理部分 \\(\\eta_{ij}\\) 和多水平设计（或“地形”）部分 \\(r_k+\\left(ar\\right)_{ik}+\\left(br\\right)_{jk}\\)。我们可以用我们见过的任何方式更改 \\(\\eta_{ij}\\)，同时保持 \\(r_k+\\left(ar\\right)_{ik}+\\left(br\\right)_{jk}\\) 不变。我们首先将处理方面表示为嵌套模型，即 \\(\\eta_{ij}=\\eta+\\alpha_i+\\beta(\\alpha)_{ij}\\)。实现该模型的 GLIMMIX 语句是：我们这样做可能是为了将推断集中在给定 的 B 的简单效应上。请注意，使用 LSMESTIMATE 和 ESTIMATE 语句来估计我们早期分析中出现的分组的均值和差异。使用均值模型的嵌套形式可以使这些估计不易出现可估性问题，从而促成这些估计。同时，重要的是，请注意我们完全没有改变随机语句。我们改变了均值模型，将关注点重新集中在我们希望强调的处理分析方面，但我们保留了多水平设计模型。相关输出（跳过了与之前重复的输出）：B() 检验告诉我们，在 的每个水平上，B 因素的均值之间存在差异，但正如我们之前看到的，这种统计量的组合妨碍了有用的解释。最好是查看 SLICE 输出或使用 ESTIMATE 和 LSMEANSTIMATE 输出进行更仔细的观察。这样是更好的。我们看到在 A1 和 A2 内有显著的 B 效应，但在 A3 内没有。将这些差异分开，我们有：以及LSMESTIMATE 输出分别列出 \\(\\begin{pmatrix}\\eta_{11}+\\eta_{12}+\\eta_{21}+\\eta_{22}\\end{pmatrix}/4\\text{,}\\begin{pmatrix}\\eta_{31}+\\eta_{32}\\end{pmatrix}/2\\) 和 \\(\\left(\\eta_{13}+\\eta_{23}\\right)/2\\) 的估计。ESTIMATE 输出分别列出 \\(\\left[\\left(\\eta_{11}+\\eta_{12}+\\eta_{21}+\\eta_{22}\\right)/4\\right]–\\left[\\left(\\eta_{31}+\\eta_{32}\\right)/2\\right]\\) 和 \\(\\begin{bmatrix}(\\eta_{13}+\\eta_{23})/2\\end{bmatrix}-\\eta_{33}\\) 的估计。我们这样做是因为示例 9.2 的分析表明 A1 和 A2 中 B1 和 B2 的均值相似，因此对这四个均值进行平均是有意义的。B1 和 B2 的均值在 A3 内没有差异，因此对它们进行平均也是有意义的。 B3 的均值与 B1 和 B2 不同，但 B3 与 A1 的组合与 B3 与 A2 的组合没有区别。因此就有了第三个 LSMESTIMATE 语句。ESTIMATE 语句获取这些分组之间的差异估计。当我们想知道差异的大小，而不仅仅是差异是否存在时，我们就会这样做。让我们更进一步。假设 B 的水平是定量的。在交互图中，均值模式表明可能存在二次回归——当然是在 A1 和 A2 内的 B 水平上。我们可以用定性-定量模型对此进行建模：具体来说，\\(\\eta_{ij}=\\beta_{0i}+\\beta_{1i}X_j+\\beta_{2i}X_j^2\\)，其中 \\(X_j\\) 表示因素 B 的第 \\(j\\) 个定量水平。GLIMMIX 语句为：变量 XB=B 以及 XB_SQ=B*B。我们必须在数据步骤中定义它们。它们允许 GLIMMIX 使用 B 作为 CLASS 变量来指定多水平设计模型，但将 XB 和 XB_SQ 分别用作 \\(X_j\\) 和 \\(X_j^2\\) 的直接变量。请注意，RANDOM 语句保持不变。CONTRAST 语句允许我们检验 A1 和 A2 的二次回归系数是否相等，如果相等，则检验它们是否与 A3 不同。第二组 CONTRAST 将线性和二次项合并为回归的共同检验。NOINT 选项为我们提供了一个满秩模型：因此，SOLUTION 向量提供了实际的回归估计，无需进行进一步的操作。相关输出：例如，A1 的估计回归方程为 \\(0.64+26.31X_j-5.71X_j^2\\)。这些估计值清楚地显示了 A1 和 A2 的关于 B 的回归的相似性以及 A3 的 B 的回归非常不同。我们还可以看到，A3 下的线性和二次系数在统计上并不显著——更准确地说，对于任何合理的 \\(\\alpha\\) 水平，我们无法拒绝 \\(H_0\\colon\\beta_{13} = 0\\) 和 \\(H_0\\colon\\beta_{23} = 0\\)。对于 A3，仅使用截距可能会产生更好的拟合效果。我们将在下面继续探讨这一点。我们在这里展示 “type III tests fixed effects” 更多的是因为它们没有用，而不是因为它们提供了有意义的输出（它们实际上没有）。此列表中的每一行都是过度组合的检验统计量。这三个自由度来自将截距、线性和二次回归的每个参数在因素 的所有水平上分别组合，并同时对其进行检验。将这些结果与上述各个回归系数的结果进行比较，你可以看到 “type III tests” 并没有提供有用的信息。最后是对比结果：这些结果证实了我们从其他角度观察到的现象。A1 和 A2 在 B 上的回归是相似的，但与 A3 的回归不同。最后，我们之前注意到，各个回归系数的检验表明，仅使用截距对 A3 的响应进行建模可能更好，因为 B 似乎在 A3 中没有效应。简化模型将是\\[\\eta_{ij}=\\beta_{0i}+\\left(<3\\right)\\beta_{1i}X_j+\\left(<3\\right)\\beta_{2i}X_j^2\\]其中 \\((< 3)\\) 是指示函数，对于 A1 和 A2 水平等于 1，对于 A3 等于 0. 这消除了用于 A3 的 B 的回归。为了使用 GLIMMIX 实现这一点，我们需要在数据集中定义指示函数。在下面的程序中，对于 A1，B_1=1，否则为 0；对于 A2，B_2=1，否则为 0；对于 A3，B_3=1，否则为 0。GLIMMIX 语句为：请注意，MODEL 语句现在根本不包含 CLASS 变量。所有三个截距均出现，并且仅显示 A1 和 A2 的线性项和二次项。注意 CONTRAST 语句的形式。每个效应只有一个系数，因为模型中的每项都是一个参数。对于学生：你应该能够写出每个对比的可估函数的精确形式。输出为：第一个表给出了简化模型的系数估计。注意，\\(\\hat\\beta_{03} =19.44\\) 估计 A3 在所有 B 水平上的预期响应。对比的结果证实，不存在统计上显著的证据来得出 A1 和 A2 的回归参数不同的结论。我们在这里没有展示它，但我们可以通过在 A1 和 A2 上合并 \\(\\beta_{0i},\\beta_{1i}\\) 和 \\(\\beta_{2i}\\) 来进一步简化模型。","code":"proc glimmix data=ds8d2;\n class block a b;\n model y=a b(a)/ddfm=kr;\n random intercept a b / subject=block;\n lsmeans b(a) / slice=a slicediff=a; \n lsmestimate b(a)‘b1+b2 avg for a1 and a2’ 1 1 0 1 1 0 0 0 0,\n                 ‘b1+b2 avg for a3’ 0 0 1 0 0 1 0 0 0,\n                 ‘b3 avg for a1 and a2’ 0 0 0 0 0 0 1 1 0 / \n                   divisor=4,2,2;\n estimate ‘b1&2 - b3 diff for a1&2’ b(a) 1 1 -2 1 1 -2 0 0 0,\n          ‘b1&2 - b3 diff for a3’ b(a) 0 0 0 0 0 0 1 1 -2 / \n             divisor=4,2;proc glimmix data=ds8d2;\n class block a b;\n model y=a xb(a) xb_sq(a)/noint s ddfm=kr;\n contrast ‘quadratic equal, a1 and a2’ xb_sq(a) 1 -1 0;\n contrast ‘quadratic equal, a3 v a1&2’ xb_sq(a) 1 1 -2;\n contrast ‘regression equal, a1 and a2’ xb(a) 1 -1 0,\n                                        xb_sq(a) 1 -1 0;\n contrast ‘regression equal, a3 v a1&2’ xb(a) 1 1 -2,\n                                        xb_sq(a) 1 1 -2; \n random intercept a b / subject=block;proc glimmix data=ds8d2;\n class block a b;\n model y=b_1 b_2 b_3 b_1*xb b_2*xb b_1*xb_sq b_2*xb_sq/noint s \n  ddfm=kr;\n contrast ‘intercept equal, a1 and a2’ b_1 1 b_2 -1;\n contrast ‘linear equal, a1 and a2’ b_1*xb 1 b_2*xb -1;\n contrast ‘quadratic equal, a1 and a2’ b_1*xb_sq 1 b_2*xb_sq -1;\n random intercept a b / subject=block;"},{"path":"chap9.html","id":"sec9-5","chapter":"第 9 章 多水平模型","heading":"9.5 边际和条件多水平模型","text":"我们在第 3 章中介绍了边际模型，并在第 5 章和第 6 章中提供了额外的背景知识。我们已经看到，高斯仅方差分量模型可以重新表述为复合对称边际模型。对于非高斯模型，我们可以对工作的相关矩阵进行类似的处理，但与高斯情况不同，这两个参数化是不等价的。对于多水平模型，条件与边际的区别在何时重要？这个问题主要在我们有负的方差分量估计时出现。为了解这是如何工作的，我们重新回顾了示例 9.2 中使用的设计结构，但使用了不同的数据。首先，我们考虑高斯情况。然后，我们简要评论了对于非高斯情况，我们的方法将如何不同。在未来的章节中，我们将更详细地讨论非高斯情况。","code":""},{"path":"chap9.html","id":"sec9-5-1","chapter":"第 9 章 多水平模型","heading":"9.5.1 高斯数据","text":"Data Set 9.4 使用与 Data Set 9.2相同的结构，但数据不同。使用我们在示例 9.2 中使用的相同 GLIMMIX 语句，方差分量估计为：请注意，因素 的重复单元之间的方差估计 \\(\\hat\\sigma_{AR}^2\\) 已设置为零。当 REML 解向量 \\(\\hat{\\symbf\\sigma}\\) 收敛到满足 \\(\\hat\\sigma_{AR}^2<0\\) 的值时，就会出现这种情况。由于负解位于参数空间之外，通常的做法是将这些方差分量估计置零。Stroup Littell (2002) 发现置零做法会对功效特性和置信区间覆盖范围产生不利影响。建议的修复方法是允许 \\(\\hat\\sigma_{AR}^2\\) 保持负值。在 GLIMMIX 中，我们通过在 PROC GLIMMIX 语句中包含 NOBOUND 作为选项来实现此目的。使用 NOBOUND 得出的方差分量估计为NOBOUND 如何影响对 和 B 处理效应的推断？不带 NOBOUND：带 NOBOUND：我们看到了 NOBOUND 对这些数据的影响：主效应检验统计量因 NOBOUND 而增加；交互检验略有下降。NOBOUND 的影响是特定于设计的——主效应和交互检验不会总是受到这种影响。从全局来看，模拟结果一致表明，NOBOUND 结果在 类错误率控制和功效特性方面更准确。我们是如何评估后者的？在第 ?? 章中，我们讨论了基于 GLMM 的功效分析。当我们说 NOBOUND 在功效特性方面更准确时，我们的意思是，当处理均值不等时，使用 NOBOUND 的拒绝率比不使用 NOBOUND 获得的拒绝率更符合第 ?? 章所示方法的理论预测。虽然 NOBOUND 产生了更准确的推断，但它给我们留下了一个尴尬的问题，即准确解释负方差的含义。例如，在这里，我们所说的 \\(\\hat\\sigma_{AR}^2=-6.595\\) 是什么意思？对于高斯数据，另一种方法是将受影响的方差分量重新表示为复合对称协方差。在我们的示例中，GLIMMIX 语句为：得到的方差分量估计：请注意，这只是 NOBOUND 结果的重新表达，与第 3 章中方差分量模型与复合对称性模型的等价性类似。主要区别在于：在统计理论中，-6.595 的协方差是良定的；而 -6.595 的方差则不是。","code":"proc glimmix data=ds8d2;\n class block a b;\n model y=a|b/ddfm=satterth;\n random intercept b / subject=block;\n random _residual_ / type=cs subject=block*a;"},{"path":"chap9.html","id":"sec9-5-2","chapter":"第 9 章 多水平模型","heading":"9.5.2 非高斯模型","text":"正如我们在第 3 章中看到的，边际复合对称模型与条件方差分量模型的等价性并不能扩展到非高斯模型。边际模型的目标是不同的参数——边际分布的均值——而条件模型的目标是 \\(\\symbf y\\mid \\symbf b\\) 的期望值，通常是指数族成员的可识别参数。考虑到这种不等价性，当我们在非高斯 GLMM 中估计负方差分量时，NOBOUND 是适当的补丁。复合对称工作相关性实际上不能被视为一种可行的替代方案。在接下来的章节中，我们将回到这个主题，通过示例进行说明。例如，在第 ?? 章中，我们将详细介绍计数数据的示例。","code":""},{"path":"chap9.html","id":"sec9-6","chapter":"第 9 章 多水平模型","heading":"9.6 总结","text":"多水平研究定义为至少存在两种随机变异源的研究。最简单的多水平研究是带子抽样的完全随机设计。对于高斯数据，多水平研究必须在线性预测器中包含一个随机效应，以解释较大单元的变异。最小单元的变异嵌入给定随机效应的观测的条件分布。对于单参数非高斯分布，与最小水平相关联的变异也可以包括在线性预测器中。不这样做是过度分散的常见原因，这一问题从第 ?? 章开始进行更详细的讨论。掌握“Fisher 会怎么做？”过程是准确构建多水平模型的有效辅助。设计结构和处理结构可以混合搭配。除少数例外情况外，没有一种处理结构必然排除某种设计结构，反之亦然。负方差分量估计影响功效和 类错误控制。通常，将负解置零会为检验统计量带来偏差，因此可能是一个问题。对于高斯数据，允许方差估计保持为负值，虽然在解释方差估计本身时很难解释，但这是保持检验统计量无偏并因此保持对 类错误控制的最佳方法。多水平模型通常可以重新表示为复合对称协方差模型。这样做可以消除负方差估计的问题，并保持检验统计量和标准误的无偏性。对于非高斯 GLMMs，将模型重新表达为复合对称协方差模型变得更加重要，因为 NOBOUND 选项更容易导致估计方程性能不佳。","code":""},{"path":"chap9.html","id":"exe9","chapter":"第 9 章 多水平模型","heading":"练习","text":"本习题部分使用文件 Ch_8_Problem1.sas 中包含的数据。本研究比较了四种处理在暴露于降解剂后的劣化情况。数据来自七个批次，这些批次是随机选取的批次总体的代表。每个批次都有足够的材料可以分成两半——一种处理可以随机分配给半个批次，另一种处理分配给另一个半个批次。材料暴露于降解剂中；在数据集中，X 表示暴露水平，X 的值等于 1, 2, 4, 8 和 16，值越大，暴露越大。响应变量用 Y 表示；你可以假定它具有高斯分布。另外，众所周知，在本研究观察到的 X 范围内，Y 对 X 的增加呈线性响应。Y 值较低则不佳；Y 值较高则良好。各批次的处理分配如下图所示：\n写出与本研究相关的统计模型的完整描述，与上面给出的描述和假定一致。\n设计是连通的吗？请解释。\n该研究的目的是确定“最佳”处理。对于研究人员来说，“最佳”意味着对增加降解物暴露带来的不良影响具有最大抵抗力的处理（Y 越大意味着抵抗力越强）。根据你在 . 中给出的模型，使用 PROC GLIMMIX 分析数据。使用相关结果来实现研究人员的目标。哪种处理最好，为什么？处理如何排名以及为什么？\n写出与本研究相关的统计模型的完整描述，与上面给出的描述和假定一致。设计是连通的吗？请解释。该研究的目的是确定“最佳”处理。对于研究人员来说，“最佳”意味着对增加降解物暴露带来的不良影响具有最大抵抗力的处理（Y 越大意味着抵抗力越强）。根据你在 . 中给出的模型，使用 PROC GLIMMIX 分析数据。使用相关结果来实现研究人员的目标。哪种处理最好，为什么？处理如何排名以及为什么？本习题使用文件 Ch_9_Problem2.sas 中包含的数据。进行了一项多州研究。处理设计为 22 析因。数据集中的因素表示为 和 B，各具有两个水平，表示为 0 和 1。原始方案要求每个州使用裂区，其中因素 作为整区因素，因素 B 作为裂区因素。州 1、州 2 和州 3 按照原始方案正确执行；州 4 和州 5 弄错了方案：它们使用因素 B 作为全区因素，使用因素 作为裂区因素。响应变量是 Y，你可以假定它具有高斯分布。研究人员现在希望对整个数据集进行单一的合并分析。\n编写包含所需元素的统计模型，以实现所需的分析。\n使用 GLIMMIX 执行与 . 一致的分析。写一份简短的报告来描述两种处理的效应。请记住，报告处理效应“大小” (“big”) 以及“类型” (“kind”) 与报告效应是否具有统计显著性同样重要。特征描述至少有一部分基于数据尺度进行——因为，非统计学读者会坚持这一点。引用相关证据来支持你的描述。\n如果你假定了随机区组效应，如果你将假定更改为固定区组，会发生什么？如果你假定了固定区组，如果你将假定更改为随机区组效应，又会发生什么？“会发生什么”包括在 GLIMMIX 语句中你需要做出哪些更改，以及输出列表中会发生哪些变化？\n本习题使用文件 Ch_9_Problem2.sas 中包含的数据。进行了一项多州研究。处理设计为 22 析因。数据集中的因素表示为 和 B，各具有两个水平，表示为 0 和 1。原始方案要求每个州使用裂区，其中因素 作为整区因素，因素 B 作为裂区因素。州 1、州 2 和州 3 按照原始方案正确执行；州 4 和州 5 弄错了方案：它们使用因素 B 作为全区因素，使用因素 作为裂区因素。响应变量是 Y，你可以假定它具有高斯分布。研究人员现在希望对整个数据集进行单一的合并分析。编写包含所需元素的统计模型，以实现所需的分析。使用 GLIMMIX 执行与 . 一致的分析。写一份简短的报告来描述两种处理的效应。请记住，报告处理效应“大小” (“big”) 以及“类型” (“kind”) 与报告效应是否具有统计显著性同样重要。特征描述至少有一部分基于数据尺度进行——因为，非统计学读者会坚持这一点。引用相关证据来支持你的描述。如果你假定了随机区组效应，如果你将假定更改为固定区组，会发生什么？如果你假定了固定区组，如果你将假定更改为随机区组效应，又会发生什么？“会发生什么”包括在 GLIMMIX 语句中你需要做出哪些更改，以及输出列表中会发生哪些变化？提示：对于 . 利用这样一个事实：将裂区结构视为不连通不完全区组设计。在整个研究中的所有区组中，给处理组合一个一致的标识。以这种方式写出分区计划并利用“Fisher 会怎么做？”过程应该会产生一个可用的方法。数据集 Ch9_Problem3a.sas 和 Ch9_Problem3b.sas 包含来自一个条区实验的数据，该实验有六个区组和 3 × 3 的处理设计—— 有 3 个水平，B 有 3 个水平。以下是条区中一个区组的示意图。在每个区组中，的水平被随机分配到一行，并在整行中应用；B 的水平被随机分配到一列，并在整列中应用。每个区组中的 到行和 B 到列的分配都是潜在唯一的。\n显示得出此设计的框架 ANOVA 的 WWFD 过程。\n写出 . 得出的线性预测器。包括关于随机模型效应的假定。\n对于以随机模型效应为条件的以下哪种响应分布，在线性预测器中包含与 block**b 对应的项是合理的？\n高斯\n贝塔\n二项\n泊松\n负二项\n\n判断题：若为你在 . 中编写线性预测器拟合负二项模型，你隐含地假定了 block**b（单元水平）效应具有伽马分布。\n数据集 Ch9_Problem3a.sas 和 Ch9_Problem3b.sas 包含来自一个条区实验的数据，该实验有六个区组和 3 × 3 的处理设计—— 有 3 个水平，B 有 3 个水平。以下是条区中一个区组的示意图。在每个区组中，的水平被随机分配到一行，并在整行中应用；B 的水平被随机分配到一列，并在整列中应用。每个区组中的 到行和 B 到列的分配都是潜在唯一的。显示得出此设计的框架 ANOVA 的 WWFD 过程。写出 . 得出的线性预测器。包括关于随机模型效应的假定。对于以随机模型效应为条件的以下哪种响应分布，在线性预测器中包含与 block**b 对应的项是合理的？\n高斯\n贝塔\n二项\n泊松\n负二项\n高斯贝塔二项泊松负二项判断题：若为你在 . 中编写线性预测器拟合负二项模型，你隐含地假定了 block**b（单元水平）效应具有伽马分布。","code":""},{"path":"bib.html","id":"bib","chapter":"参考文献","heading":"参考文献","text":"\nAgresti, . 2013. Categorical Data Analysis, 2nd edition.Akaike, H. 1974. new look statistical model identification. IEEE Transaction Automatic Control AC-19, 716–723.Anderson, R.L. Nelson, L.. 1975. family models involving intersecting straight lines concomitant experimental designs used evaluating response fertilizer nutrients. Biometrics 31, 303–318.Atkinson, .C. 1969. use residuals concomitant variable. Biometrika 56, 33–41.Bartlett, M.S. 1947. use transformations. Biometrics 3, 39–52.Barlett, M.S. 1978. Nearest neighbour models analysis field experiments (discussion). J Royal Statistical Society B 40, 147–74.Bates, D.G. Watts, D.M. 1988. Nonlinear Regression Analysis Applications. New York: Wiley.Bello, N.M. , Steibel J.P. Pursley J.R. 2006. Optimizing ovulation first GnRH improved outcomes hormonal injection ovsynch lactating dairy cows. J Dairy Science 89(9), 3413–3424.Bhattacharya, C.G. 1998. Goodness Yates-Rao procedure recovery inter-block. Information Sankhya 61(1), 134–144.Bolker, B.M. , Brooks, M.E. , Clark, C.J. , Geange, S.M. , Poulsen, J.R. , Stevens, H.H. White, J.S. 2009.Generalized linear mixed models: practical guide ecology evolution. Trends Ecology & Evolution 24(3), 127–135.Box, G.E.P. 1976. Science statistics. Journal American Statistical Association 71(356), 791–799. doi:10.1080/01621459.1976.10480949.Box, G.E.P. Behnken, D.W. 1960. new three-level designs study quantitative variables. Technometrics 2, 455–475.Box, G.E.P. Draper, N.R. 1987. Empirical Model Building Response Surfaces. New York: Wiley.Box, G.E.P. , Hunter, J.S. Hunter, W.G. 2005. Statistics Experimenters, 2nd edition. New York: Wiley.Box, G.E.P. , Jenkins, G.M. Reinsel, G.C. 2008. Time Series Analysis: Forecasting & Control, 3rd Edition. New York: Wiley.Breslow, N.E. Clayton, D.G. 1993. Approximate inference generalized linear mixed models. J American Statistical Association 88(421), 9–25.Breslow, N.E. Lin, X. 1995. Bias correction generalised linear mixed models single component dispersion. Biometrika 81, 81–91.Burnham, K.P. Anderson, D.R. 1998. Model Selection Inference: Practical Information-Theoretic Approach. New York: Springer-Verlag.Burnham, K.P. Anderson, D.R. 2002. Model Selection Multi-Model Inference. New York: SpringerVerlag.Casella, G . Berger, R.L. 2002. Statistical Inference, 2nd edition. Pacific Grove, CA: Duxbury.Christiansen, R ., Johnson, W ., Branscum, . Hanson, T.E. 2011. Bayesian Ideas Data Analysis: Introduction Scientists Statisticians. Boca Raton, FL. CRC Press.Claassen, E.. 2014. Reduced Bias Method Estimating Variance Components Generalized Linear Mixed Models PhD Disseration, Lincoln, NE. University Nebraska.Cochran, W.G. Cox, G.M. 1957. Experimental Designs, 2nd edition. New York: Wiley.Cressie, N..C. 1993. Statistics Spatial Data. New York: Wiley.Cressie, N..C. Wikle, C.K. 2011. Statistics Spatio-Temporal Data. New York: Wiley.Davidian, M . Giltinan, D.M. 1995. Nonlinear Models Repeated Measurement Data. New York: Chapman Hall.Davidian, M . Giltinan, D.M. 2002. Nonlinear models repeated measurement data: n update overview. J Agricultural Biological Environmental Statistics 8(4), 387–419.de Boor, C. 1978. Practical Guide Splines. New York: Springer Verlag. http://dx.doi.org/10.1007/978-1-4612-6333-3Demidenko, E. 2004. Mixed Models: Theory Applications. New York: Wiley.Diggle, P.J. , Heagerty, P ., Liang, K-Y . Zeger, S.L. 2002. Analysis Longitudinal Data, 2nd edition. Oxford, UK: Oxford Press.Dobson, .J. Barnett, .G. 2008. Introduction Generalized Linear Models, 3rd edition. Boca Raton, FL: CRC Press.Dunnett, C.W. Hsu J.C. 1992. factor analytic approach simultaneous inference general linear models. J Computational Graphical Statistics, 1, 151–168.Dunnett, C.W. 1955. multiple comparison procedure comparing several treatments control. J American Statistical Association 50, 1096–1121.Eisenhart, C. 1947. assumptions underlying analysis variance. Biometrics 3, 1–21.Federer, W.T. 1955. Experimental Design - Theory Applications. New York: Macmillan Publishing.Federer, W.T. King, F. 2007. Variations Split Plot Split Block Experiment Designs. New York: Wiley.Ferrari, S.L.P. Cribari-Neto, F. 2004. Beta regression modeling rates proportions J Applied Statistics 31, 799–815.Fisher, R.. 1935. Comments Yates, “Complex Experiments.” J Royal Statistical Society Supplement, 2, 181–223.Fisher, R.. Mackenzie, W.. 1923. Studies Crop Variation II: manurial response different potato varieties. J Agricultural Science 13, 311–320.Fitzmaurice, G ., Davidian, M ., Verbeeke, G . Molenberghs, G ., ed. 2009. Longitudinal Data Analysis. Boca Raton, FL: Chapman Hall.Frenzel, M ., Stroup, W.W. Paparozzi, E.T. 2010. Review: Update Modeling Design Strategies Agricultural Dose-Response Experiments Proceedings 22nd Conference Applied Statistics Agriculture. Manhattan, KS: Kansas State University Department Statistics, 245 - 266.Gallant, .R. 1987. Nonlinear Statistical Models. New York: Wiley.Geisbrecht, F. G. Burns, J. C. 1985. Two-stage analysis based mixed model: large-sample asymptotic theory small sample simulation results. Biometrics 41(2), 477–486.Gelman, . 2005. Analysis variance - important ever. Annals Statistics 33, 1–53.Gelman, . 2006. Prior distributions variance parameters hierarchical models. Bayesian Analysis , 515–533.Gelman, ., Carlin, J ., Stern, H . Rubin, D. 2004. Bayesian Data Analysis, 2nd edition. London, UK: Chapman Hall.Gilmour, .R. 2007. Mixed model regression mapping QTL detection experimental crosses. Computational Statistics Data Analysis 51, 3749–3764.Gilmour, .R. et al. 1995. Average information REML: efficient algorithm variance parameter estimation linear mixed models. Biometrics, 51(4), 1440–1450. JSTOR, https://doi.org/10.2307/2533274. Accessed 24 Jan. 2024.Gimour, .R., Thompson, R. Cullis, B.R. 1995. Average information REML: Efficient Algorithm Variance Parameter Estimation Linear Mixed Models. Biometrics 51, 1440–1450.Goldberger, .S. 1962. Best linear unbiased prediction generalized regression model. J American Statistical Association, 57, 369–375.Goodnight, J. 1979. tutorial SWEEP operator. American Statistician 33(3), 149–158.Gotway, C.. Stroup, W.W. 1997. generalized linear model approach spatial data prediction. J Agricultural Biological Environmental Statistics 2, 157–187.Green, J.L. , Stroup, W.W. Fellers, P.S. 2017. Defining program effects: distribution-based Perspective Statistics Public Policy 4(1), 1–10.Greenhouse, S.W. Geisser, S. 1959. methods analysis profile data. Psychometrika 32, 95–112.Gu, C. 2002. Smoothing Spline ANOVA Models. New York: Springer-Verlag.Guerin, L . Stroup, W.W. 2000. simulation study evaluate PROC MIXED analysis repeated measures data. Proceedings 12th Conference Applied Statistics Agriculture. Manhattan, KS: Kansas State University, Department Statistics, 170–203.Hardin, J.W. Hilbe, J.M. 2003. Generalized Estimating Equations. Boca Raton, FL: Chapman Hall.Hardin, J.W. Hilbe, J.M. 2018. Generalized Linear Models Extensions, 4th Edition. College Station, TX: STATA Press.Harville, D.. 1976. Extensions Gauss-Markov theorem include estimation random effects. Annals Statistics 4(2), 384–395.Harville, D.. 1977. Maximum likelihood approaches variance component estimation related problems. J American Statistical Association 72(358), 320–338.Harville, D.. 1997. Matrix Algebra Statisticians Perspective. New York: Springer-Verlag.Harville, D.. Jeske, D.R. 1992. Mean squared error estimation prediction general linear model. J American Statistical Association 87, 724–731.Hastie, T.J. Tibshriani, R.J. 1990. Generalized Additive Models. Boca Raton, FL: CRC Press.Hedecker, D . Gibbons, R.D. 1994. random effects ordinal regression model multilevel analysis. Biometrics 50, 933–944.Henderson, C.R. 1963. Selection index expected genetic advance Hanson, W.D. Robinson, H.F. , Statistical Genetics Plant Breeding. 141–163, National Academy Science - National Research Council Publication 982.Henderson, C.R. 1975. Best linear unbiased estimation prediction selection model. Biometrics 31(2), 423–447.Henderson, C.R. 1984. Applications Linear Models Animal Breeding. University Guelph.Henderson, C.R. 1985. Best linear unbiased prediction using relationship matrices derived selected base populations. Journal Dairy Science 68, 443–448.Henderson, C.R. 1950. Estimation genetic parameters. Annals Mathematical Statistics 21, 309–310.Hilbe, J.M. 2011. Negative Binomial Regression, 2nd edition. Cambridge, UK: Cambridge University Press.Huynh, H . Feldt, L.S. 1970. Conditions mean square ratios repeated measurements designs exact F-Distributions. Journal American Statistical Association 65, 1582–1589.Isaaks, E. H. Srivastava, R. M. 1989. Introduction Applied Geostatistics Oxford. UK: Oxford University\nPress.Jansen, J. 1990. statistical analysis ordinal data extravariation present. Applied Statistics 39, 75–84.Jiang, J. 2007. Linear Generalized Linear Mixed Models Applications. New York: SpringerVerlag.Joe, H . Zhu, R. 2005. Generalized Poisson distribution: property mixture poisson comparison negative binomial distribution. Biometrical Journal, 47, 219–229.Johnson, D.L. Thompson, R. 1995. Restricted Maximum Likelihood Estimation Variance Components Univariate Animal Models Using Sparse Matrix Techniques Average Information. Journal Dairy Science 78, 449–456.Journel, . G. Huijbregts, C.J. 1978. Mining Statistics. London, UK: Academic Press.Kackar, R.N. Harville, D.. 1984. Approximations standard errors fixed random effect mixed linear models. Journal American Statistical Association 79(388), 853–862.Kenward, M.G. Roger, J.H. 1997. Small sample inference fixed effects restricted maximum likelihood. Biometrics 53(3), 983–997.Khuri, . . Cornell, J.. 1996. Response Surfaces: Designs Analyses. New York: Marcel-Dekker.Konishi, S . Kitigawa, G. 2008. Information Criteria Statistical Modeling. New York: Springer-Verlag.Kullbach, S . Leibler, R.. 1951. information sufficiency. Annals Mathematical Statistics 22, 79–86.Laird, N.M. Ware, J.H. 1982. Random effects models longitudinal data. Biometrics 38(4), 963–974.Lambert, D. 1992. Zero-Inflated Poisson regression models application defects manufacturing. Technometics 34, 1–14.Landes, R.D. , Stroup, W.W. , Paparozzi, E.T. Conley, M.E. 1999. Nonlinear models multi-factor plant nutrition experiments. Proceedings 11th Conference Applied Statistics Agriculture. Manhattan, KS: Kansas State University, Department Statistics, 105–119.Lee, Y . Nelder, J.. 1996. Hierarchical generalized linear models (discussion). Journal Royal Statististical Society B 58, 619–678.Lee, Y ., Nelder, J.. Patiwan, Y. 2006. Generalized Linear Models Random Effects: Unified Analysis Via h-likelihood. Boca Raton, FL: Chapman Hall.Liang, K.-Y. Zeger, S.R. 1986. Longitudinal data analysis using generalized linear models. Biometrika 73(1), 13–22.Lin, X. Breslow, N.E. 1995. Bias correction generalized linear mixed models multiple components dispersion. J American Statistical Association 91, 1007–1016.Linstrom, M.J. Bates, D.M. 1988. Newton-Raphson EM algorithms linear mixed-effects models repeated-measures data. J American Statistical Association 83, 1014–1022.Littell, R.C. 1980. Examples GLM applications SAS Users’ Group International: Proceedings Fifth Annual Conference. Cary, NC: SAS Institute, Inc. 208–214.Littell, R.C. , Milliken, G.. , Stroup, W.W. , Wolfinger, R.D. Schabenberger, O. 2006. SAS Mixed Models, 2nd edition. Cary, NC: SAS Institute, Inc.Littell, R.C. , Stroup, W.W. Freund, R.J. 2002. SAS Linear Models, 4th edition. Cary, NC: SAS Institute, Inc.Lohr, V.. O’Brien, R.G. 1984. Power Analysis Univariate Linear Models: SAS Makes Easy SAS Users’ Group International: Proceedings Ninth Annual Conference. Cary, NC: SAS Institute, Inc. 847–852.McCaffery, D.F. Lockwood, J.R. , Koretz, D ., Louis,T.. Hamilton, L. 2004. Models value-added modeling teacher effects. J Educational Behavioral Statistics 29(1), 67–101.McCullagh, P . Nelder, J.. 1989 Generalized Linear Models, 2nd edition New York: Chapman HallMcCulloch, C.E. 1997. Maximum likelihood algorithms generalized linear mixed models. J American Statistical Association 92, 162–170McCulloch, C.E. , Searles, S.R. Neuhaus, J.M. 2008. Generalized, Linear Mixed Models, 2nd edition. New York: Wiley.McLean, R.. , Sanders, W.L. Stroup, W.W. 1991. unified approach mixed linear models. American Statistician 45, 54–64.Mead, R. 1988. Design Experiments Cambridge, UK: Cambridge University Press.Milliken, G. . Johnson, D.E. 2008. Analysis Messy Data, Vol. 1, 2nd edition. New York: Chapman Hall.Molenberghs, G . Verbeeke, G. 2006. Models Discrete Longitudinal Data. New York: Springer-Verlag.Morel, J.G. , Bokossa, M.C. Neerchal, N.K. 2003. Small sample correction variance GEE estimators. Biometrical Journal 45(4), 395–409.Mullahy, J. 1986. Specification testing modified count data models. J Econometrics 33, 341–365.Murray, D.M. , Varnell, S.P. Blitstein, J.L. 2004. Design analysis group-randomized trials: review recent methodological developments. American Journal Public Health 94, 423–432.Myers, R.H. , Montgomery, D.C. Anderson-Cook, C.M. 2016. Response Surface Methodology: Process Product Optimization Using Designed Experiments, 4th edition. New York: Wiley.Nelder, J.. 1968. combination information generally balanced designs. J Royal Statistical Society B 30, 303–311.Nelder, J.. 1977. reformulation linear models. J Royal Statistical Society 140(1), 48–77.Nelder, J.. 1994. Statistics linear models: back basics. Statistics Computing 4, 221-234.Nelder, J.. 1998. great mixed-model muddle alive flourishing, alas!. Food Quality Preference 9, 157–159.Nelder, J.. Wedderburn, R.W.M. 1972. Generalized linear models. J Royal Statistical Society 135(3), 370–384.Nelder, J.. Lee, Y. 1992. Likelihood, quasi-likelihood pseudo-likelihood: comparisons. J Royal Statistical Society B 54(1), 273–284.Nelson, P.R. 1982. Exact critical points analysis means. Communications Statistics, 11, 699–709.Nelson, P.R. 1991. Numerical evaluation multivariate normal integrals correlations. Frontiers Statistical Scientific Theory & Industrial Applications, 97–114.Nelson, P.R. 1993. Additional uses analysis means extended tables critical values technometrics. Technometrics 35, 61–71.Ntzoufras, . 2009. Bayesian Modelsing Using WINBugs. Hoboken, NJ: John Wiley Sons.O’Brien, R.G. Lohr, V.. 1984. Power Analysis Linear Models: Time Come SAS Users’ Group International: Proceedings Ninth Annual Conference. Cary, NC: SAS Institute, Inc. 840–846.Olson, L. M. , Stroup, W. W. , Paparozzi, E. T. Conley, M. E. 2001. Model building multifactor plant nutrition experiments. Proceedings the18th Conference Applied Statistics Agriculture. Manhattan, KS: Kansas State Univerity Department Statistics. 183–206.Papadakis, J.S. 1937. Methode statistique pour des experiences sur champ. Bull. Inst. Amel. Plantes Salonique (23), 13–29.Park, T . Casella, G. 2008. Bayesian Lasso. Journal American Statistical Association 103(482), 681–686.Patiwan, Y. 2001. Likelihood: Statistical Modelling Inference Using Likelihood. Oxford, UK: Oxford University Press.Paparozzi, E.T. , Stroup, W.W. Conley, M.E. 2005. investigate four-way nutrient interactions plants: new look response surface methods. J American Society Horticultural Science, 130, 459–468.Patterson, H.D. Thompson, R. 1971. Recovery inter-block information block sizes unequal. Biometrika 58, 545–554.Pinheiro, J.C. Bates, D.M. 1995. Approximations log-likelihood function non-linear mixed effects model. J Computational Graphical Statistics 4(1), 12–35.Pinheiro, J.C. Chao, E.C. 2006. Efficient Laplacian adaptive Gaussian quadrature algorithms multilevel generalized linear mixed models. J Computational Graphical Statistics 15, 58–81.Prasad, N.G.N. Rao, J.N.K. 1990. estimation mean squared error small-area estimators. J American Statistical Association 85(409), 163–171.Raudenbush, S.M. 2004. value-added models estimating imply statistical practice? J Educational Behavioral Statistics 29(1), 121–129.Raudenbush, S.M. 2009. Targets inference heirarchical models longitudinal data Fitzmaurice, G., Davidian , M., Verbeeke, G . Molenberghs, G ., ed. Longitudinal Data Analysis, Boca Raton, FL: Chapman Hall.Raudenbush, S.M. Byrk, .S. 2002. Heirarchical Linear Models: Applications Data Analysis Methods. Newbury Park, CA: Sage Press.Robinson, G.K. 1991. BLUP good thing. Statical Science 6(1), 15–32. Ruppert, D. W. , M.P. Carroll, R.J. 2003. Semiparametric Regression. Cambridge, UK: Cambridge University Press.Sanders, W.L. , Saxton, . Horn, B. 1997. Tennessee value-added assessment system: quantitative outcomes-based approach educational assessment J. Millman , Grading Teachers, Grading Schools: Student Achievement Valid Educational Measure? 137–162, Thousand Oaks, CA: Corwin Press, Inc.SAS Institute Inc. 2023. SAS/STAT® 15.3 User’s Guide. Cary, NC: SAS Institute Inc.Satterthwaite, F.E. 1941. Synthesis variance. Psychometrika 6, 309–316.Satterthwaite, F.E. 1946. approximate distribution estimates variance components. Biometrics Bulletin 2, 110–114.Schabenberger, O. 2008. Aspects Analysis Split-Plot Experiments JSM 2008 Section Physical Engineering Sciences. Abstract available: www.amstat.org/meetings/jsm/2008/onlineprogram/index.cfmSchabenberger, O . Gotway, C.. 2004. Statistical Methods Spatial Data Analysis. Boca Raton, FL: CRC Press.Schabenberger, O . Pierce, F.J. 2002. Contemporary Statistical Models Plant Soil Sciences. Boca Raton, FL: CRC Press.Schall, R. 1991. Estimation generalized linear models random effects. Biometrika 78(4), 719–727.Schott, J.R. 2005. Matrix Analysis Statisticians, 2nd edition. New York: Wiley.Schwarz, G. 1978. Estimating dimension model. Annals Statistics 6, 461–464.Searle, S.R. 1971. Linear Models. New York: Wiley.Searle, S.R. 1987. Linear Models Unbalanced Data. New York: Wiley.Searle, S.R. , Casella, G . McCulloch, C.E. 1992. Variance Component Estimation. New York: Wiley.Self, S.G. Liang, K.-Y. 1987. Asymptotic properties maximum likelihood estimators likelihood ratio tests nonstandard conditions. J American Statistical Association 398, 605–610.Sidak, Z. 1967. Rectangular confidence regions means multivariate normal distributions. J American Statistical Association 62, 626–633.Simpson, E.H. 1951. interpretation interaction contingency tables. J Royal Statistical Society B 13, 238–241.Snedecor, G . Cochran, W.G. 1989. Statistical Methods, 8th edition. Ames IA: Iowa State University Press. Speed, T. 2010. ANOVA thing IMS. Bulletin, 39(4), 16.Steel, R.D.G. , Torrie, J.H. Dickey, D.. 1996. Principles Procedures Statistics: Biometrical Approach, 3rd edition. New York: McGraw-Hill.Stiratelli, R ., Laird, N . Ware, J.H. 1984. Random effects models serial observations binary response. Biometrics 40(4), 961–971.Stokes, M.E. , Davis, C.S. Koch, G.C. 2000. Categorical Data Analysis Using SAS System, 2nd edition. Cary, NC: SAS Institute, Inc.Stroup, W.W. 1999. Mixed Model Procedures Assess Power, Precision, Sample Size Design Experiments 1999 Proceedings Biopharmaceutical Section. Alexandria, VA: American Statistical Association, 15–24.Stroup, W.W. 2002. Power analysis based spatial effects mixed models: tool comparing design analysis strategies presence spatial variability. J Agricultural, Biological, Environmental Statistics 7, 491–511.Stroup, W.W. , Baenziger, P.S. Mulitze, D.K. 1994. Removing Spatial Variation Wheat Yield Trials: Comparison Methods Crop Science 86, 62–66.Stroup, W.W. Claassen, E.. 2020. Pseudo-likelihood versus Quadrature: thought knew, think know, still trying figure . J Agricultural, Biological Environmental Statistics 25, 639–656.Stroup, W.W. , Guo, S ., Paparozzi, E.T. Conley, M.E. 2006. comparison models designs experiments nonlinear dose-response relationships. Proceedings 18th Conference Applied Statistics Agriculture. Manhattan, KS: Kansas State University Department Statistics, 214–241.Stroup, W.W. Littell, R.C. 2002. Impact Variance Component Estimates Fixed Effect Inference Unbalanced Linear Mixed Models. Proceedings 14th Conference Applied Statistics Agriculture. Manhattan, KS, Kansas State University Department Statistics, 32–48.Stroup, W.W. , Milliken, G.. , Wolfinger, R.D. Claassen, E.. 2018. SAS Mixed Models: Introduction Basic Applications. Cary, NC: SAS Institute, Inc.Tibshirani, R.J. 1996. Regression Shrinkage Selection via Lasso. Journal Royal Statistical Society: Series B (Methodological), 58(1), 267–288.Vonesh, E.F. Chinchilli, V.M. 1997. Linear Nonlinear Models Analysis Repeated Measurements. New York: Marcel-Dekker.Wedderburn, R.W.M. 1974. Quasi-likelihood functions, generalized linear models Gauss-Newton method. Biometrika 61(3), 439–447.Westfall, P.H. , Tobias, R.D. , Rom, D ., Wolfinger, R.D. Hochberg, Y. 1999. Multiple Comparisons Multiple Tests Using SAS System. Cary, NC: SAS Institute, Inc.Williams, R.M. 1952. Experimental designs serially correlated observations. Biometrika 39, 151–67.Wolfinger, R.D. O’Connell, M. 1993. Generalized linear mixed models: Pseudo-likelihood Approach. J Statistical Computation Simulation 4, 233–243.Wright, S. 1922. Coefficients inbreeding relationship. American Naturalist 56, 330–338. Wu, L. 2010. Mixed Effects Models Complex Data. Boca Raton, FL: CRC Press.Yates, F. 1935. Complex experiments. J Royal Statistical Society 2, 181–223.Yates, F. 1940. recovery inter-block information balanced incomplete block designs. Annals Eugenics 10, 317–325.Young, L.J. Young, J.B. 1998. Statistical Ecology. New York: Springer-Verlag.Young, L.J. , Campbell, N.L. Capuano, G.. 1998. Analysis overdispersed count data single-factor experiments: comparative study. J Agricultural, Biological, Environmental Statistics 4, 258–275.Zeger, S.L. Liang K.-Y. 1986. Longitudinal data analysis discrete continuous outcomes. Biometrics 42, 121–130.Zeger, S.L. , Liang, K.-Y. Albert, P.S. 1988. Models longitudinal data: generalized estimating equation approach. Biometrics 44(4), 1049–1060.Zwillinger, D. (editor), 2018. CRC Standard Mathematical Tables Formulas (Advances Applied Mathematics), 33rd edition. Boca Raton: CRC Press.","code":""}]
